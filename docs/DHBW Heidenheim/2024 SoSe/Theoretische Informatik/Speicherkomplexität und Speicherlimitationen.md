Bei der Effizienzanalyse von Algorithmen konzentriert man sich nicht nur auf die Laufzeitkosten, sondern auch auf den Speicherbedarf. Der Speicherbedarf ist besonders wichtig, wenn:

- **Speicher knapp** ist, beispielsweise auf Ger√§ten mit begrenztem Speicher.
- **Datenmengen sehr gro√ü** sind, wie im Fall von Big Data.

Algorithmen sollten sowohl schnell als auch speichereffizient sein. Man muss jedoch entscheiden, ob man Geschwindigkeit √ºber Speicherverbrauch stellt oder umgekehrt.

## Big-O-Notation f√ºr Speicherbedarf

Neben der Zeitkomplexit√§t, die beschreibt, wie viele Schritte ein Algorithmus bei einer bestimmten Anzahl von Datenelementen (N) ben√∂tigt, gibt es die Speicherkomplexit√§t, die sich darauf konzentriert, wie viele Speichereinheiten ein Algorithmus zus√§tzlich bei N Datenelementen ben√∂tigt.

### Beispiel: Funktion `MakeUppercase()`

Die Funktion `MakeUppercase()` nimmt ein Array von Strings und gibt ein neues Array zur√ºck, in dem alle Strings in Gro√übuchstaben konvertiert sind.

- Eingabe: `["fred", "anna", "oskar", "gerda"]`
- Ausgabe: `["FRED", "ANNA", "OSKAR", "GERDA"]`

> [!example]- üìå **Beispiel in C#:**
> ```csharp
> string[] MakeUppercase(string[] inputArray)
> {
>     string[] resultArray = new string[inputArray.Length];
>     for (int i = 0; i < inputArray.Length; i++)
>     {
>         resultArray[i] = inputArray[i].ToUpper();
>     }
>     return resultArray;
> }
> ```
> In diesem Fall werden zwei Arrays im Hauptspeicher gehalten: das Originalarray und das neu erzeugte Array. Der Gesamtspeicherbedarf ist proportional zur Menge der Eingangsdaten, also O(N).

### Speichereffiziente Version (In-Place)

Eine speichereffizientere Variante verzichtet auf die Erzeugung eines neuen Arrays und modifiziert das √ºbergebene Array direkt ("in place").

> [!example]- üìå **Beispiel in C#:**
> ```csharp
> void MakeUppercaseInPlace(string[] inputArray)
> {
>     for (int i = 0; i < inputArray.Length; i++)
>     {
>         inputArray[i] = inputArray[i].ToUpper();
>     }
> }
> ```
> In dieser Version wird kein zus√§tzlicher Speicher ben√∂tigt. Der Speicherbedarf ist konstant, also O(1).

## Vergleich der Implementierungen

| Version    | Zeitkomplexit√§t | Speicherkomplexit√§t |
|------------|-----------------|---------------------|
| Version #1 | O(N)            | O(N)                |
| Version #2 | O(N)            | O(1)                |

## Trade-offs zwischen Zeit und Speicher

Manchmal muss man zwischen Zeit und Speicher abw√§gen. Ein Algorithmus kann schneller sein, ben√∂tigt aber mehr Speicher, oder er ist langsamer, ben√∂tigt aber weniger Speicher.

### Beispiel: √úberpr√ºfung auf Duplikate in einem Array

#### Version #1

- **Zeitkomplexit√§t:** O(N¬≤) (verschachtelte Schleifen)
- **Speicherkomplexit√§t:** O(1) (kein zus√§tzlicher Speicher)

#### Version #2

- **Zeitkomplexit√§t:** O(N) (eine Schleife und ein zus√§tzliches Array)
- **Speicherkomplexit√§t:** O(M), wobei M der Wertebereich des Arrays ist.

> [!example]- üìå **Beispiel in C#:**
> ```csharp
> // Version #1: Ineffizient aber speicherschonend
> bool ContainsDuplicates(string[] array)
> {
>     for (int i = 0; i < array.Length; i++)
>     {
>         for (int j = i + 1; j < array.Length; j++)
>         {
>             if (array[i] == array[j])
>                 return true;
>         }
>     }
>     return false;
> }
>
> // Version #2: Effizient aber speicherintensiver
> bool ContainsDuplicatesEfficient(string[] array)
> {
>     var set = new HashSet<string>();
>     foreach (var item in array)
>     {
>         if (set.Contains(item))
>             return true;
>         set.Add(item);
>     }
>     return false;
> }
> ```

### Effizienz der verschiedenen Versionen

| Version    | Zeitkomplexit√§t | Speicherkomplexit√§t |
|------------|-----------------|---------------------|
| Version #1 | O(N¬≤)           | O(1)                |
| Version #2 | O(N)            | O(M)                |
| Version #3 | O(N log N + N)  | O(log N)            |

- **Version #2** ist optimal, wenn Geschwindigkeit ben√∂tigt wird und der maximale Wertebereich bekannt und handhabbar ist.
- **Version #1** ist optimal, wenn der Speicher knapp ist oder der Wertebereich sehr gro√ü oder unbekannt ist.
- **Version #3** bietet einen guten Kompromiss zwischen Zeit und Platz.

## Sortierung gro√üer, externer Datenmengen

Bei der Sortierung externer Daten kann nicht die gesamte Datenmenge gleichzeitig in den Speicher geladen werden. Stattdessen werden Teilmengen sortiert, was zus√§tzliche Strategien wie "Divide and Conquer" (Teile, Sortiere und Erobere) erfordert. Dieser Ansatz erlaubt auch eine Parallelisierung auf mehrere Rechenkerne, wodurch die Effizienz weiter gesteigert werden kann.
