{"path":"DHBW Heidenheim/2025 WiSe/Datenbanken/Skript/Hänisch/DatenbankScript.pdf","text":"Relationale Datenbanken Die alte Welt: Relationale Datenbanken, Konzepte, Entwurf und Programmierung Till Hänisch This book is for sale at http://leanpub.com/realtionaledatenbanken This version was published on 2015-03-09 This is a Leanpub book. Leanpub empowers authors and publishers with the Lean Publishing process. Lean Publishing is the act of publishing an in-progress ebook using lightweight tools and many iterations to get reader feedback, pivot until you have the right book and build traction once you do. ©2014 - 2015 Till Hänisch Contents Einleitung . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 Wichtige Begriffe . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 Eigenschaften von DBMS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 Transaktionen . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 Datenmodelle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 Konzeptionelles Datenmodell . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 Logisches Datenmodell . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 Normalisierung . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 Zugriff auf Daten . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 DML (Data Manipulation Language) . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 MySQL . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 Installation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 Realisierung eines Datenmodells in MySQL . . . . . . . . . . . . . . . . . . . . . . . . 34 Indices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43 Berechtigungen . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44 Stored Procedures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46 Trigger . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51 Views . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54 Entwicklung von Datenbankanwendungen . . . . . . . . . . . . . . . . . . . . . . . . 56 Realisierung . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58 ODBC . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60 Access als Datenbank-Client . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62 PHP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67 Direkter Zugriff mit SQL . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70 Object Relational Mapping (ORM) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76 Erweiterte Konzepte von Datenbanksystemen . . . . . . . . . . . . . . . . . . . . . . . 87 Performance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 90 Zugriffspfade und Indices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94 Recovery . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101 Transaktionen . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103 Verteilte Datenbanken . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106 Ausblick . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110 Einleitung Datenbanken sind aus der IT-Infrastruktur nicht wegzudenken. Sie stellen in der einen oder anderen Form die Grundlage praktisch aller Anwendungen dar, seien es Informationssysteme im Unternehmen oder Apps auf dem iphone. Die folgenden Kapitel haben das Ziel, Sie mit den Grundlagen dieser Systeme vertraut zu machen, um zu verstehen, wofür man Datenbanken braucht und auf welchen Prinzipien sie aufbauen. In den letzten zwei Jahren sind die NoSQL-Datenbanken immer populärer geworden. Um zu verstehen, was das Besondere an diesen Systemen ist, ist ein Verständnis der Grundlagen der relationalen Datenbanken aber Voraussetzung. Abgesehen davon sind relationale Datenbanken in den allermeisten Fällen die angemessene Technologie zur Speicherung von Daten. In diesem ersten Teil wird zunächst die Abbildung der Daten auf die spezielle Welt der Relationen behandelt, die Datenmodellierung. Dann wird der Zugriff auf die Daten mit SQL dargestellt. Diese Teile sind systemunabhängig, also im wesentlichen für alle (relationalen) Datenbanksys- teme gleich. Ab einem bestimmten Punkt müssen wir den bequemen, systemunabhängigen Weg verlassen und uns einem konkreten DBMS (Daten Bank Management System) zuwenden, hier wird MySQL verwendet. Im zweiten Teil wird die interne Funktionsweise von klassischen relationalen Datenbanken und das Tuning von Datenbanken behandelt. Schließlich werden verteilte Datenbanken und deren Probleme diskutiert. Einleitung 2 Wichtige Begriffe In diesem Abschnitt werden einige wichtige Begriffe definiert. Die Definitionen sind auf die Verwendung in diesem Buch abgestimmt, allgemeinere Definitionen mit ausführlichen Erläuterungen finden Sie in der Literatur. Datenbank Unter einer Datenbank versteht man nicht eine bestimmte Software oder eine Datei, sondern eine Sammlung von logisch zusammenhängenden Datensätzen, die eine Vereinfachung der realen Welt (“Miniworld”) zu einem bestimmten Zweck darstellen. Eine Datenbank ist also ein Modell der Realität. Soll ein solches Modell eine einfachere Struktur als die reale Welt haben (Ja, das wollen wir unbedingt!), dann kann es nur für einen bestimmten Zweck verwendet werden. Entscheidend dabei ist, den Zweck nicht zu speziell (zu einfaches Modell) und nicht zu allgemein (zu komplexes Modell) zu gestalten. Im ersten Fall ist die Datenbank nicht flexibel genug, um zukünftige Anforderungen zu erfüllen, im zweiten Fall ist sie so komplex, dass die Entwicklung der Anwendung (und oft auch die Bedienung) zu aufwendig werden. Datenbanksystem Eine Datenbank ist ein eher abstraktes Konzept. Um diese zu nutzen braucht man ein Daten- banksystem. Darunter versteht man nicht nur die verwendete Software, sondern auch das „Drumherum”, also die eigentlichen Daten, die Hardware, auf der das System läuft, die ver- wendete Software (das DBMS) und die Benutzer. Hier unterscheidet man verschiedene Rollen, den Entwickler, den Datenbankadministrator (DBA) und den Anwender. Relationale Datenbank Es gibt viele Arten, die Daten in einer Datenbank zu organisieren, etwa in Form von Hierarchien. In der Praxis relevant – und hier ausschließlich behandelt - ist heute aber nahezu nur eine Form, die der Relationen. Das relationale Modell geht auf Codd (“A Relational Model of Data for Large Shared Data Banks”) zurück. Eine relationale Datenbank hat folgende Eigenschaften: • Für den Anwender stellen sich die Daten in Form von Tabellen dar • Die möglichen Operationen erzeugen neue Tabellen aus bestehenden, z.B. – Untermenge der Zeilen einer Tabelle – Untermenge der Spalten einer Tabelle Operationen Eine relationale Datenbank benötigt im Prinzip nur eine geringe Anzahl von Funktionalitäten, diese sind, etwas vereinfacht: Einleitung 3 • Tabellen hinzufügen • Datensätze hinzufügen (insert) • Datensätze lesen (select) • Datensätze ändern (update) • Datensätze löschen (delete) • Tabellen löschen Einleitung 4 Eigenschaften von DBMS Datenbankmanagementsysteme müssen eine Reihe von Eigenschaften aufweisen, um in der Praxis sinnvoll genutzt werden zu können: • Persistenz – Gespeicherte Daten bleiben dauerhaft auch bei Systemausfällen erhalten. • Datenunabhängigkeit – Zugreifende Programme müssen nicht geändert werden, wenn sich die physikalische Datenstruktur ändert. In Grenzen gilt dies auch für Änderungen an der logischen Datenstruktur. Letztlich bedeutet dies, dass bestehende Programme bei geänderten Anforderungen nicht abgeändert, sondern nur um neue Funktionen ergänzt werden müssen. Zumindest, wenn man das Datenmodell, also die Struktur der Daten in der Datenbank, clever angelegt hat. Was genau clever bedeutet, ist Gegenstand des nächsten Kapitels. • Unterschiedliche Sichten – Eine Datenbank ist insbesondere dann von Vorteil, wenn man genau eine (na ja, in der Praxis nicht allzu viele ;-) Datenbanken hat, in der alle (in der Praxis möglichst viele) Daten gespeichert sind. Das bedeutet aber, dass diese Datenbank viel mehr Daten enthält, als für jede einzelne Anwendung nötig sind. Dies bringt zwei Probleme mit sich: Erstens wird die Datenbank unübersichtlich, zweitens darf im Allgemeinen nicht jeder Anwender alle Daten sehen (denken Sie etwa an personenbezogene Daten, beispielsweise das Gehalt eines Mitarbeiters). Deshalb muss es möglich sein, Ausschnitte aus den Daten zu definieren und Zugriffsrechte auf diesen Ausschnitten festzulegen. • Transaktionen – Gibt es nur eine Datenbank, greifen im Allgemeinen mehrere (in der Praxis oft sehr viele) Anwender gleichzeit auf die Daten zu. Diese dürfen sich nicht “in die Quere” kommen, es darf nicht passieren, dass etwa ein Anwender unbeabsichtigt die Änderungen eines anderen überschreibt. Dies wird durch Transaktionen verhindert. • Schnell – Datenbanksysteme sind so konstruiert, dass sie (wiederum nur dann, wenn man „clever” vorgeht) einen sehr schnellen Zugriff auch auf große Datenmengen er- lauben. Wird ein bestimmter Datensatz gesucht, dauert dies auch bei Milliarden Datensätzen typischerweise nur einige Millisekunden. Geht man weniger clever vor, dauert der Zugriff auch bei wenigen Datensätzen sehr lang. Diese Probleme sind im Schwerpunkt Gegenstand des zweiten Teils. • Teuer – Die großen Anbieter lassen sich die teilweise jahrzentelange Entwicklung und Erfahrung bezahlen. Sehr teuer kann hier siebenstellig bedeuten …. Open Source Systeme spielen zwar bei vielen Anwendungen, insbesondere im Web eine wichtige Rolle, für unternehmenskritische Anwendungen werden aber meistens die großen kommerziellen Systeme verwendet (Oracle, DB/2, MS SQL-Server). Einleitung 5 Transaktionen Eine wichtige Eigenschaft von Datenbanksystemen ist die Möglichkeit des gleichzeitigen Zugriffs auf die Daten durch viele Anwender. Dieser Zugriff muß in geeigneter Form geregelt werden. Dazu werden die Zugriffe auf die Datenbank in sogenannte „Logical Units of Work” oder Transaktionen strukturiert. Diese haben folgende Eigenschaften: • Atomicity – Die Änderungen an den Daten durch eine Transaktion finden entweder vollständig oder gar nicht statt. Beispiel: Geld wird von Konto 1 abgehoben und Konto 2 gutgeschrieben. Bricht das Programm aus irgendeinem Grund zwischen diesen beiden Aktionen ab, werden die bisherigen Änderungen automatisch rückgängig gemacht. • Consistency – Änderungen finden so statt, dass das System in sich konsistent bleibt. Beispiel: Das DBMS verhindert, dass das zu einer Buchung gehörige Konto gelöscht wird. • Isolation – Für jede Transaktion Ti sieht das System so aus, als ob alle Tj mit i ̸= j entweder vor oder nach Ti ablaufen. Beispiel: Wenn jemand anderes gleichzeitig eine Buchung auf Konto 1 oder Konto 2 macht, wird die Überweisung trotzdem korrekt ausgeführt. Transaktionen sollen sich nicht stören. • Durability – Wenn eine Transaktion abgeschlossen ist, bleiben die Daten auch bei Abstürzen usw. erhalten. Beispiel: Auch wenn der Geldautomat nach der Auszahlung abstürzt, taucht die Auszahlung im Konto (Auszug) auf. Datenmodelle Zu Beginn der Entwicklung einer Datenbank-Anwendung steht die Entwicklung eines Daten- modells, das ist die Überlegung, welche Informationen wie gespeichert werden sollen. Um eine möglichst gute Trennung der verschiedenen Abstraktionsebenen zu erreichen, sind Datenbanken in einer dreischichtigen Architektur realisiert: Drei Ebenen (ANSI-SPARC) Architektur, Abb. nach Wikipedia Anwendungsprogramme befinden sich dabei auf der externen Ebene. Hier sind für jede Anwen- dung und/oder jeden Benutzer unterschiedliche Sichten auf die Daten realisierbar. Die gesamten Daten mit ihren Zusammenhängen werden auf der konzeptionellen Ebene definiert. Wie diese dann tatsächlich gespeichert werden, also etwa, in welchen Dateien, auf welchen Platten, welche Indices verwendet werden usw. ist auf der internen Ebene definiert. Analog hierzu werden Datenmodelle auf drei Ebenen erstellt: Das konzeptionelle, das logische und das physikalische Datenmodell. Datenmodelle 7 Konzeptionelles Datenmodell Das konzeptionelle oder auch semantische Datenmodell ist eine semantische Darstellung der Konzepte, die in einer Anwendung wichtig sind. Semantisch bedeutet hier, dass keine technis- chen Eigenschaften dargestellt werden, sondern nur, welche Daten relevant sind und wie sie zusammenhängen. Dazu werden wichtige „Dinge” und ihre Beziehungen untereinander in Form eines Entity- Relationship-Modells dargestellt. Grundlage ist letztlich die Systemtheorie. Das ER-Modell geht auf Chen (“The Entity-Relationship Model – Toward a Unified View of Data”) zurück: “The entity-relationship model adopts the more natural view that the real world consists of entities and relationships. It incorporates some of the important semantic information about the real world.” Das ER-Modell besteht aus drei Komponenten: • Entities: Dinge aus der realen Welt, etwa Personen, Autos, Vorlesungen, Rechnungen • Relationships: Beziehungen zwischen Entities • Attribute: Eigenschaften, die einen Entity oder eine Relationship näher beschreiben, z.B. Entity Person, Attribute „Name”, „Vorname” Für jeden einzelnen Entitiy hat ein Attribut einen Wert Attribute können aus mehreren Teilen bestehen, z.B. „Adresse” könnte in „PLZ”, „Straße”, „Ort” zerlegt werden. Im Gegensatz zu diesen zusammengesetzten Attributen heißen Attribute, die nicht zerlegt werden können, atomar oder „einfach”. Attribute können einen, aber auch mehrere Werte haben. Ein Entity type definiert eine Gruppe von Entities, die dieselben Attribute haben. Ein Entity type wird definiert durch die Menge der Attribute und einen Namen. Alle Entities, die zu einem Entity type gehören heißen entity set. Analogie zur OOP (objektorientierten Programmierung): • Entity type = Klasse • Entity = Objekt • Entity set = ? (gibt’s nicht!) Hat ein Entity type ein oder mehrere Attribute, die für alle möglichen Entities unterschiedlich ist, heißen diese Schlüsselattribute. Durch den Schlüssel kann jeder Entity identifiziert werden. Entities ohne Schlüsselattribut heißen Weak Entities. Datenmodelle 8 Relationships Untersucht man Entities näher, dann findet man Attribute, die sich auf andere Entities beziehen, etwa könnte der Entity “Mitarbeiter” ein Attribut “Abteilung” haben, das auf den Entity “Abteilung” verweist. Umgekehrt könnte der Entity “Abteilung” ein Attribut “Leiter” haben, das auf einen Mitarbeiter verweist. Diese Attribute werden durch Relationships dargestellt (und eben nicht durch Attribute, das ist das Besondere am Entity Relationship Modell oder kurz ERM). Entity Relationship Diagramm in Chen-Notation Ein Relationship type R zwischen n Entity types E_1 … E_n definiert eine Menge von Assozia- tionen (Relationship set) zwischen Entities dieser Typen. R ist ein Menge von Relationship instances ri, wobei jedes ri n Entities (E1 … En) miteinander verknüpft. Die Ordnung (degree) einer Relationship ist die Zahl der jeweils beteiligten Entities (Entity types), bei „leitet” also = 2 (binär). Relationships höherer als dritter Ordnung sind mir in der Praxis keine bekannt. Jeder Entity type, der an einer Relationship beteiligt ist, hat eine bestimmte Rolle z.B. bei der Relationship “arbeitet für” zwischen Mitarbeiter und Abteilung Mitarbeiter „beschäftigt bei” Abteilung Abteilung „hat Angestellten” Mitarbeiter Wenn eine Relationship verschiedene Entity types miteinander verknüpft, sind Rollennamen redundant. Wenn aber beide Entities zum gleichen Entity type gehören (rekursive Relationship), sind Rollen zur Unterscheidung nötig. Ein wichtiges Beispiel hierfür ist die Darstellung von Hierarchien, etwa zwischen Mitarbeitern: Hier gäbe es etwa eine Relationship “Chef von” zwischen zwei Entities vom Typ Mitarbeiter. Wer aber jetzt der Chef von wem ist, muss durch Rollen definiert werden. Durch die Kardinalität wird definiert, wie viele Entities jeweils teilnehmen können z.B. hat “arbeitet für” zwischen Abteilung und Mitarbeiter die Kardinalität 1:N, jeder Mitarbeiter gehört zu genau einer Abteilung und eine Abteilung kann mehrere Mitarbeiter haben¹. ¹Natürlich ist bei bestimmten Anwendungen auch der Fall denkbar, dass ein Mitarbeiter für mehrere Abteilungen arbeitet, dann wäre die Kardinalität M:N. Datenmodelle 9 Die möglichen Kardinalitäten sind: 1:1, 1:N, M:N Ist jeder existierende Entity des Typs E an einer Relationship beteiligt, nennt man das „total participation” oder „existence dependency”, d.h. ein Entity dieses Typs kann nur existieren, wenn er an R beteiligt ist. Ist dies nicht der Fall, heißt dies „partial participation” Relationships können (wie Entities) Attribute haben. ERD (Entity Relationship Diagramm) ERD in Crowfoot Notation Im ERD werden Entity types und Relationship types dargestellt (im Folgenden eigentlich falsch, aber gebräuchlich als Entity und Realtionship bezeichnet). Es gibt verschiedene Notationen für ER-Diagramme. Zwei weit verbreitete sind die Crowfoot-Notation und die Notation nach Chen. Ein Entity wird immer als Rechteck dargestellt, das den Namen des Entities enthält. In der Crowfoot Notation werden die Attribute ebenfalls in diesem Rechteck dargestellt. Bei der Chen Notation werden die Attribute jeweils als Ellipsen gezeichnet. Der Name von Schlüsselattributen wird unterstrichen dargestellt. Relationships werden in der Chen-Notation als Rauten dargestellt, die den Namen der Relationship enthalten. In der Crowfoot-Notation werden diese als Linien gezeichnet. Die Kardinalitäten werden in der Chen-Notation als Text (1:1, 1:N, M:N) am jeweiligen Ende der Verbindungslinie angegeben, bei der Crowfoot-Notation grafisch dargestellt. Datenmodelle 10 Vergleich von Chen- und Crowfoot-Notation Die Chen-Notation ist zunächst etwas anschaulicher als die Crowfoot-Notation, aber insbeson- dere mit gängigen Zeichenprogrammen nur schlecht umsetzbar (Rauten). Ausserdem ist die Datenmodelle 11 Crowfoot-Notation kompakter, es passt mehr Information auf eine gegebene Fläche Papier. Deshalb: Beim Skizzieren von Hand wird die Chen-Notation verwendet, bei komplexeren Modellen und Erstellung auf dem Rechner die Crowfoot-Notation. Die Namen von Entities werden groß geschrieben, Attribute mit großen Anfangsbuchstaben, Rollen klein. Entity types werden im Singular benannt, normalerweise werden hier Nomen verwendet, Relationships sind normalerweise Verben. Die Namen der Relationships sollten so gewählt werden, dass sich von links nach recht bzw. oben nach unten gelesen ein Sinn ergibt. Eine alternative Notation für die Kardinalitäten ist die aus der UML bekannte min-max-Notation: Wenn ein Entity type E an einer Relationship R teilnimmt, heißt (min, max), dass mindestens „min” und höchstens „max” Entities beteiligt sind. partial participation (0,) total participation (1,) Wert für min, max typ. (0,1,*) Es gibt eine Vielzahl von Erweiterungen des „klassischen” ER-Diagramms nach Chen, etwa zur Abbildung von Spezialisierungen, Alternativen usw. Diese werden hier nicht behandelt, es stellt sich die Frage, ob solche Probleme dann nicht besser in UML modelliert werden. Zusammenfassung ER-Diagramme werden zur konzeptionellen Modellierung verwendet. Sie dienen zur Darstellung der semantischen Struktur eines Datenmodells. Dies stellt eine Abstraktion über der Realität dar. Wichtig bei der Erstellung eines ERDs ist die Übersichtlichkeit ! Datenmodelle 12 Logisches Datenmodell Ist das konzeptionelle Modell fertig, kann es in ein relationales Modell umgewandelt werden. Dieses ist (noch) nicht spezifisch für ein bestimmtes Datenbanksystem. Es wird auch als logisches Datenmodell bezeichnet. Das logische Datenmodell enthält nur noch Relationen, es gibt keine Unterscheidung zwischen Entities und Relationships. Grundsätzlich werden Entities auf Relationen abgebildet. Jede Rela- tion braucht einen Primärschlüssel. Falls dieser im konzeptionellen Modell noch nicht definiert wurde, ist das jetzt nachzuholen. Die Abbildung der Relationships und der Attribute erfordert etwas mehr Aufwand, trotzdem ist die Umwandlung des konzeptionellen in ein logisches Datenmodell ein einfacher Prozess, der einigen wenigen Regeln folgt. Regeln zur Transformation: 1. Entity E –> Relation R mit allen einfachen Attributen von E Falls E zusammengesetzte Attribute hat –> atomare Komponenten jeweils in R übernehmen. Eins der Schlüsselattribute von E wird Primärschlüssel von R. Falls dies zusammengesetzt ist, werden die Komponenten zusammen Primärschlüssel. 2. Weak Entity W –> Relation R (wie oben) zusätzlich den Primärschlüssel des „übergeordneten” Entities als Fremdschlüssel. Primärschlüs- sel wird der Fremdschlüssel und (wenn vorhanden) Teilschlüssel von R. 3. binäre 1:1 Relationship R –> wähle eine der beteiligten Relationen S und T aus (z.B. S) und füge als Fremdschlüssel den Primärschlüssel von T dazu. Anmerkung: Wenn möglich, sollte S in total participation sein (dann keine NULL im Fremdschlüssel) 4. binäre 1:N Relationship –> Relation S, die der „N”-Seite der Relationship entspricht, erhält den Primärschlüssel von T („1”-Seite) als Fremdschlüssel. Falls R Attribute hat –> zu S dazu 5. binäre M:N Relationship –> neue Relations S S enthält die Primärschlüssel der beiden beteiligten Relationen als Fremdschlüssel. Falls R Attribute hat –> zu S dazu 6. mehrwertiges Attribut A –> neue Relation R mit Attribut A’, das die einzelnen Werte von A enthält, zusätzlich den Primärschlüssel K der Relation, die A enthält als Fremdschlüssel, Primärschlüssel ist (A’,K) 7. Relationships höherer Ordnung R –> neue Relation S S enthält die Primärschlüssel der beteiligten Relationen als Fremdschlüssel. Wenn R Attribute hat –> zu S dazu Datenmodelle 13 Normalisierung Fügt man zu einer Tabelle mehr oder weniger willkürlich Spalten hinzu (das kommt im “echten Leben” durchaus häufiger vor), stellt man im Lauf der Zeit fest, dass sich Probleme ergeben. Nehmen wir an, ein Vertriebsmitarbeiter speichert seine Kontakte in einer Tabelle in einer Datenbank. Zunächst hat diese die Spalten Name, Vorname, email, Telefonnummer, Firma. Im Lauf der Zeit stellt sich heraus, dass er gelegentlich auch die postalische Adresse braucht, also fügt er die Spalte Adresse dazu. Das funktioniert auch für eine Weile ganz gut, im Lauf der Zeit stellt sich aber heraus, dass sich die Adressen gelegentlich ändern. Für die Firma “Farbkreis” gibt es jetzt mehrere Adressen. Sind das jetzt mehrere Standorte, oder wurde irgendwann die Adresse bei einem Kontakt wegen eines Umzugs geändert und bei einem anderen wurde das vergessen? Solange es sich um nur wenige Daten handelt, die nur von einer Person geändert werden, können solche Inkonsistenzen leicht wieder bereinigt werden. Bei einer größeren Datenbank, die von mehreren Benutzern verwendet wird, stellt das aber ein großes Problem dar. Woher kommen solche Anomalien und wie kann man sie verhindern? Letztlich sind diese Probleme auf Redundanzen zurückzuführen, eine Information (in diesem Fall die Adresse einer Firma) wird an mehreren Stellen gespeichert. Vermeidet man das nicht, führt das irgendwann praktisch unausweichlich zu Problemen. Wie aber erkennt man solche problematischen Situationen? Ursprünglich wurden von Codd (1971) 3 Normalformen definiert (später kamen noch mehr dazu, BCNF, 4., 5. und 6. Normalform), deren Einhaltung solche Probleme vermeidet. Normalisierung bezeichnet die Umwandlung beliebiger Relationen in eine höhere Normalform. Jede Stufe stellt eine „Verschärfung” gegenüber der vorherigen dar: Eine Relation ist beispiel- sweise nur dann in 2. Normalform, wenn sie in 1. Normalform ist und zusätzliche Bedingungen erfüllt. Datenmodelle 14 Normalisierungsstufen Normalformen sind Kriterien für einen guten Datenbankentwurf, kein “Kochbuch”. In der Regel führt eine Normalisierung zur Aufspaltung von Relationen. Eine strenge Normalisierung führt unter Umständen zu einer großen Zahl von Relationen. Hier ist abzuwägen, ob eine vollständige Normalisierung nötig ist, Normalisierung ist kein Selbstzweck. Zur Normalisierung ist Wissen über die Bedeutung der Attribute nötig. Deshalb sind Beispiele hier immer im Kontext zu sehen. In der Regel ist eine Normalisierung bis zur dritten Normalform sinnvoll. Gegebenenfalls wird dann wieder (etwa wegen Performance) denormalisiert. Funktionale Abhängigkeit (functional dependency FD) Die Definition der Normalformen erfolgt über den Begriff der funktionalen Abhängigkeit (functional dependency). Ein Attribut B ist funktional abhängig vom Attribut A wenn der Wert von B feststeht, falls A bekannt ist. A und B können dabei auch Mengen von Attributen sein. B ist voll funktional von A abhängig, wenn es keine Teilmenge von A gibt, die ebenfalls den Wert von B festlegt. Eine transitive Abhängigkeit liegt vor, wenn B von A funktional abhängig ist und C von B. Die Entscheidung, ob ein Attribut funktional von einem anderen abhängig ist, erfordert also Kenntnis über die inhaltliche Bedeutung der Attribute, also in der Regel Wissen über den Kontext. 1. Normalform Alle Attribute sind atomar, das heißt, es gibt keine Substruktur der Attribute. Datenmodelle 15 Die erste Normalform wird häufig (absichtlich) verletzt, um den Aufwand überschaubar zu halten. Einfachstes Beispiel: Ein Name wird oft als (Nachname,Vorname,Titel) abgebildet. Genau genommen haben aber die meisten Menschen mehrere Vornamen, es gibt Namenszusätze, Adelsprädikate, wenn jemand einen Titel hat, hat er oft auch einen zweiten usw. Dies voll- ständig abzubilden lohnt in den meisten Fällen die Mühe nicht. Also nimmt man (Nachname, Vorname, Titel) und setzt Nachname =”Hänisch, von der rauhen Alb”, Vorname=”Till Matthias”, Titel=”Prof. Dipl. Phys.” 2. Normalform Eine Relation ist in zweiter Normalform, wenn sie in erster Normalform ist und alle Nicht- Schlüsselattribute voll funktional abhängig vom Schlüssel sind. Wenn der Schlüssel aus einem Attribut besteht (und die Relation in 1. Normalform ist), ist eine Relation immer in 2. Normalform. 3. Normalform Eine Relation ist in dritter Normalform, wenn sie in 2. Normalform ist und keine transitive Abhängigkeit von Nicht-Schlüsselattributen aufweist. Die 3. Normalform betrifft nur Nicht-Schlüsselattribute, die Boyce-Codd-Normalform verallge- meinert dies auf alle Attribute. Zusammenfassung: Wenn auf der linken Seite der funktionalen Abhängigkeiten nur der (vollständige) Schlüssel steht, dann ist die Relation in 1. bis 3. Normalform. „Your Attributes shall depend on the key, the whole key and nothing but the key, so help me Codd” (Der ursprüngliche Autor dieses Zitats ist mir leider nicht bekannt, ich habe es zuerst in der Newsgroup comp.databases.oracle gelesen) Ist eine Relation nicht in Normalform, dann müssen Relationen zerlegt werden: Normalform Test Abhilfe 1. NF Relation sollte keine mehrwertigen Attribute haben Neue Relation für jedes mehrwertige Attribut 2. NF Bei Relationen mit zusammengesetztem PK sollte kein Attribut von einem Teil des Schlüssels funktional abhängig sein Zerlegung, neue Relation für jeden Teil des Schlüssels mit den abhängigen Attributen. (Achtung: Beziehung zum PK der ursprünglichen Relation und allen abhängigen Attributen muß erhalten bleiben) 3.NF Kein Nicht-Schlüsselattribut sollte von einem anderen Nicht-Schlüsselattribut funktional abhängig sein Zerlegung, neue Relation mit den Nicht-Schlüsselattributen und ihren abhängigen Attributen Datenmodelle 16 Weitere Einzelheiten zu höheren Normalformen, verlustfreier Zerlegung usw. finden sich in der Literatur wie etwa bei Elmasri oder Date. Für die Praxis sind diese Definitionen und Verfahren jedoch weit weniger wichtig, wenn (!) die vorgeschlagene Methodik verwendet wird: Geht man von einem (sinnvollen) ER-Modell aus, wird das daraus erhaltene logische Datenmodell in der Regel normalisiert sein. Da ein Entity eine abgeschlossene, identifizierbare, selbstständige Einheit darstellt, deren Eigen- schaften (und eben nichts anderes) von seinen Attributen charakterisiert werden und sich nicht auf andere Entities beziehen, werden von pathologischen Fällen abgesehen keine anderen funk- tionalen Abhängigkeiten als vom Schlüssel vorliegen. Und das genau sind dann normalisierte Relationen (in 4. NF). Höhere Normalformen (5. und 6. Normalform) führen zu einer großen Anzahl von Tabellen, die letztlich jeweils nur noch den Schlüssel und ein einziges Attribut enthalten. Diese Formen sind von Spezialfällen (etwa Data Warehouses) abgesehen weniger gebräuchlich, erfahren jedoch bei etwas freier Interpretation in Form von Column-Stores wieder neue Aufmerksamkeit. Zugriff auf Daten In diesem Kapitel werden die folgenden Beispieldaten verwendet²: mysql> SELECT * FROM DEPT; +--------+------------+----------+ | DEPTNO | DNAME | LOC | +--------+------------+----------+ | 10 | ACCOUNTING | NEW YORK | | 20 | RESEARCH | DALLAS | | 30 | SALES | CHICAGO | | 40 | OPERATIONS | BOSTON | +--------+------------+----------+ mysql> SELECT * FROM EMP; +-------+--------+-----------+------+------------+------+------+--------+ | EMPNO | ENAME | JOB | MGR | HIREDATE | SAL | COMM | DEPTNO | +-------+--------+-----------+------+------------+------+------+--------+ | 7369 | SMITH | CLERK | 7902 | 1980-12-17 | 800 | NULL | 20 | | 7499 | ALLEN | SALESMAN | 7698 | 1981-02-10 | 1600 | 300 | 30 | | 7521 | WARD | SALESMAN | 7698 | 1981-02-22 | 1250 | 500 | 30 | | 7566 | JONES | MANAGER | 7839 | 1981-04-02 | 2975 | NULL | 20 | | 7654 | MARTIN | SALESMAN | 7698 | 1981-09-28 | 1250 | 1400 | 30 | | 7698 | BLAKE | MANAGER | 7839 | 1981-05-01 | 2850 | NULL | 30 | | 7782 | CLARK | MANAGER | 7839 | 1981-06-09 | 2450 | NULL | 10 | | 7788 | SCOTT | ANALYST | 7566 | 1982-12-09 | 3000 | NULL | 20 | | 7839 | KING | PRESIDENT | NULL | 1981-11-17 | 5000 | NULL | 10 | | 7844 | TURNER | SALESMAN | 7698 | 1981-09-08 | 1500 | 0 | 30 | | 7876 | ADAMS | CLERK | 7788 | 1983-01-12 | 1100 | NULL | 20 | | 7900 | JAMES | CLERK | 7698 | 1981-12-03 | 950 | NULL | 30 | | 7902 | FORD | ANALYST | 7566 | 1981-12-03 | 3000 | NULL | 20 | | 7934 | MILLER | CLERK | 7782 | 1982-01-23 | 1300 | NULL | 10 | +-------+--------+-----------+------+------------+------+------+--------+ Die Relationenalgebra definiert bereits die möglichen Operationen auf Tabellen, die Schreibweise ist für den Alltagsgebrauch allerdings unpraktisch. Deshalb werden Query Languages verwendet. Ingres, ein frühes experimentelles Datenbanksystem aus den 70er Jahren von Michael Stone- breaker entwickelt, verwendete eine relativ direkte Abbildung der Relationenalgebra: ²Dieses Beispiel ist nicht willkürlich gewählt, sondern ein Klassiker. Diese Tabellen wurden in der Oracle-Dokumentation verwendet und sind insofern ausgesprochen elegant, als praktisch alle Features von SQL mit diesem simplen Modell erklärt werden können. Zugriff auf Daten 18 Abfragen RANGE OF t_1 IS R_1 RANGE OF t_i IS R_i RETRIEVE (t_1.A_1,..., t_i.A_i) WHERE P (t_1,..., t_i) hierbei sind Ti Tupelvariablen, Ri Relationen und Pi Prädikate. Daten hinzufügen APPEND TO emp (empno = 4711, ename = 'smith') Daten ändern RANGE OF e IS EMP REPLACE (sal = 1000) WHERE (deptno = 10) Eine aktuelle Version von Ingres ist frei im Quelltext verfügbar. RDB von DEC (inzwischen Oracle) hatte neben SQL ebenfalls eine ähnliche Abfragesprache. In den 70er Jahren wurde von IBM QBE (Query by Example) entwickelt, um Anwendern ohne Programmierkenntnissen die Abfrage von Daten zu ermöglichen. Diese Idee wurde in den achtziger Jahren von Borland bei Paradox wieder aufgegriffen, in den Neunzigern bei Access verwendet. Da die Daten als Tabellen vorliegen, wird eine grafische Darstellung erzeugt. Hier werden Felder ausgewählt, Kriterien eingetragen usw. Dieses Verfahren wurde bei vielen nicht relationalen Desktop-Datenbanksystemen wie Paradox oder Filemaker implementiert. Außerdem verwenden Reporting-Tools und BI-Systeme typischerweise QBE. Der heute übliche Standard bei relationalen Datenbanken ist SQL (Structured Query Language). SQL wurde von IBM Anfang der 70er Jahre als Abfragesprache für das „System R” entwickelt. SQL kann dabei einerseits interaktiv für Endanwender, andererseits auch als Programmier- schnittstelle (embedded SQL) verwendet werden. Aufgrund des großen Erfolges wurde SQL von allen Herstellern übernommen und standardisiert. Das bedeutet aber weder, dass alle Hersteller diesen aktuellen Standard unterstützen, noch, dass sie sich darauf beschränken. So ist SQL zwar im Prinzip datenbankunabhängig, aber erstens gibt es feine Unterschiede in der Semantik und zweitens sind die Erweiterungen der verschiedenen Hersteller (natürlich) nicht zueinander kompatibel. Soll ein System also von einem Datenbanksystem auf ein anderes umgestellt werden, bedeutet dies, je nach Architektur, unter Zugriff auf Daten 19 Umständen erhebliche Arbeit. Deshalb ist es sinnvoll, die systemabhängigen Teile zu isolieren um die Menge zu ändernden Codes klein zu halten. SQL beinhaltet drei Arten von Anweisungen, DML (Data Manipulation Language), DDL (Data Definition Language) und DCL (Data Control Language). SQL ist von der Struktur her eine einfache Sprache, der DML-Teil umfasst im Wesentlichen nur 4 Statements: SELECT, INSERT, UPDATE und DELETE. SELECT Die Select-Anweisung holt Datensätze aus der Datenbank. Genauer gesagt erzeugt SELECT eine neue, temporäre Tabelle aus einer oder mehreren bereits bestehenden durch Auswahl von Spalten und Zeilen. Grundstruktur SELECT [ALL|DISTINCT] Attributliste FROM R_1,..., R_n [WHERE Pr\\\"adikat] [ORDER BY A_1 [ASC|DESC] [\\{,A_i [ASC|DESC]\\}]] Attributliste ist eine durch Komma getrennte Liste der gewünschten Attribute. Wenn mehrere Tabellen kombiniert werden, wird den Attributen der Tabellen- oder Aliasname durch einen Punkt getrennt vorangestellt. Anstelle von Attributen können auch Ausdrücke, insbesondere Funktionen, die Attribute (genauer gesagt deren Werte) als Parameter erhalten. SELECT EMP.ename, DEPT.dname, DEPT.mgr SELECT EMP.ename AS nachname (Alias) Das Prädikat kann Attribute, Konstanten, Operatoren (=, <>, >, <, <=, >=,…, AND, OR, NOT) und Funktionen enthalten Eine vollständige Definition des SELECT Statements ist zu unübersichtlich, deshalb werden die wesentlichen Aspekte im folgenden anhand von Beispielen dargestellt. Alle Attribute aller Datensätze einer Tabelle SELECT * FROM emp Das Prädikat kann sehr komplex sein, beispielsweise wiederum SELECT-Anweisungen enthal- ten. In Ausdrücken sind die “üblichen” Operatoren verwendbar: Zugriff auf Daten 20 Namen der Mitarbeiter in DEPT 20 SELECT ename FROM emp WHERE deptno = 20 Es existieren jedoch zahlreiche datenbankspezifische Operatoren wie etwa BETWEEN: Alle Mitarbeiter mit Gehalt zwischen 2000 und 3000 SELECT ename FROM emp WHERE sal >= 2000 AND sal <= 3000 SELECT ename FROM emp WHERE sal BETWEEN 2000 AND 3000 Vergleich von Strings Für einen exakten Vergleich von Strings wird der “=”-Operator verwendet, dabei können keine Wildcards verwendet werden. Für diesen Fall existiert der LIKE-Operator: SELECT ename FROM emp WHERE ename LIKE 'smi%' % steht für eine beliebige Zeichenfolge _ steht für 1 beliebiges Zeichen z.B bei Umlauten wie Hänisch (mit Hä, oder Hae…?) WHERE ename LIKE 'H%nisch' JOINS Ein Join ist eine Verknüpfung mehrerer Tabellen über (gemeinsame) Attribute SELECT Attributliste FROM R_1,..., R_n WHERE R_j.A_jk = R_e.A_mn AND ... Wird der ‘=’-Operator verwendet, nennt man dies Equi-Join, sonst Non-Equi-Join Eine alternative Formulierung von Joins wurde im SQL 92-Standard definiert: SELECT Attributliste FROM R1 JOIN R2 ON R1.Ai = R2.Ak Rj.Ajk = Re.Amn JOIN Rn ON … Korrelationsvariablen dienen der Zuordnung von Attributen zu Relationen: Zugriff auf Daten 21 SELECT e.ename, d.danem FROM emp e, dept d WHERE e.deptno = d.deptno Dies ist insbesondere bei Self-Joins wichtig: Alle Mitarbeiter, die mehr verdienen als ihr Manager SELECT e.ename,e.sal FROM emp e, emp m WHERE e.mgr = m.empno AND e.sal > m.sal SUBQUERIES Die WHERE-clause kann sich auf eine andere (Sub-) Query beziehen. Formen: Alle Mitarbeiter mit demselben Job wie ‘Jones’ SELECT ename, job FROM emp WHERE job = ANY ( SELECT job FROM emp WHERE ename = 'Jones' ) Subqueries sind unter Umständen nicht unproblematisch, etwa die folgende, die alle Mitarbeiter, die mindestens einen anderen managen, ausgibt: SELECT ename, job FROM emp e WHERE EXISTS ( SELECT * FROM emp WHERE mgr = e.empno ) Diese Subquery muss für jeden Tupel der Hauptquery ausgeführt werden (da in der FROM- Clause der Subquery auf ein Attribut der Hauptquery Bezug genommen wird). Eine solche Correlated Subquery sorgt bei großen Datenmengen für extrem schlechte Perfor- mance und sollte deshalb nach Möglichkeit vermieden werden, d.h. nach Möglichkeit durch einen Join ersetzt werden. Hier werden Duplikate ausgegeben, diese werden mit DISTINCT eliminiert Ersatz einer Subquery durch einen JOIN SELECT DISTINCT m.ename, m.job FROM emp m, emp e WHERE e.mgr = m.empno Damit lassen sich aber nicht alle Probleme lösen, etwa die folgende Abfrage, die den Mitarbeiter mit dem höchsten Gehalt liefert: {title=”Nicht durch einen JOIN ersetzbare Subquery”, lang=sql} Zugriff auf Daten 22 SELECT ename, sal FROM emp e WHERE NOT EXISTS ( SELECT * FROM emp WHERE sal > e.sal ) Aggregate Functions Oft sind nicht die einzelnen Tupel, sondern nur eine Zusammenfassung des Inhalts von Interesse. Dies leisten Aggregatfunktionen. Zahl der Mitarbeiter SELECT COUNT (*) FROM emp Damit kann dann auch die vorhin problematische Subquery aufgelöst werden: {title=”Angestellter mit höchstem Gehalt”, lang=sql} SELECT ename, sal FROM emp WHERE sal >= ( SELECT MAX (sal) FROM emp ) Soll nun etwa das durchschnittliche Gehalt pro Abteilung bestimmt werden, reichen Aggregat- funktionen alleine nicht mehr aus, es müssen Gruppen gebildet werden. Dazu dient die GROUP BY Anweisung: Durchschnittliches Gehalt je Abteilung SELECT deptno, AVG (sal) FROM emp GROUP BY deptno GROUP BY bewirkt eine interne Teilmengenbildung, Aggregatfunktionen wirken jeweils auf diese Teilmengen. Mit WHERE werden einzelne Zeilen aus der Betrachtung eliminiert, mit HAVING können ganze Gruppen eliminiert werden. Durchschnittliches Gehalt je Abteilung mit mindestens 5 Mitarbeitern SELECT deptno, AVG (sal) FROM emp GROUP BY deptno HAVING COUNT (*) >= 5 Mengenoperationen mit Query Ergebnissen SELECT-Statements liefern Relationen (= Mengen) zurück, diese können mit den gebräuchlichen Mengenoperationen verknüpft werden. Verwendung finden diese Operationen etwa bei Spezial- isierungen. Nehmen wir etwa an, es gäbe die beiden Spezialisierungen Student und Dozent einer Person, dann könnten die gemeinsamen Eigenschaften (also eben die der Person) durch eine Vereinigung der beiden Abfragen auf Student und Dozent extrahiert werden: Zugriff auf Daten 23 Alle Personen (=Elemente der Oberklasse) SELECT Name FROM Student UNION SELECT Name FROM Dozent ORDER BY Name Zugriff auf Daten 24 DML (Data Manipulation Language) Mit den DML-Anweisungen werden Datensätze verändert. ### Insert Die Insert-Anwesiung fügt Datensätze zu einer Tabelle hinzu Einfügen einer neuen Person INSERT INTO Person (Name, Vorname) VALUES ('Haenisch', 'Till') Dabei können nicht nur einzelne Zeilen angelegt werden, sondern auch die Ergebnisse einer Abfrage auf einer anderen Tabelle: Erzeuge Personen-Einträge für jeden EMP INSERT INTO Person (Name) SELECT ename FROM emp Delete Die DELETE-Anweisung löscht Datensätze aus einer Tabelle Löschen aller Angestellten, die mehr verdienen als ihr Chef DELETE FROM emp WHERE empno IN ( SELECT e.empno FROM emp e, emp m WHERE e.mgr = m.empno AND e.sal > m.sal ) Eine häufig benötigte Funktion ist das Löschen von Duplikaten: Doppelte Personen löschen DELETE FROM Person WHERE id = ( SELECT MAX (id) FROM Person GROUP BY Name HAVING COUNT (*) > 1 ) Update Der Update-Befehl ändert existierende Tupel Zugriff auf Daten 25 Erhöhen des Gehalts aller Angestelten um 10%, die weniger als Durchschnitt verdienen: UPDATE emp SET sal = 1.1 * sal WHERE sal < ( SELECT AVG (sal) FROM emp ) Während des Updates ändert sich der Durchschnitt!!! Daher: Die Subquery wird vor dem UPDATE durchgeführt. MySQL MySQL ist neben PostgreSQL eines der am weitesten verbreiteten relationalen Datenbanksys- teme. Es gibt neben den kommerziellen Lizenzen, die von Oracle vertrieben werden, auch eine Community-Edition, die frei verfügbar ist. Das wesentliche Merkmal der MySQL-Architektur ist die Möglichkeit, verschiedene Storage- Engines zur Speicherung der Tabellendaten zu verwenden. Die Auswahl erfolgt pro Tabelle. MySQL-Architektur (Aus dem MySQL Reference Manual) Bis zur Version 5.5 wurde die MyISAM-Engine als Standard verwendet, diese wurde speziell für MySQL entwickelt und war für dessen Erfolg mit verantwortlich. MyISAM unterstützt keine Transaktionen und kann die Daten nur auf Tabellenebene sperren. Dafür ist die Performance sehr gut. MyISAM verwendet normalerweise Btrees als Index, kann aber auch einen Volltextindex verwenden. MySQL 27 Vergleich der wesentlichen Eigenschaften der MySQL Storage Engines. [http://dev.mysql.com/doc/refman/5.1/en/storage-engines.html] Feature MyISAM Memory InnoDB Archive NDB Storage limits 256TB RAM 64TB None 384EB Transactions Nein Nein Ja Nein Ja Locking granularity Table Table Row Table Row MVCC Nein Nein Ja Nein Nein Geospatial data type support Ja Nein Ja Ja Ja Geospatial indexing support Ja Nein Nein Nein Nein B-tree indexes Ja Ja Ja Nein Ja Hash indexes Nein Ja Nein Nein Ja Full-text search indexes Ja Nein Ja Nein Nein Clustered indexes Nein Nein Ja Nein Nein Data caches Nein N/A Ja Nein Ja Index caches Ja N/A Ja Nein Ja Compressed data Ja Nein Ja Ja Nein Encrypted data Ja Ja Ja Ja Ja Cluster database support Nein Nein Nein Nein Ja Replication support Ja Ja Ja Ja Ja Foreign key support Nein Nein Ja Nein Nein Backup / point-in-time recovery Ja Ja Ja Ja Ja Query cache support Ja Ja Ja Ja Ja Update statistics for data dictionary Ja Ja Ja Ja Ja MyISAM unterstützt im Gegensatz zu InnoDB keine Transaktionen und keine Locks, so dass sie sich im wesentlichen für Anwendungen eignet, bei denen die Daten verhältnismäßig selten geschrieben, aber oft gelesen werden. Gerade Internet-Anwendungen wie Content- Management-Systeme oder Online Shops haben genau diese Eigenschaft, so dass leicht ver- ständlich ist, warum MySQL hier oft eingesetzt wird. Für OLTP-Systeme ist MyISAM dagegen nicht geeignet. Dies ist einer der Gründe, warum InnoDB inzwischen die Default-Storage-Engine ist. Die relative Performance der verschiedenen Storage-Engines hängt von vielen Faktoren, ins- besondere von der Art der Zugriffe ab. Jede Messung ist hier mit Vorsicht zu interpretieren. Letztlich muß die Performance mit der eigenen Anwendung und den eigenen Daten untersucht werden. Für Spezialanwendungen (etwa die Speicherung von Session-Daten bei Web-Anwendungen) ist die Memory-Engine interessant: Hier werden die Daten vollständig im Hauptspeicher gehalten und nicht auf Festplatte gesichert. Das bedeutet natürlich, dass die Daten beim Herunterfahren (oder Absturz) des Datenbanksystems verloren gehen, für transiente Daten spielt dies aber keine Rolle. Dafür ist hier die Performance, insbesondere bei Zugriffen, die ausschließlich über den Primärschlüssel erfolgen, unter Umständen sehr gut. Sollen oder müssen die Daten über mehrere Rechner verteilt werden, kann die NDB-Cluster- MySQL 28 Engine eingesetzt werden. Bei solchen Anwendungen werden heute gerne sogenannte NoSQL- Datenbanken verwendet, das bedeutet aber, dass ein weiteres System erlernt, installiert und gewartet werden muß. Deshalb sollte zunächst geprüft werden, ob das Problem auch durch eine geeignete Storage-Engine gelöst werden kann. Es gibt weitere spezielle Storage-Engines wie etwa die CSV-Engine, die die Daten in einer CSV- Datei (Comma Separated Values) speichert. Dies erleichtert unter Umständen den Import oder Export von Daten. Die Storage-Engine kann beim Anlegen einer Tabelle oder nachträglich mit dem ENGINE- Parameter festgelegt werden: ALTER TABLE X ENGINE=MyISAM; MySQL 29 Installation Zur Installation von MySQL laden Sie zunächst die aktuelle Version der Community-Edition von www.mysql.com herunter. Im Folgenden wird die Version 5.5. unter Microsoft Windows verwendet. Nach der Installation und der Ausführung des Server Instance Configuration Wizards (hierbei können die Standard-Werte akzeptiert werden, nur für „ernsthafte” Anwendungen sind Anpassungen erforderlich) kann der MySQL-Server verwendet werden. Zunächst sollte die Funktion geprüft werden, indem in der Eingabeaufforderung (Start->suchen- >cmd) der mysql-Client gestartet wird. mysql Client, Anlegen eines neuen Benutzers Nach dem Parameter –u wird der Benutzername angegeben, -p bedeutet, dass dieser Benutzer ein Kennwort benötigt, das nach der Aufforderung eingegeben werden muß. Nach der Installation existiert nur der Benutzer root mit dem im Configuration Wizard angegebenen Kennwort. Dieser hat alle Rechte, deshalb sollte im Normalfall mit einem anderen Benutzer gearbeitet werden. Soll das Kennwort des root-Benutzers geändert werden, kann das mit dem mysqladmin-Programm durchgeführt werden. mysqladmin ist ein reines Administrationstool, um Datenbanken anzulegen (create), bestehende Datenbanken zu löschen (drop), den Server-Status abzufragen (status) oder Passwörter zu ändern (password). Eine Übersicht der mysqladmin-Befehle finden Sie in der Online-Dokumentation von MySQL. Ein neuer Benutzer wird mit dem Befehl create user erzeugt. MySQL bietet die Möglichkeit, einen Benutzer nur für bestimmte Client-Rechner zuzulassen. Dies erhöht die Sicherheit des Systems erheblich. Im einfachsten Fall greift der Benutzer nur vom selben Rechner aus zu, auf dem der MySQL-Server läuft, als Rechnername wird dann localhost verwendet. MySQL 30 Im nächsten Schritt wird die Datenbank url angelegt, die in den weiteren Beispielen verwendet wird. Damit der neue Benutzer mit dieser Datenbank arbeiten kann, müssen ihm entsprechende Rechte eingeräumt werden. Hier erhält der Benutzer till alle Rechte auf alle Objekte der Datenbank url. Dies ist die normale Form für den Eigentümer der Datenbank bzw. Anwendung. Für andere Benutzer können differenzierte Rechte definiert werden, Details dazu später. Um die bisher durchgeführten Operationen zu testen, melden wir uns mit dem neuen Benutzer an der Datenbank an: Anmelden an mysql, Erzeugen einer Tabelle Zunächst muß die Datenbank ausgewählt werden, mit der gearbeitet werden soll, dazu dient der Befehl use. Mit show tables können die vorhandenen Tabellen angezeigt werden, bisher keine. Um die Berechtigungen zu testen, legen wir eine Tabelle beispiel mit zwei Spalten an. Damit ist die Installation abgeschlossen und getestet. In den folgenden Kapiteln werden wesentliche Eigenschaften von MySQL am Beispiel dargestellt. Dazu wird der mysql-Client verwendet. Es existieren zwar grafische Oberflächen zur Erstellung und Verwaltung von Datenbanken, Tabellen, Benutzern und Rechten doch stehen diese nicht immer zur Verfügung, etwa aus Sicherheitsgründen. Die wichtigsten GUI-Tools für MySQL sind phpMyAdmin und MySQL Workbench. phpMyAdmin ist eine Oberfläche zur Administration von MySQL. Über ein Browser-Interface können Datenbankobjekte wie Tabellen verwaltet werden. MySQL 31 Darstellung einer Tabelle in phpMyAdmin phpMyAdmin wird von vielen Web-Hostern zur Verfügung gestellt und kann ansonsten, da eine PHP-Anwendung, leicht selber installiert werden. phpMyAdmin ist das bevorzugte Werkzeug zur Verwaltung von MySQL-Datenbanken, die in Web-Anwendungen integriert sind. MySQL Workbench ist eine Desktop-Anwendung zur Verwaltung und zum Entwurf von MySQL Datenbanken. MySQL 32 Oberfläche von MySQL Workbench MySQL Workbench bietet aber nicht nur Funktionen zur Verwaltung, sondern auch zum Entwurf von Datenbanken. Aus den erstellten ER-Modellen kann dann eine Datenbank generiert werden. Alternativ kann aus einer existierenden Datenbank ein ER-Modell generiert werden („Reverse Engineering”). MySQL 33 Datenmodell der Beispieldatenbank in MySQL Workbench Beide Werkzeuge sind kostenlos verfügbar. Die Werkzeuge mysql und mysqladmin sind die klassischen, mitgelieferten Werkzeuge zum Arbeiten mit MySQL. Als Kommandozeilen-Werkzeuge werden sie v.a. von Administratoren eingesetzt, benötigen wenig Speicher, keine weiteren Werkzeuge (Browser) und Installationen und können gut für automatisierte Aufgaben eingesetzt werden. Beim Aufruf können Sie mit dem Parameter -u einen Benutzernamen mitgeben, mit -p ein Passwort, mit -h einen Rechnernamen sowie als letzten Parameter den Namen der Datenbank, somit: mysql -u root -p password -h localhost url. Die Datenbank wird mit use datenbankname ausgewählt. mysqladmin erledigt typische Administrationsaufgaben wie Datenbanken erzeugen, löschen, den Server-Status abfragen oder Passwörter ändern. Grafische Werkzeuge wie phpMyAdmin oder MySQL Workbench können die Arbeit erleichtern, sind aber nicht immer einsetzbar. MySQL 34 Realisierung eines Datenmodells in MySQL Bei der Entwicklung von Datenbankanwendungen geht man üblicherweise so vor, dass zunächst ein konzeptionelles Datenmodell erstellt wird, meist in Form eines Entity-Relationship-Modells (ERM). Dieses enthält Informationen über die beteiligten Entities und deren Beziehungen. Dieses Modell ist nicht spezifisch für eine bestimmte Art von Datenbanksystem, sondern eine sehr abstrakte Art der System-Modellierung. Eine Alternative zum ERM ist die Verwendung von UML. Im nächsten Schritt wird dieses abstrakte Modell in ein relationales Modell umgewandelt. Dieses ist noch nicht spezifisch für ein bestimmtes Datenbanksystem (also etwa MySQL oder Oracle), enthält aber nur noch Relationen, die im nächsten Schritt, dem physischen Modell auf Tabellen abgebildet werden. Damit beschäftigt sich dieses Kapitel. Dazu wird als Beispiel ein stark vereinfachtes Modell einer Bank verwendet: Konzeptionelles Datenmodell einer Bank Die Bank hat Kunden, die einen Namen und eine eindeutige ID haben. Zusätzlich wird gespeichert, seit wann sie Kunden bei dieser Bank sind. Jeder Kunde hat ein oder mehrere Konten. Jedes Konto hat eine eindeutige Nummer, den aktuellen (Konto-) Stand und ein Kreditlimit. Ein Konto hat einen Typ (etwa Giro). Dieses Modell muss jetzt in ein logisches Datenmodell umgewandelt werden: Logisches Datenmodell Da der Typ keine weiteren Attribute hat, wird er direkt als Attribut in die Tabelle Konto aufgenommen. Die 1:N-Beziehung zwischen Konto und Kunde wird so realisiert, dass die Tabelle Konto das Attribut Kunde_id als Fremdschlüssel erhält. Als Primärschlüssel wird Kunde(ID) und Konto(Nummer) verwendet. MySQL 35 Um das physische Datenmodell zu erhalten, das dann in SQL-Code ausgedrückt wird, müssen • Datentypen für die einzelnen Attribute festgelegt werden • Konsistenzbedingungen definiert werden, die Business rules abbilden • Je nach Datenbanksystem gegebenfalls Speicher- und Zugriffsstrukturen festgelegt werden • Berechtigungskonzepte umgesetzt werden Diese Liste ist nicht vollständig, es gibt hier keine eindeutige Trennlinie, wo die Modellierung aufhört und der Betrieb/das Tuning der Datenbank anfängt beziehungsweise welche dieser Aspekte (etwa Konsistenzsicherung) in der Anwendungsarchitektur berücksichtigt werden sollen. In den folgenden Abschnitten werden diese Schritte näher erläutert. MySQL Datentypen Neben Strings fester und variabler Länge sowie ganzen Zahlen unterstützen relationale Daten- banksysteme noch Fließkommazahlen, Zeit- und Datumswerte sowie spezielle Typen für große Objekte. Am wichtigsten sind jedoch die ganzen bzw. Festkommazahlen und Strings. Numerische MySQL Datentypen Datentyp Art Wertebereich, Besonderheiten BIT(n) Bitfeld N Bits, max. 64, default 1 BOOLEAN TINYINT(1), 0=false TINYINT Ganze Zahl 8 Bit SMALLINT Ganze Zahl 16 Bit INT, INTEGER Ganze Zahl 32 Bit BIGINT Ganze Zahl 64 Bit DECIMAL (m,n) Festkommazahl m Stellen, n Nachkommastellen FLOAT Fließkommazahl 32 Bit DOUBLE Fließkommazahl 64 Bit Gebräuchlich sind hier vor allem INTEGER für IDs und allgemein ganze Zahlen sowie DECIMAL- Typen für Zahlen einer bestimmten Genauigkeit. Dies ist vor allem für Geldbeträge wichtig, hier wird typischerweise ein DECIMAL-Typ, etwa DECIMAL(15,2) verwendet. Dies bedeutet, dass der Betrag mit insgesamt 15 Stellen, davon 2 Nachkommastellen dargestellt wird. Fließkommazahlen werden eher selten verwendet, da Rundungsfehler auftreten können. Die Ganzzahltypen können auch als UNSIGNED, also vorzeichenlos definiert werden, hier muß jedoch darauf geachtet werden, dass dies Konsequenzen bei Berechnungen hat. So ist das Ergebnis einer Berechnung, an der ein UNSIGNED-Wert beteiligt ist, immer UNSIGNED usw. Diese sollten also nur verwendet werden, wenn sich dadurch tatsächlich ein signifikanter Nutzen ergibt. MySQL 36 MySQL Datums- und Zeit-Typen Datentyp Verwendung DATE Datum DATETIME Datum und Uhrzeit TIMESTAMP Eine Zeitangabe, Sekunden seit dem 1.1.1970 00:00 Uhr TIME Zeit in Stunden, Bereich zwischen -838:59:59 und +838:59:59 Stunden YEAR(4) Jahr, vierstellig DATE beziehungsweise DATETIME werden verwendet, um ein bestimmtes Datum mit oder Uhrzeit zu speichern. Das Standardformat ist hier YYYY-MM-DD HH:MM:SS. TIMESTAMP wird typischerweise verwendet, um das Datum der letzten Modifikation, Erzeugung usw. zu speichern. Diese werden üblicherweise automatisch belegt. MySQL setzt die erste TIMESTAMP-Spalte einer Zeile beim Anlegen oder Ändern auf das aktuelle Datum/Uhrzeit, wenn kein Wert für diese Spalte angegeben wird. Für Einzelheiten siehe [mysqlref]. Die Darstellung von Datums- und Zeitwerten als DATE bzw. DATETIME-Typen ist jedoch nicht ohne Tücken: Die Darstellung hängt vom jeweiligen Zeit- und Datumsformat ab, Zeitzonen können nicht erfasst werden und es ist keine Genauigkeit definierbar (also z.B. „(irgendwann im) Mai 2012”). Unter Umständen ist es deshalb sinnvoll, Datumsangaben als Strings zu speichern, dazu sollte dann die Darstellung nach ISO 8601 verwendet werden, um die richtige Sortierreihenfolge zu erhalten. String-Datentypen bei MySQL Datentyp Verwendung CHAR(n) String fester Länge n Zeichen, maximal 255 Zeichen, wird ggf. mit Leerzeichen aufgefüllt VARCHAR(n) String variabler Länge, maximal n Zeichen BINARY/VARBINARY Wie CHAR, VARCHAR, allerdings Binärwerte ENUM(value1,value2…) Ein String, der nur Werte aus der angegebenen Liste annehmen kann, Intern ein INTEGER. TEXT/BLOB Langer Text/Binärstring mit max. 65535 Zeichen/Bytes MEDIUMTEXT/MEDIUMBLOB Langer Text/Binärstring mit max. etwa 16 Millionen Zeichen/Byte (2ˆ24 – 1) LONGTEXT/LONGBLOB Langer Text/Binärstring mit max. 4 GB Der Typ CHAR(n) wird typischerweise für alphanumerische Kennungen verwendet, etwa Auftrags- oder Artikelnummern, die nicht nur aus Zahlen bestehen und eine feste Länge haben. Eine weitere wichtige Anwendung ist die Nutzung eines CHAR(1) als Boolean (J/N)-Variable. Für Texte wird normalerweise der VARCHAR-Type verwendet. Dessen maximale Länge ist allerd- ings je nach Datenbanksystem beschränkt. Bei MySQL darf eine Zeile (alle Spalten zusammen) einer Tabelle maximal 65535 Byte belegen. Werden die Zeichen im utf8-Code dargestellt, der MySQL 37 Standard bei MySQL, dann kann die maximale Länge in Zeichen nicht sicher angegeben werden, da die Darstellung je nach Zeichen ein bis drei Byte benötigt. Lange Texte sollten deshalb nicht als VARCHAR dargestellt werden. Dafür sind die BLOB bzw. TEXT- Typen gedacht. Allerdings gibt es hier Einschränkungen beim Suchen und Sortieren, hier wird beispielsweise nur der Anfang der Daten berücksichtigt. Generell sollten TEXT/BLOB-Attribute nur als Ganzes vom Anwendungsprogramm ausgewertet werden. Datentypen für die Beispielanwendung Das einzige Stringfeld der Beispielanwendung ist der Name des Kunden, hier verwenden wir VARCHAR als Datentyp. Die Länge wählen wir etwas willkürlich zu 50 Zeichen. In einer realistischen Anwendung wäre der Name komplexer aufgebaut, würde vermutlich zumindest aus dem Nachnamen, dem Vornamen, einem Namenszusatz und einem Titel bestehen. Hier wäre die Diskussion um die Feldlängen dann komplexer. Letztlich gibt es hier aber zwei unterschiedliche Ansätze: So kurz wie nötig oder so lang wie möglich. Wenn etwa der Nachname eines Kunden dargestellt werden soll, lassen sich mit einer Feldlänge von 30 Zeichen praktisch alle in der Praxis vorkommenden Fälle darstellen (Adelwandsteiner- Siebenbeutel hat nur 28 Zeichen und ist damit ein Zeichen länger als das gerne verwendete Beispiel Leutheusser-Schnarrenberger). Aber natürlich sind Fälle konstruierbar, die länger werden könnten. Warum also nicht gleich 255 Zeichen? Das wäre der zweite Ansatz. Der braucht zwar auch nicht mehr Speicherplatz – belegt wird bei einem VARCHAR immer nur der tatsächliche Bedarf – kann aber die Anwendungsen- twicklung erheblich erschweren: Alle Felder in Masken und Reports müssen dann zwei Längen haben, nämlich die maximal dargestellte Länge und die tatsächliche Maximallänge. Es müssen Vorkehrungen getroffen werden, wie genau in übersichtlicher Form angezeigt wird, wenn ein Datensatz über die sichtbare Feldlänge hinaus geht usw. Insbesondere bei spaltenorientierten Reports erfordert dies unter Umständen einen erheblichen Aufwand. Letztlich läßt sich die Frage nach der richtigen Feldlänge – kurz oder lang – nur schwer allgemeingültig beantworten, dies muss im Einzelfall abhängig von der Anwendung entschieden werden. Für die ID des Kunden verwenden wir INTEGER als Datentyp. Die ID ist ein interner Schlüssel, hier verwenden wir eine fortlaufende Nummer. Dies kann in MySQL mit AUTO_INCREMENT realisiert werden. Hat ein Attribut beispielsweise den Datentyp INTEGER AUTO_INCREMENT werden die Werte automatisch fortlaufend erzeugt, wenn beim INSERT für dieses Attribut kein Wert angegeben wurde. Es wäre aber falsch, davon auszugehen, dass mit dieser Methode (exakt) fortlaufende Nummern erzeugt werden: Wenn beispielsweise in einer Transaktion ein Datensatz angelegt wird und diese Transaktion dann einen ROLLBACK durchführt, wird der AUTO_INCREMENT-Wert nicht zurückgesetzt, hier entsteht also eine Lücke. Dieses Verhalten ist nicht MySQL-spezifisch, ein anderes Verhalten ließe sich nur erreichen, wenn für die Dauer der Transaktion alle anderen INSERTs auf diese Tabelle verhindert würden, was kaum wünschenswert ist. Das Attribut seit, das angibt, seit wann eine Person Kunde dieser Bank ist, wird als DATE abgebildet. MySQL 38 Die Kontonummer würde in einer realen Anwendung eine „sprechende” Nummer sein, also aus mehreren Komponenten bestehen. Zur Darstellung einer deutschen Kontonummer könnte ein CHAR(10) verwendet werden. Kontostand und Kreditlimit werden als Festkommazahlen mit einer Genauigkeit von einem Cent dargestellt, also mit zwei Nachkommastellen. Wieviele Stellen insgesamt erforderlich sind, hängt davon ab, wieviel Geld ein Kunde maximal auf seinem Konto haben kann. Gehen wir davon aus, dass der Bundeshaushalt etwa 300 Milliarden beträgt, sollten 12 Stellen vor dem Komma ausreichend sein. Als Datentyp wählen wir also DECIMAL(14,2). Für den Kontentyp wählen wir ENUM als Datentyp, so dass gewährleistet ist, dass keine ungültigen Werte eingetragen werden können. Bis hierher sieht unser SQL zur Erzeugung der Tabellen also so aus: CREATE TABLE Kunde ( ID INTEGER, Name VARCHAR(30), seit DATE); CREATE TABLE Konto ( Nummer CHAR(10) PRIMARY KEY, Stand DECIMAL(14,2), Limit DECIMAL(14,2), Type ENUM('Giro','Spar','Kredit'), Kunde_ID INTEGER); Versuchen wir, die Tabellen in MySQL so anzulegen, stellen wir fest, dass ein Attribut mit Namen Limit nicht akzeptiert wird, dies ist ein reserviertes Wort von MySQL. Es gibt zwei Möglichkeiten, dieses Problem zu lösen, entweder man verwendet einen anderen Namen für das Attribut (wir werden im Folgenden KreditLimit verwenden) oder man setzt den Namen des Attributs immer in „Backticks”, die einfachen Anführungszeichen, die von links oben nach rechts unten gehen. Zusammenfassung MySQL bietet eine Vielzahl von Datentypen, für uns sind die folgenden wesentlichen Datentypen interessant: Integer-Datentypen: TINYINT, SMALLINT, MEDIUMINT, INT oder INTEGER, BIGINT, SE- RIAL, Fließkommazahlen: FLOAT, REAL oder DOUBLE, Datum und Uhrzeit: DATE, TIME, DATETIME, YEAR, TIMESTAMP, Zeichenketten: CHAR, VARCHAR sowie Datentypen für Binärdaten (BINARY, BLOB), Aufzählungen (ENUM), Mengen (SET) und spezielle räumliche Datentypen. MySQL 39 Integritätsbedingungen Relationale Datenbanken bieten eine Reihe von Möglichkeiten, um die Integrität der Daten zu sichern. Zunächst können auf der Ebene einzelner Attribute Bedingungen definiert werden, etwa ob ein Wert für das Attribut angegeben sein muss, ob es sich um einen Schlüssel handelt, oder ob der angegebene Wert ein für dieses Attribut gültiger Wert ist. Innerhalb einer Tabelle lassen sich weitere Integritätsbedingungen formulieren, etwa dass ein Attribut der Primärschlüssel der Tabelle ist oder Gültigkeitsbedingungen, die mehrere Attribute umfassen. Schließlich lassen sich über Tabellen hinweg Integritätsbedingungen formulieren, die die referentielle Integrität, also Fremdschlüsselbeziehungen, sichern. Außerdem können in Form von Triggern beliebige eigene Bedingungen definiert werden. CONSTRAINTS auf Attributebene Am wichtigsten ist hier die Definition von Attributen, für die Werte angegeben werden müssen und die Definition von Primärschlüsseln. Dies wird durch Column-Constraints realisiert, die nach dem Datentyp eines Attributs angegeben werden können. MySQL Column Constraints Constraint Bedeutung [NOT] NULL Wert darf [nicht] NULL sein DEFAULT value Wird kein Wert angegeben, erhält das Attribut den Wert value UNIQUE Wert des Attributs ist eindeutig, ist also ein candidate key PRIMARY KEY Attribut ist Primärschlüssel Im Allgemeinen sollte für jedes Attribut explizit festgelegt werden, ob ein Wert angegeben werden muß oder nicht. Besteht der Primärschlüssel aus mehr als einem Attribut, dann kann er auch mit einem Table Constraint definiert werden. Damit sieht unser Beispiel so aus: CREATE TABLE Kunde ( ID INTEGER AUTO_INCREMENT PRIMARY KEY, Name VARCHAR(30) NOT NULL, seit DATE); CREATE TABLE Konto ( Nummer CHAR(10) PRIMARY KEY, Stand DECIMAL(14,2) NOT NULL, KreditLimit DECIMAL(14,2) NOT NULL, Type ENUM('Giro','Spar','Kredit') NOT NULL, Kunde_ID INTEGER NOT NULL); MySQL 40 Integritätsbedingungen auf Tabellenebene Hier können verschiedene Integritätsbedingungen definiert werden, etwa Schlüssel, die mehr als ein Attribut umfassen. Dies sind Spezialfälle, die in der MySQL-Dokumentation beschrieben sind. Für uns interessant sind CHECK-Constraints: Diese definieren Gültigkeitsbedingungen für Attributwerte. Eine wichtige Anwendung ist die Kontrolle der Werte bei selbst definierten bool’schen Typen, etwa Antwort CHAR(1) Steht in diesem Attribut der Wert J soll dies für Ja stehen, N steht für Nein. Was passiert aber, wenn ein ungültiger Wert, etwa j eingegeben wird? Dies verhindert ein CHECK Constraint CHECK (Antwort IN ('J', 'N')) Leider ignoriert MySQL diese Constraints. Hier muss eine solche Bedingung auf einem anderen Weg, etwa als ENUM Typ umgesetzt werden. Komplexere CHECKs, etwa ALTER TABLE Konto ADD CONSTRAINT CHECK ((Stand >= 0) OR (ABS(Stand) <= Kreditlimit)); können zwar definiert werden, werden von MySQL aber nicht geprüft. Referentielle Integrität Bei der Abbildung auf Tabellen werden die Beziehungen zwischen Entities als Attribute (Fremd- schlüssel) dargestellt. Das bedeutet, dass die Beziehungen nicht als Objekte in der Datenbank existieren. Werden sie nicht in geeigneter Form dargestellt, geht diese wichtige Information verloren und schlimmer noch, die Integrität ist nicht mehr gesichert. In unserem Beispiel muss etwa dafür gesorgt werden, dass jedes Konto zu einem (existierenden) Kunden gehört. Wird der zugehörige Kunde gelöscht, muss eine geeignete Aktion erfolgen: Entweder wird das Löschen des Kunden verhindert oder das Konto wird mit gelöscht. Nur so bleibt die Datenbank in einem konsistenten Zustand. Dies wird durch FOREIGN KEY Constraints erreicht: CREATE TABLE Konto ( Nummer CHAR(10) PRIMARY KEY, Stand DECIMAL(14,2) NOT NULL, KreditLimit DECIMAL(14,2) NOT NULL, Type ENUM('Giro','Spar','Kredit') NOT NULL, Kunde_ID INTEGER NOT NULL, CONSTRAINT FK_Kunde FOREIGN KEY (Kunde_ID) REFERENCES Kunde(ID)); MySQL 41 Versuchen wir nun, einem existierenden Konto den Kunden zu entziehen, geht das nicht: Verletzung eines FOREIGN KEY Constraints Je nachdem, welches Verhalten für die jeweilige Anwendung sinnvoll ist, sind verschiedene Aktionen definierbar. Der Default ist für unser Beispiel genau das richtige Verhalten: Bevor ein Kunde gelöscht werden kann, müssen alle abhängigen Konten gelöscht werden. Abhängig von der Anwendung kann das aber anders sein, so kann etwa das Verhalten bei DELETE und UPDATE unterschiedlich sein, interessant sind hier neben dem Default-Verhalten die Aktionen CASCADE und SET NULL: CONSTRAINT FK_Kunde FOREIGN KEY (Kunde_ID) REFERENCES Kunde(ID) ON DELETE CASCADE Diese Form würde das Konto mit löschen, wenn ein Kunde gelöscht wird. CONSTRAINT FK_Kunde FOREIGN KEY (Kunde_ID) REFERENCES Kunde(ID) ON UPDATE CASCADE Hier würde bei einem Update der ID in der Tabelle Kunde der Wert im Attribut Kunde_ID in der Tabelle Konto mit geändert. Dies ist sinnvoll, wenn sprechende Schlüssel verwendet werden, also etwa die email-Adresse: Ändert sich diese, muss sie in allen abhängigen Tabellen mit geändert werden. Das erfolgt am besten automatisch. FOREIGN KEY Constraints werden nur von der InnoDB-Engine unterstützt. Damit ist das physische Datenmodell unseres Beispiels fertig: MySQL 42 Umsetzung des Beispiels in MySQL Zusammenfassung MySQL bietet die im Rahmen von SQL üblichen Mechanismen zur Definition von Primärschlüs- seln (CREATE TABLE … PRIMARY KEY) und Fremdschlüsseln (FOREIGN KEY). In Verbindung mit der InnoDB überwacht MySQL auch die Referenzielle Integrität zwischen Fremdschlüssel- Attributen. Weitere Mechanismen bei Verletzung der Referenziellen Integrität können mit den entsprechenden SQL-Kommandos (ON DELETE / ON UPDATE) verwaltet werden. MySQL 43 Indices Indices dienen der Beschleunigung von Zugriffen auf einzelne Datensätze. Wie bei einem Buch ist es bei einer Datenbank sehr mühsam, eine bestimmte Stelle ohne Verwendung von Indices (Verzeichnissen) zu finden. Ein Buch müsste Seite für Seite durchgeblättert werden, eine Datenbank wird Datensatz für Datensatz durchsucht. Ein Index einer Datenbank entspricht in etwa dem Stichwortverzeichnis eines Buchs: Dieses ist in geeigneter Weise sortiert und erlaubt es, einen gewünschten Begriff schnell zu lokalisieren. Im Index ist ein Verweis auf die Seite (die Datensatznummer) enthalten. Eine Datenbank enthält im Gegensatz zu einem Buch im Allgemeinen mehrere Indices, diese können je nach Bedarf angelegt werden. In einem Lexikon sind die einzelnen Artikel sortiert, dies entspricht einem clustered Index einer Datenbank: Hier werden alle Datensätze einer Tabelle in einer Indexstruktur gespeichert. MySQL speichert alle Indices außer Geodaten als BTrees. Außerdem kann die Memory-Engine auch Hash-Indices verwenden. Die InnoDB-Engine legt bei Bedarf (und wenn genügend Haupt- speicher verfügbar ist) zusätzliche Hash-Indices im Hauptspeicher an, sogenannte Adaptive Hash Indices. Diese werden allerdings nur dann erzeugt, wenn bereits ein entsprechender BTree-Index existiert. Bitmap-Indices werden nicht unterstützt. Die MyISAM-Engine kann auch Indices auf BLOB/TEXT-Attribute erzeugen, allerdings nicht auf den kompletten Feldinhalt, sondern nur auf den vorderen Teil. Die Größe des zu indizierenden Bereichs muss bei der Erstellung des Indexes angegeben werden. Indices auf Primärschlüssel und Candidate Keys (UNIQUE-Constraint) werden automatisch angelegt. Sollen zusätzliche Indices verwendet werden, kann dies entweder beim Anlegen der Tabelle in der CREATE TABLE-Anweisung oder später durch CREATE INDEX bzw. ALTER TABLE erfolgen. Die InnoDB-Engine speichert die Tabellendaten als Clustered Index, das bedeutet, die Daten werden im (ersten) Index gespeichert. Das bedeutet, dass jede Tabelle zwingend einen Index haben muss. Wird vom Anwender kein Index definiert, wird automatisch eine Spalte ROW_ID zur Tabelle hinzugefügt, diese wird dann als Primärschlüssel verwendet (ähnlich wie bei Oracle). Statt dieses Attribut zu verwenden sollte der Anwender also lieber selber einen (sinnvollen) Primärschlüssel definieren. Bitte beachten Sie, dass Indices auch einen Nachteil haben: Sie machen ein Datenbanksystem zunächst einmal langsamer, weil bei jedem UPDATE/INSERT/DELETE-Befehl auch der Index aktualisiert sowie ggf. komplett neu aufgebaut werden muss. Somit ist ein Index nur wirklich in den Fällen sinnvoll, in denen in einer Datenbank mehr Abfragen als Änderungen in den Tabellen erfolgen. MySQL legt bei der Definition einer Tabelle einen Index auf dem Primärschlüssel an. Bei der Definition von Fremdschlüsseln legt MySQL analog die entsprechenden Indices an. Für den Datenbank-Administrator, der weitere Indices definieren will, bietet MySQL die üblichen SQL- Kommandos (CREATE INDEX name ON tabelle(attributliste) MySQL 44 Berechtigungen Der SQL-Standard verwendet ein Discretionary-Access-Control-Schema, das heisst, der Zugriff auf ein Objekt wird (allein) aufgrund der Identität des Zugreifenden ermittelt, Rechte werden für Benutzer definiert. Diese Berechtigungen können mit den DCL-Kommandos (Data Control Language) definiert werden. Dazu wird die GRANT Anweisung verwendet, die ein Recht (etwa INSERT, UPDATE, DELETE, SELECT, ALTER, EXECUTE usw.) auf ein Datenbankobjekt an einen Benutzer vergibt. Legt ein Benutzer ein Objekt an, hat er zunächst alle Rechte an diesem Objekt, alle anderen Benutzer haben keinerlei Rechte. Sollen andere Benutzer auf ein Objekt zugreifen, müssen die Rechte explizit vergeben werden: GRANT SELECT ON Konto TO Alice; Diese Anweisung gibt dem Benutzer Alice das Recht, die Tabelle Konto zu lesen. Möchte sie das tun, muss sie dem Namen der Tabelle den Namen des Eigentümers voranstellen: SELECT * FROM Bob.Konto; Rechte können mit der REVOKE-Anweisung wieder entzogen werden. Auf diese Weise definierte Rechte können im Prinzip als Matrix dargestellt werden: Joe Jack Jill Objekt 1 ALL Objekt 2 SELECT SELECT Objekt 3 ALL In der Praxis ist diese Matrix meistens sehr dünn besetzt, so dass es Verschwendung wäre, die vollständige Matrix zu speichern. In der Praxis werden deshalb die Zeilen in Form von Access Control Listen gespeichert. MySQL definiert Rechte (anders als der SQL-Standard) für die Kombination aus Benutzer und Rechner. Legt ein Benutzer ein Objekt (Tabelle oder Stored Procedure) an, hat er zunächst alle Rechte an diesem Objekt, alle anderen Benutzer haben keinerlei Rechte. Sollen andere Benutzer auf ein Objekt zugreifen, müssen die Rechte explizit vergeben werden: GRANT SELECT ON Konto TO 'Alice'@'localhost'; Diese Anweisung gibt dem Benutzer Alice das Recht, auf die Tabelle Konto zuzugreifen. Möchte sie das tun, muss sie dem Namen der Tabelle den Namen des Eigentümers voranstellen: MySQL 45 SELECT * FROM till.Konto; Beim Namen des Objekts können Wildcards verwendet werden, ON url.* betrifft etwa alle Objekte der Datenbank url. Sollen alle Privilegien vergeben werden, wird das Recht ALL verwendet: GRANT ALL ON url.* TO 'Bob'@'localhost'; vergibt alle Rechte an allen Objekten der Datenbank an den Benutzer Bob. Diese Konstruktion verwendet man üblicherweise nur für den Eigentümer einer Datenbank. Man kann auch systemweit Rechte vergeben: GRANT ALL ON *.* TO 'Bob'@'localhost'; Gibt Bob alle Rechte an allen Objekten, das entspräche dem speziellen Benutzer root. Ein Problem bei dieser Art der Rechte-Definition ist, dass es bei einer größeren Anzahl an Benutzern sehr viele Rechte-Definitionen gibt. Außerdem ist nicht mehr nachvollziehbar, warum ein Benutzer ein bestimmtes Recht hat. Dies erschwert es vor allem, Rechte bei Änderungen, etwa in der Organisation, zu pflegen. Aus diesen Gründen verwenden viele Systemen rollenbasierte Rechtekonzepte (Role Based Access Control, RBAC): Rechte werden an Rollen vergeben. Die Rollen können in Form einer Hierarchie aufgebaut sein, es kann also praktisch eine Rechtev- ererbung zwischen Rollen geben. Diese Rollen werden dann Benutzern zugeordnet. Damit kann eine saubere Trennung zwischen den Personen und den Funktionen, die sie ausführen (müssen) erreicht werden. MySQL unterstützt keine Rollen, die Rechte können nur direkt an Benutzer vergeben werden. MySQL 46 Stored Procedures Seit Version 5 unterstützt MySQL Stored Procedures. Das ist Programmcode, der in der Daten- bank gespeichert und im Datenbankserver ausgeführt wird. Dies kann beispielsweise die Perfor- mance verbessern oder die Sicherheit einer Anwendung erhöhen. Stored Procedures werden, je nach Datenbanksystem, in unterschiedlichen Programmiersprachen verfasst, die meisten Datenbanksysteme bieten hierfür eine eigene, spezielle Programmier- sprache an, etwa PL/SQL von Oracle oder TransactSQL von Microsoft. Diese Programmier- sprachen sind typischerweise PASCAL-ähnliche Sprachen, die um SQL-Syntax erweitert wur- den. MySQL unterstützt den ANSI SQL/PSM Standard 2003. Der Begriff Stored Procedures wird hier als Oberbegriff für Stored Procedures, Stored Functions und Trigger verwendet. Stored Functions liefern im Unterschied zu Stored Procedures einen Wert zurück, sie können beispielsweise verwendet werden, um eigene Funktionen zu realisieren, die in SQL-Anweisungen verwendet werden. Trigger sind spezielle Stored Procedures, die automatisch vom DBMS bei bestimmten Aktionen aufgerufen werden, etwa wenn ein Datensatz gelöscht werden soll. Prinzipiell lassen sich mit Stored Procedures beliebige Programme schreiben: 1 DELIMITER $$ 2 DROP FUNCTION IF EXISTS fib; 3 CREATE FUNCTION fib(n INT) RETURNS INT 4 BEGIN 5 DECLARE f2 INT; 6 DECLARE f1 INT; 7 DECLARE f INT; 8 DECLARE cnt INT; 9 SET cnt = 2; 10 SET f2 = 0; 11 SET f1 = 1; 12 IF (n = 0) THEN 13 SET f = f2; 14 END IF; 15 IF (n = 1) THEN 16 SET f = f1; 17 END IF; 18 WHILE (cnt <= n) 19 DO 20 SET f = f1 + f2; MySQL 47 21 SET f2 = f1; 22 SET f1 = f; 23 SET cnt = cnt + 1; 24 END WHILE; 25 RETURN f; 26 END$$ 27 DELIMITER ; Die Funktion fib berechnet die n-te Fibonacci Zahl. Die nächste Fibonacci Zahl erhält man, indem man ihre beiden Vorgänger addiert. Die ersten beiden Fibonacci-Zahlen sind 0 und 1. Damit ergibt sich dann die Folge 0,1,1,2,3,5,8,13 usw. Die Funktion kann beispielsweise in einer SELECT- Anweisung aufgerufen werden, etwa SELECT fib(5); Gehen wir die Funktion Schritt für Schritt durch. Anweisungen in Stored Procedures werden mit einem Semikolon abgeschlossen. Dieses ist aber gleichzeitig das Trennzeichen für SQL- Anweisungen und würde damit eben auch die CREATE FUNCTION-Anweisung beenden. Damit das nicht passiert, setzen wir in Zeile 1 das SQL-Trennzeichen auf einen anderen Wert, hier $$. In Zeile 2 wird die Funktion fib gelöscht, falls sie bereits existiert. In Zeile 3 wird die Funktion fib deklariert. Sie erhält einen Parameter vom Typ INT und liefert einen INT-Wert zurück. In den Zeilen 5-8 werden Variablen deklariert. Hier können alle MySQL- Datentypen verwendet werden. Sollen der Wert einer Variablen gesetzt werden, muss die SET- Anweisung verwendet werden (Zeilen 9-11). Im Gegensatz zu reinem SQL können hier Kontrollstrukturen wie Verzweigungen und Schleifen verwendet werden. Dadurch können Funktionalitäten realisiert werden, die mit reinem SQL nicht möglich oder sehr schwierig sind. Stored Procedures können zur Konsistenzsicherung eingesetzt werden. In unserem Beispiel könnte etwa durch einen Programmierfehler oder bösen Willen der Stand eines Kontos erhöht werden, ohne dass ein anderes Konto belastet wird. Das sollte natürlich nicht vorkommen. Dies läßt sich dadurch verhindert, dass alle schreibenden Zugriffe auf die Tabelle Konto über eine Stored Procedure abgewickelt werden: MySQL 48 DELIMITER $$ DROP PROCEDURE IF EXISTS Ueberweisung$$ CREATE PROCEDURE Ueberweisung(KontoVon CHAR(10), KontoNach CHAR(10), Betrag D\\ ECIMAL(14,2)) BEGIN UPDATE Konto SET Stand = Stand - Betrag WHERE Nummer = KontoVon; UPDATE Konto SET Stand = Stand + Betrag WHERE Nummer = KontoNach; END$$ DELIMITER ; Die Prozedur fasst lediglich die beiden UPDATE-Anweisungen zusammen. Für andere Zugriffe, beispielsweise Ein- oder Auszahlungen, müßten vergleichbare Prozeduren erstellt werden. Wenn jetzt den Anwendern die Schreibrechte auf die Tabelle Konto entzogen werden, können sie keine UPDATES mehr manuell ausführen. Lediglich ein Benutzer (der Eigentümer) erhält diese Rechte und legt die Stored Procedure an. Die anderen (normalen) Benutzer erhalten das EXECUTE-Recht für die Prozedur. So können Business Rules zentral definiert und verbindlich umgesetzt werden. Konsistenzsicherung durch Stored Procedures MySQL 49 In dieser Stored Procedure können noch weitere Prüfungen realisiert werden, zum Beispiel sollte geprüft werden, ob die Konten überhaupt existieren. Wird dies nicht geprüft, könnte in der oben gezeigten Version wieder Geld „aus dem nichts” erzeugt werden, indem von einem nicht existierenden Konto aus überwiesen wird. Dieses Problem löst die folgende Version, die vor der eigentlichen Überweisung prüft, ob die Konten vorhanden sind. DELIMITER $$ DROP PROCEDURE IF EXISTS Ueberweisung$$ CREATE PROCEDURE Ueberweisung(KontoVon CHAR(10), KontoNach CHAR(10),Betrag DE\\ CIMAL(14,2)) BEGIN DECLARE num INT; DECLARE msg VARCHAR(100); START TRANSACTION; SELECT COUNT(*) INTO num FROM Konto WHERE Nummer = KontoVon; IF (num <> 1) THEN SET msg = CONCAT('Konto existiert nicht: ', KontoVon); SIGNAL SQLSTATE '45000' SET MESSAGE_TEXT=msg; END IF; SELECT COUNT(*) INTO num FROM Konto WHERE Nummer = KontoNach; IF (num <> 1) THEN SET msg = CONCAT('Konto existiert nicht: ', KontoNach); SIGNAL SQLSTATE '45000' SET MESSAGE_TEXT=msg; END IF; UPDATE Konto SET Stand = Stand - Betrag WHERE Nummer = KontoVon; UPDATE Konto SET Stand = Stand + Betrag WHERE Nummer = KontoNach; COMMIT; END$$ DELIMITER ; Existiert eins der Konten nicht, wird eine Fehlermeldung erzeugt und die Operation abgebrochen. Auch komplexere Prüfungen, etwa ob die Kreditlinie ausreichend ist oder ob der Betrag gültig ist oder ob der Typ des Kontos stimmt könnten hier natürlich auch realisiert werden. Werden diese Prüfungen in einer Stored Procedure realisiert, hat dies den Vorteil, dass die Geschäftslogik an genau einer Stelle, nämlich im Datenbankserver realisiert ist. Durch Vergabe MySQL 50 entsprechender Berechtigungen kann gewährleistet werden, dass diese Prüfungen nicht umgan- gen werden können. Der Nachteil dieser Methode ist, dass die Anwendungen keine SQL-UPDATE Anweisungen verwenden können, sondern eine Stored Procedure aufrufen müssen. Dies läßt sich vermeiden, wenn Trigger verwendet werden. MySQL 51 Trigger Trigger sind spezielle Stored Procedures, die automatisch aufgerufen werden, wenn ein bes- timmtes Ereignis eintritt. Sie stammen aus dem Bereich der aktiven Datenbanksysteme, die typischerweise ein ECA-Schema (Event, Condition, Action) verwenden. Auslösende Events bei relationalen Datenbanken sind Schreibzugriffe auf Tabellen, also INSERT-, UPDATE- und DELETE-Anweisungen. Im Trigger sind die Variablen NEW bzw. OLD verfügbar, die die Werte der zu ändernden Zeile vor bzw. nach der Änderung enthalten. Trigger zur Konsistenzsicherung Ein Trigger wird mit der CREATE TRIGGER Anweisung erzeugt. Trigger können entweder vor oder nach der jeweiligen Anweisung aufgerufen werden (BEFORE bzw. AFTER). Trigger können entweder einmal pro SQL-Anweisung oder für jede betroffene Zeile (FOR EACH ROW) aufgerufen werden. Es darf je Aufrufkondition und Tabelle nur ein Trigger angelegt werden. Trigger können genutzt werden, um Vorgänge zu protokollieren. Soll etwa jede Änderung des Kontostands protokolliert werden, kann folgender Trigger verwendet werden: MySQL 52 CREATE TABLE Buchung ( ID INT NOT NULL AUTO_INCREMENT PRIMARY KEY, Konto CHAR(10) NOT NULL, StandAlt DECIMAL(14,2) NOT NULL, StandNeu DECIMAL(14,2) NOT NULL, Wann TIMESTAMP DEFAULT NOW()); DELIMITER $$ CREATE TRIGGER T_LOG_Konto AFTER UPDATE ON Konto FOR EACH ROW BEGIN INSERT INTO Buchung(Konto,StandAlt,StandNeu) VALUES(NEW.Nummer,OLD.Stand,NEW.Stand); END$$ DELIMITER ; Jedes Update auf die Tabelle Konto führt jetzt zu einem Eintrag in die Tabelle Buchung. Wiederum muß durch geeignete Zugriffsrechte verhindert werden, dass diese manipuliert werden kann. Protokollierung durch Trigger So können neben reinen Protokoll-Funktionen auch denormalisierte Darstellungen aktuell gehalten werden. Hätte unser Datenmodell etwa so ausgesehen, dass alle Vorgänge in einer Tabelle Buchung gespeichert, könnte der aktuelle Stand eines Kontos durch Summierung über diese Datensätze berechnet werden. Deshalb bräuchte die Tabelle Konto kein Attribut Stand. Diese Berechnung bei jedem Zugriff durchzuführen wäre aber recht aufwendig. Deshalb wäre es unter Umständen sinnvoll, den aktuellen Stand in der Tabelle Konto zu speichern, und diesen Wert automatisch durch einen Trigger auf der Tabelle Buchung zu aktualisieren. MySQL 53 MySQL ruft bei Foreign Key Cascades keine Trigger auf, das heisst, wenn etwa ein Datensatz wegen eines FOREIGN KEY-Constraints des Typs ON DELETE CASCADE automatisch gelöscht wird, wird ein eventuell existierender DELETE-Trigger nicht aufgerufen. Stored Procedures enthalten Code, der im Datenbankserver ausgeführt wird. Sie werden zusam- men mit Triggern zur Abbildung von Business-Rules in der Datenbank verwendet. Stored Procedures werden in einer datenbankspezifischen Programmiersprache abgefasst, die direkt SQL-Anweisungen enthalten kann. Trigger sind eine spezielle Form von Stored Procedures, die automatisch aufgerufen werden, wenn Daten geändert werden. MySQL 54 Views Views sind ein mächtiges Werkzeug bei der Entwicklung von Datenbankanwendungen, etwa zur Denormalisierung oder zur Realisierung von Berechtigungskonzepten. Seit Version 5.0 unterstützt MySQL Views. Ein View wird mit der CREATE VIEW Anweisung angelegt: Anlegen eines Views in MySQL Ein View ist eine Art virtuelle Tabelle und kann bei Abfragen genau so wie eine Tabelle verwendet werden. Schwieriger wir das bei DML-Anweisungen (INSERT, UPDATE, DELETE), hier funktioniert das nur in Spezialfällen. Wenn man nun seine Anwendung auf Views aufbauen will, kann das zu Problemen führen, die mit MySQL nicht lösbar sind. Andere Datenbanksysteme bieten hier Hilfsmittel, Oracle etwa die INSTEAD OF Trigger, die die gewünschte Funktionalität bei Schreibzugriffen realisieren können. Views bieten verschiedene Sichten auf die Daten, ohne dass Redundanzen entstehen. Dürfen etwa bestimmte Nutzer nur die Kontostände, aber nicht das jeweilige Kreditlimit sehen, kann dies über einen View realisiert werden: CREATE VIEW Kontostand AS SELECT Nummer,Stand,Kunde_ID FROM Konto; Dürfen etwa bestimmte Benutzer nur bestimmte Kontenarten einsehen, könnten entsprechende Views erzeugt werden: CREATE VIEW Girokonten AS SELECT * FROM Konto WHERE TYPE='Giro'; Die Definition von Views kann auch Funktionen enthalten. Soll etwa im vorigen Beispiel die Definition dynamisch erfolgen, also als Datum in einer Tabelle gespeichert werden, kann dies ebenfalls mit einem View realisiert werden: MySQL 55 Dynamische Views Werden die Zugriffsrechte entsprechend gesetzt, können auf diese Weise komplexe Berechti- gungsmodelle umgesetzt werden. Da die Datensätze eines Views jedesmal neu berechnet, das heißt, die zugrundeliegende Abfrage jedesmal neu ausgeführt wird, kann dies unter Umständen zu Performance-Problemen führen. Andere Datenbanksysteme bieten hierfür Materialized Views an. Diese werden von MySQL aber (noch) nicht unterstützt. Views sind virtuelle Tabellen. Die enthaltenen Daten sind nicht gespeichert, sondern werden bei jeder Abfrage neu berechnet. Views können unterschiedliche Sichten auf die Daten anbieten. Durch geeignet gewählte Zugriffsrechte können komplexe Berechtigungskozepte umgesetzt werden. Mit Views können häufig verwendete Zusammenstellungen von Daten, auch über mehrere Tabellen hinweg, einfach verfügbar gemacht werden. Entwicklung von Datenbankanwendungen Werden die Daten in einem Datenbankserver verwaltet, stellt sich die Frage, wie diese bearbeitet werden sollen. Der direkte Zugriff mit SQL über ein Werkzeug wie mysql oder PHPmyadmin (http://www.phpmyadmin.net/) ist für Administratoren oder Entwickler gedacht. End-Anwender benötigen eine komfortable, übersichtliche Oberfläche, insbesondere, wenn das Datenmodell komplexer wird. Da die Daten in der Datenbank, einem separaten Prozess, im Allgemeinen auf einem separaten Rechner gespeichert sind, kommt bei Anwendungen zum Zugriff auf eine Datenbank meistens eine Client-Server-Architektur zur Anwendung. Client Server Architektur: Die Daten werden zentral gehalten, die Anwendung läuft auf unabhängigen Client-Rechnern. Diese Architektur kommt auch heute noch oft zum Einsatz, erfordert aber eine Client-Software, die die Anwendungslogik enthält. Dieses Programm, entweder speziell erstellt oder eine Standard-Software wie etwa Microsoft Excel, muss auf dem Client-Rechner installiert sein. Dies führt erstens zu einer großen Zahl von benötigten Lizenzen und zweitens zu Aufwand für die Installation und Wartung. Im Unternehmenseinsatz kann dies zu erheblichen Kosten führen. Außerdem ist - je nach Anwendung - die Verwendung auf bestimmte Endgeräte eingeschränkt: Nicht jede Client-Software läuft auch auf Smartphones oder Tablet-Computern. Andererseits hat hier der Anwender die größte Flexibilität: Kann über eine standardisierte Schnittstelle auf die Daten zugegriffen werden kann sich der Anwender selber die gewünschte Software installieren oder gewünschte Funktionen implementieren. Besonders einfach wird dies durch Werkzeuge wie Excel, die Anwender aus dem betrieb- swirtschaftlichen Umfeld in die Lage versetzen, ihre eigenen Anwendungen auf der Basis der Unternehmensdaten zu entwickeln. Heute entwickelt man Anwendungen im Allgemeinen so, dass sie erstens ohne Installation einer speziellen Software und zweitens möglichst unabhängig vom verwendeten Endgerät Entwicklung von Datenbankanwendungen 57 verwendet werden können. Dies wird dadurch erreicht, dass die Anwendung als sogenannte Web-Anwendung im Browser ausgeführt wird. Die Anwendung erzeugt also HTML, der Browser ist nur für die Darstellung zuständig. Die Anwendung wird vom Webserver aufgerufen und gesteuert. Um dies möglichst effizient und plattformunabhängig zu gestalten, werden Web-Anwendungen meistens in einer Scriptsprache entwickelt. Populäre Beispiele sind etwa PHP, ruby oder ASP.NET. Architektur von Web-Anwendungen: Die Daten werden zentral gehalten, auf dem Webserver läuft ein Programm, das die Anwendungslogik enthält und HTML erzeugt. Dieses wird vom Browser dargestellt. Dieser klassische Aufbau entspricht aber oft nicht mehr den heutigen Ansprüchen an Komfort, Skalierbarkeit, Verfügbarkeit und Flexibilität. Zum einen führt dies dazu, dass ein Teil der Anwendungslogik in den Browser verlagert wird. Zumindest die Validierung der eingegebenen Daten sollte soweit als möglich sofort, das heißt nicht erst nach dem Absenden eines Formulars erfolgen, sondern spätestens beim Verlassen eines Eingabefeldes. Dies kann mit Javascript umgesetzt werden. Anwendungen, die einen höheren Interaktionsgrad erfordern, wie etwa ein email-Client oder ein Chat-System benötigen zusätzlich eine Kommunikation zwischen Javascript-Elementen und dem Server. Sollen etwa bei der Eingabe einer email-Adresse passende Vorschläge aus dem Adressbuch angezeigt werden, kann nicht jedesmal die Seite neu aufgebaut werden, die Oberfläche muß dynamisch neue Daten nachladen können. Dies wird mit AJAX realisiert. Entwicklung von Datenbankanwendungen 58 Realisierung Je nach Datenbanksystem (und Client-Programm) ist es durchaus möglich und unter Umständen sogar sinnvoll, den SQL-Client direkt zu benutzen, um Reports, also Auswertungen der gespe- icherten Daten, zu erstellen. Zu Großrechnerzeiten durchaus üblich, ist diese Methode heute etwas in Vergessenheit geraten. Da die Daten in einer relationalen Datenbank ja in Form von Tabellen strukturiert sind, und das Ergebnis auch komplexer Abfragen wiederum eine Tabelle erzeugt, kann diese auch direkt ausgegeben werden, wenn die Anforderungen an das Layout nicht hoch sind. Dies ist oft bei internen Reports der Fall. Wesentliche Vorteile dieser Methode sind, dass keine zusätzlichen Programme benötigt werden und die Reports leicht automatisch, etwa zu einer bestimmten Zeit oder einem bestimmten Datum, erstellt werden können. In den folgenden Abschnitten werden verschiedene Methoden zum Zugriff auf Datenbanken behandelt. Die Daten befinden sich jeweils in einer mysql-Datenbank. Als Beispiel wird ein einfacher Clone von tinyURL (http://tinyurl.com) verwendet: Das System speichert URLs und Abkürzungen dazu, so dass statt einer langen URL eine kurze verwendet werden kann. Wird etwa eine URL per email verschickt, kann es passieren, dass eine lange URL umgebrochen, also auf mehrere Zeilen verteilt wird. Wenn der Empfänger dann auf den Link klickt, kann es passieren, dass dem Browser nur eine Zeile der URL übergeben wird und damit natürlich ein Fehler erzeugt wird. Das System macht also aus Links wie http://www.amazon.de/Maklerverwaltungsprogramme-Zukunft-Unterst%C3%BCtzung-Versicherungs– Finanzvertrieben/dp/389952506X/ref=sr_1_1?ie=UTF8&qid=1345452300&sr=8-1 kurze links wie etwa http://meinserver.de/amazon1 die problemlos verschickt werden können. Klickt der Empfänger auf den kurzen Link, wird der Originallink aus der Datenbank gesucht und ein HTTP-redirect durchgeführt, so dass die gewünschte Seite angezeigt wird. Außerdem wird der Zugriff mitprotokolliert, so kann nachvollzogen werden, wie oft (und ggf. von wem) auf welche Links zugegriffen wurde. Dies ist etwa bei Newslettern wichtig, um festzustellen, wie interessant die einzelnen Beiträge waren. Das Datenmodell ist einfach, es besteht aus zwei Tabellen, “links” und “visits”. Die Tabelle “links” enthält die Zuordnung vom Original-Link zur Abkürzung, die Tabelle “visits” protokolliert die Aufrufe. Entwicklung von Datenbankanwendungen 59 Entity Relationship Modell der Beispielanwendung Umsetzung in mysql Im Folgenden werden verschiedene Methoden vorgestellt, wie auf diese Daten zugegriffen werden kann. Den Schwerpunkt bildet (natürlich) der Zugriff über eine Web-Schnittstelle, aber auch sogenannte Fat-Clients, also lokal installierte Programme sind für spezielle Aufgaben sinnvoll einsetzbar. Dies ist beispielsweise bei der Auswertung mit aufwendigen Reporting Tools der Fall, kann aber auch bei sehr einfachen Anwendungen sinnvoll sein. Die Beispiele sind in verschiedenen Programmiersprachen wie etwa Java, PHP, C oder ruby realisiert. Kenntnisse in diesen Programmiersprachen sind aber nicht erforderlich, es sollen nur die verschiedenen Ansätze zum Zugriff auf Datenbanken dargestellt werden. Die Beispielpro- gramme verzichten der Übersichtlichkeit halber auf praktisch jede Art von Fehlerbehandlung und realisieren jeweils auch nur einen Teil der Funktionalität. Eine vollständige Implementierung findet sich in [Sheong10], aus dem die Idee zu diesem Beispiel stammt. Entwicklung von Datenbankanwendungen 60 ODBC Dass Access eine leistungsfähige Desktop-Datenbank ist, ist bekannt. Weniger bekannt ist, dass sich Access sehr gut eignet, um Client-Anwendungen zu erstellen, die auf einen Datenbankserver zugreifen. Die Stärke von Access ist die einfache Erstellung von Formularen und Reports. Soll dabei nicht die interne Jet-Engine, sondern ein separates Datenbanksystem verwendet werden, erfolgt der Zugriff über ODBC (Open Database Connectivity). ODBC wurde Anfang der Neunziger Jahre von Microsoft entwickelt und wurde bald zum Industriestandard für den systemunabhängigen Zugriff auf relationale Datenbanksysteme. .. “Am Anfang waren die Daten, sie waren unformatiert, und Dunkelheit herrschte auf der Erde. Und Codd sagte: „Es werde ein relationales Datenmodell”. Und so geschah es. Und Codd sagte: „Die Daten sollen von den Datenstrukturen der Applikationsprogramme getrennt werden, so daß eine Datenunabhängigkeit entstehe”. Und es war gut. Und die DBMS-Hersteller sagten: „Wir wollen fruchtbar sein und uns mehren”. Und so geschah es. Und die Benutzer sagten: „Wir wollen Applikationen einsetzen, um auf die Daten von allen DBMS-Herstellern zuzugreifen” Und die Applikationsentwickler senkten die Häupter und sagten: „Wir müssen durch das finstere Tal mit den Precompilern oder CLI’s, Kommunikationsstacks und Protokollen aller Hersteller wandern”. Und es war nicht gut … Und so entstand ODBC” Aus [Geiger99] Entwicklung von Datenbankanwendungen 61 ODBC-Architektur, aus http://dev.mysql.com/doc/refman/5.1/de/myodbc-general-information.html Der entscheidende Vorteil von ODBC verglichen mit anderen Zugriffs-Schnittstellen wie etwa Embedded SQL ist, dass die Anwendung durch den Driver-Manager vom Datenbanksystem entkoppelt ist. Die Anwendung verbindet sich erst zur Laufzeit mit dem Driver Manager. Die Datenbank wird dabei durch den Data Source Name (DSN) identifiziert. Der Driver Manager leitet dann die Anfragen der Anwendung an den entsprechenden, datenbankspezifischen ODBC- Treiber weiter, der diese wiederum an das Datenbanksystem weitergibt. Die Anwendungen sehen nur die systemunabhängige Programmierschnittstelle des ODBC- Treibers und können so unabhängig vom zugrundeliegenden Datenbanksystem entwickelt werden. Microsoft verwendet ODBC auch als interne Programmierschnittstelle, die anderen Hersteller liefern passende Treiber für Ihre Systeme. ODBC hat sich auch plattformübergreifend durchge- setzt, die bei Java verwendete JDBC-Schnittstelle orientiert sich sehr stark an ODBC. ODBC erlaubt es, Anwendungen unabhängig von der verwendeten Datenbank zu entwickeln. Der Zugriff erfolgt über den Treibermanager, bei Windows Teil des Betriebssystems. Entwicklung von Datenbankanwendungen 62 Access als Datenbank-Client Microsoft Access kann über ODBC auf beliebige Datenbanken zugreifen. Dazu muss zunächst eine Datenquelle angelegt werden, diese benötigt einen eindeutigen Namen, den Data Source Name (DSN). Dies erfolgt (unter Windows) über die Systemsteuerung im ODBC-Datenquellen- Administrator. Hier wird eine neue DSN angelegt. Dabei wird zwischen Benutzer-DSN, System-DSN und Datei-DSN unterschieden. Benutzer- DSNs stehen nur dem aktuell angemeldeten Benutzer zur Verfügung, SYSTEM-DSNs sind für alle sichtbar. Eine Datei-DSN ist eine Datenquelle, die sich auf eine Datei bezieht und nicht auf einen Datenbankserver. Über eine Datei-DSN kann etwa auf Excel-, dBase- oder Textdateien zugegriffen werden. Zum Zugriff auf eine MySQL-Datenbank wird, je nach Anforderung, eine Benutzer-DSN oder eine System-DSN verwendet. Nach Auswahl des Treibers werden in einem Dialog die notwendigen Parameter zum Zugriff angegeben, die mit dem Test-Button überprüft werden können. Erstellen einer ODBC-DSN Damit ist die DSN für Anwendungen verfügbar. In Access kann beispielsweise in der Regis- terkarte “Externe Daten” über ODBC auf einen Datenbankserver zugegriffen werden. Die so verknüpften Daten können dann in Formularen oder Reports genau so verwendet werden wie lokale Tabellen. So kann beispielsweise mit wenigen Klicks ein Formular zur Bearbeitung von Daten oder eine Auswertung erstellt werden. Entwicklung von Datenbankanwendungen 63 Zugriff auf eine MySQL-Datenbank mit einem Access-Formular In der folgenden Abbildung ist etwa die Anzahl der Aufrufe pro Monat dargestellt. Sollen solche Berichte ausgedruckt werden, ist es verhältnismäßig aufwendig, diese etwa per PHP in einer Web-Anwendung zu erstellen. Müssen solche Auswertungen nur lokal ausgeführt werden, ist es erheblich weniger Aufwand, diese mit einem Werkzeug wie Access oder Excel zu erstellen, als in Form einer Web-Anwendung. Entwicklung von Datenbankanwendungen 64 Access Report Da Excel ebenfalls auf ODBC-Datenquellen zugreifen kann ist auch die Erstellung von Diagram- men kein Problem: Zugriff mit Excel auf eine MySQL-Datenbank Besonders interessant wird diese Art des Zugriffs auf Datenbanken dadurch, dass Programme wie Access oder Excel durch Scripte gesteuert werden können. So können solche Darstellungen Entwicklung von Datenbankanwendungen 65 automatisiert erstellt und etwa verschickt oder ausgedruckt werden. Für einfachere Anwendun- gen kann so ein komplexes Report-Writing-Tool ersetzt werden. Web-Ansicht des Script-Systems Solche Architekturen werden in der Praxis oft eingesetzt: Ein umfangreiches Online-Script- System (http://wi2.dhbw-heidenheim.de/script) speichert alle Daten in einer MySQL-Datenbank. Die Darstellung im WWW ist mit PHP realisiert, die Pflege der Daten erfolgt über Access- Formulare. Eine wesentliche Funktion hier ist die hierarchische Anordnung der einzelnen Elemente, dies ist mit einer Web-Anwendung nur aufwendig zu realisieren. Entwicklung von Datenbankanwendungen 66 Pflege der Daten mit einem Access-Formular Microsoft Access kann als Client für andere Datenbanksysteme eingesetzt werden. So können ohne großen Aufwand leistungsfähige und ansprechende Oberflächen entwickelt werden. Dies ist dann sinnvoll, wenn der Zugriff nur durch wenige Anwender erfolgt. In Kombination mit PHP können auch komplexe Anwendungen schnell realisiert werden. Entwicklung von Datenbankanwendungen 67 PHP PHP ist eine sehr weit verbreitete Scriptsprache zur Entwicklung von Web-Anwendungen (http://www.php.net/). PHP-Code wird meistens in HTML-Seiten eingebettet, dadurch wird die schrittweise Entwicklung einer Website ausgehend von statischen HTML-Seiten hin zur fertigen Anwendung erleichtert. PHP hat eine C-ähnliche Syntax, die einfach zu erlernen ist. Dateien, die PHP-Scripte enthalten, haben die Endung .php. Wird eine solche Datei angefordert, erkennt der Webserver an der Endung, dass es sich um ein PHP-Programm handelt, das ausgeführt werden muss. Dazu wird der enthaltene PHP-Code ausgeführt, und das darin erzeugte HTML an den Browser gesendet. Dieser merkt also (abgesehen von der Endung des Dateinamens) nichts davon, dass es sich um ein Programm handelt. Im folgenden Beispiel werden die Zahlen von 1 bis 10 ausgegeben: <html> <body> <?php $i=0; while ($i < 10) { echo $i . \"<br/>\"; $i = $i + 1; } ?> </body> </html> Der Zugriff auf Datenbanken kann auf verschiedene Arten erfolgen, hier wird die mysqli- Schnittstelle verwendet. mysqli ist eigentlich eine objektorientierte Programmierschnittstelle, hier wird der Einfachheit halber die prozedurale Version verwendet. <?php $db = mysqli_connect('localhost','till','geheim','url'); $result = mysqli_query($db, \"SELECT short_url, long_url FROM links WHERE short = 'ct-1204-05'\"); while ($row = mysqli_fetch_array($result)) { echo $row[0] . $row[1] . \"<br/>\"; } Entwicklung von Datenbankanwendungen 68 mysqli_close($db); ?> Zunächst wird mit der Funktion mysqli_connect eine Verbindung zur Datenbank aufgebaut. Mit mysqli_query wird dann eine Abfrage ausgeführt, das Ergebnis wird in der Variable $result gespeichert. Dieses wird dann in einer Schleife zeilenweise ausgegeben: mysqli_fetch_array liefert die Elemente der aktuellen Zeile als Array zurück. Untersuchen wir unser Beispiel, den tinyURL-Clone: 1 <?php 2 if (isset($_GET[\"short\"])) { 3 $short = mysql_real_escape_string($_GET[\"short\"]); 4 $db = mysqli_connect(\"localhost\",\"till\",\"geheim\",\"url\"); 5 $sql = \"SELECT * FROM links WHERE short = '\" . $short . \"'\"; 6 if ($result = mysqli_query($db,$sql)) 7 if (mysqli_num_rows($result) > 0) { 8 $row = mysqli_fetch_array($result); 9 $statement = mysqli_prepare($db,\"INSERT INTO visit(url,ip,timestamp) values (?,?,NOW())\"); 10 $ip = $_SERVER[\"REMOTE_ADDR\"]); 11 mysqli_bind_param($statement,\"ss\",$short,$ip); 12 mysqli_execute($statement); 13 header('HTTP/1.1 302 Moved Temporarily'); 14 header('Location: ' . $row[2]); 15 exit; Entwicklung von Datenbankanwendungen 69 16} 17 } 18 ?> Das Script url.php wird mit dem Parameter short aufgerufen, der die Kurzform der URL enthält und leitet den Browser dann auf die gewünschte Seite weiter. Der Aufruf sieht also etwa so aus: http://server.de/url.php?short=ct-1204-05 Zunächst wird in den Zeilen 2 und 3 geprüft, ob der Parameter short gesetzt ist. Falls ja, werden ungültige Zeichen, die Sicherheitsprobleme verursachen können, mit der Funktion mysql_real_- escape_string maskiert. In Zeile 4 wird eine Verbindung zur Datenbank “url” aufgebaut, in Zeile 5 die SQL-Anweisung zur Abfrage erzeugt und in Zeile 6 ausgeführt. Wenn die Abfrage Treffer liefert, der Wert des Parameters short also gültig war (Zeile 7) wird der erste Datensatz geladen (Zeile 8). In den Zeilen 9-11 wird der Aufruf in der Tabelle “visits” protokolliert. Dazu wird hier ein prepared Statement verwendet, das in Zeile 9 erzeugt wird. Für das Attribut timestamp wird die mysql-Funktion NOW() verwendet, die das aktuelle Datum und Uhrzeit liefert. Die SQL-Anweisung enthält Platzhalter für die URL und die IP-Adresse des Aufrufers. Diese werden in Zeile 10 gesetzt, die Funktion mysqli_bind_param erwartet als ersten Parameter einen String, der die Datentypen der weiteren Argumente festlegt. Der Buchstabe “s” steht für einen String, hier werden also zwei Strings übergeben. Der erste Wert ist die aufgerufene (Kurz-) URL, der zweite die IP-Adresse des Aufrufers. Diese ist in PHP über das Array _SERVER verfügbar. Schließlich muss der aufrufende Browser noch auf die neue URL umgeleitet werden. Dazu wird ein entsprechender HTTP-Header erzeugt, dazu wird die PHP-Funktion header() verwendet. PHP-Anweisungen werden in HTML-Dateien eingebettet. Dadurch kann eine Web-Anwendung in kleinen Schritten, ausgehend von einer HTML-Datei, erstellt werden. PHP ist eine inter- pretierte Scriptsprache, die auch von Programmieranfängern leicht erlernbar ist. Mit PHP können sehr einfach Web-Anwendungen realisiert werden, die auf eine MySQL-Datenbank zugreifen. Entwicklung von Datenbankanwendungen 70 Direkter Zugriff mit SQL Es gibt verschiedene Techniken, um aus (selbst erstellten) Programmen auf relationale Daten- banken zuzugreifen. Jedes RDBMS bietet eine native, das bedeutet produktspezifische, Program- mierschnittstelle. Mit allen Vor- und Nachteilen, die solche proprietären Schnittstellen haben: Der volle Funktionsumfang ist nur über diese Schnittstelle verfügbar, die Performance ist oft besser, aber Anwendungen müssen speziell für ein Datenbanksystem erstellt werden. Soll eine solche Schnittstelle verwendet werden, ist darauf zu achten, dass die datenbankspezifis- chen Teile der Anwendung gut gekapselt werden, etwa in speziellen Klassen oder Modulen. An- sonsten wird der Aufwand bei einem Systemwechsel oder bei einer Änderung der Schnittstelle erheblich sein. Letztlich ist dies der Weg, den systemunabhängige Architekturen wie etwa ODBC gehen: Im ODBC-Treiber ist der Zugriff auf die systemspezifische Programmierschnittstelle gekapselt, die Anwendung nutzt nur eine Abstraktionsschicht. Diese Abstraktionsschichten erlauben die systemunabhängige Ausführung von SQL-Anweisungen, sind aber vom Standpunkt des Programmierers nicht sehr komfortabel in der Anwendung. Deshalb werden heutzutage üblicherweise objektorientierte Schnittstellen zur Datenbank ver- wendet, etwa JDBC in Java oder ADO.NET in C#. Ein Cursor verweist auf die aktuelle Position in einem Abfrage-Ergebnis Alle sprachunabhängigen Programmierschnittstellen haben das Problem, dass eine Tabelle, insbesondere wenn ihre Größe nicht vorab bekannt ist, kein elementarer Datentyp ist. Da eine SQL-SELECT-Anweisung aber eine solche erzeugt, stellt sich die Frage, wie das Ergebnis unabhängig von der Programmiersprache dargestellt werden soll. Dazu wird ein Cursor verwendet: Eine Tabelle besteht aus einer Folge von Zeilen, die jeweils aus einer Folge von Elementen bestehen. Mit einem Cursor kann jeweils auf eine Zeile zugegriffen werden, zu Beginn ist dies die erste Zeile der Tabelle. Ein Cursor kann, sofern die Tabelle noch weitere Daten enthält, auf die nächste Zeile gesetzt werden. Aus der aktuellen Zeile können in beliebiger Folge die einzelnen Elemente ausgelesen werden. Das Cursor-Konzept findet sich in leicht unterschiedlicher Form in allen Programmierschnittstellen relationaler Datenbanken. Entwicklung von Datenbankanwendungen 71 Beispiel: Oracle Call Interface Das Oracle Call Interface ist die Programmierschnittstelle für Oracle, auf der alle anderen Werkzeuge und Schnittstellen wie SQL*Plus oder der Oracle ODBC-Treiber aufsetzen. OCI bietet ein API für die Programmiersprache C (es existiert auch eine Version für C++, OCCI). Ein einfaches Beispiel zeigt die prinzipielle Verwendung von OCI: static char cmd[] = \"INSERT INTO MESSAGE(SEVERITY,CODE) VALUES(:Severity,:Code)\"; if (!olog(&lda, hda, (unsigned char *)pszUserid, -1, (unsigned char *)pszPassword, -1, (unsigned char *)pszNetAlias, -1,(ub4)OCI_LM_DEF)) if (!oopen(&cda, &lda, (text *) 0, -1, -1, (text *) 0, -1)) if (!oparse(&cda,(unsigned char *) cmd,-1,0,2)) ProcessMessage(&msg); /* normaler C-Code */ if ((!obndrv(&cda,(unsigned char *)\":Severity\",-1, (unsigned char *) Severity, strlen(Severity), VARCHAR2_TYPE,-1,0,0,-1,-1)) || (obndrv(&cda,(unsigned char *)\":Code\",-1, (unsigned char *)Code, strlen(Code), VARCHAR2_TYPE,-1,0,0,-1,-1))) if (!oexec(&cda)) Zunächst wird durch die Funktion olog eine Verbindung zur Datenbank aufgebaut, pszUserid und pszPassword enthalten den Benutzernamen und das Kennwort, pszNetAlias den SQLNetNamen des Rechners (SQLNet ist die Oracle Netzwerk-Technologie). Mit oopen und oparse wird das SQL-Statement an den Datenbankserver gesendet und analysiert. Im SQL-Code in der ersten Zeile des Beispiels sind noch keine Werte für die einzufügenden Attribute angegeben, sondern nur Platzhalter. Die tatsächlichen Werte werden mit der Funktion obndrv gesetzt, mit oexec wird die Anweisung ausgeführt. Der Sinn der Trennung von SQL-Analyse und Binden der Werte war zunächst eine Steigerung der Performance. Weit wichtiger ist heute der Schutz vor SQL-Injection-Angriffen: Wird die SQL-Anweisung analysiert, bevor die Werte, die möglicherweise schadhaften SQL- Code enthalten eingetragen sind, kommt dieser nicht zur Ausführung. Deshalb sollten, wo möglich, diese sogenannten Prepared Statements verwendet werden. Es gibt Funktionalitäten, die nur über die native Schnittstelle zugänglich sind. So bietet Oracle etwa ein Feature namens Query Result Change Notfication: Eine Anwendung kann eine Abfrage registrieren, etwa Entwicklung von Datenbankanwendungen 72 SELECT id, status, timestamp from orders where TRUNC(order_date) = TRUNC(SYSDATE) die alle Bestellungen des heutigen Tages ausgibt. Ändern sich diese Daten in der Datenbank, kommt also etwa eine Bestellung dazu oder ändert sich ein Status, wird eine Callback-Funktion im Client aufgerufen. So kann in der Anwendung eine jederzeit dem tatsächlichen Stand entsprechende Sicht auf die Daten dargestellt werden, ohne die Abfrage ständig wiederholen zu müssen (was eine erhebliche Last für den Datenbankserver zur Folge hätte). Am Beispiel wird die Komplexität von OCI nur im Ansatz ersichtlich, um www.orafaq.com zu zitieren: .. Common problems with OCI: OCI code is difficult to write and to maintain Very few people can write, let alone maintain OCI code Native Schnittstellen wie OCI sollten nur dann Anwendung finden, wenn es zwingend erforder- lich ist, also etwa aus Performance-Gründen. Beispiel: ODBC ODBC ist die native Programmierschnittstelle von Microsoft, gleichzeitig allgemein akzeptierter Industriestandard. Der Aufbau ähnelt stark dem anderer nativer Programmierschnittstellen, das Beispiel aus dem Abschnitt über OCI mit ODBC-Funktionen realisiert: static char cmd[] = \"INSERT INTO MESSAGE(SEVERITY,CODE) VALUES (?,?)\"; rc=SQLAllocEnv(&henv); rc=SQLAllocConnect(henv,&hdbc); rc=SQLConnect(hdbc,\"Kurs\",SQL_NTS,ODBC_USERNAME,SQL_NTS, ODBC_PASSWORD,SQL_NTS); rc=SQLAllocStmt(hdbc,&hstmt); rc=SQLPrepare(hstmt,cmd,SQL_NTS); rc=SQLBindParameter(hstmt,1,SQL_PARAM_INPUT,SQL_C_CHAR, SQL_VARCHAR, strlen(Severity),0,Severity,0,NULL); rc=SQLBindParameter(hstmt,2,SQL_PARAM_INPUT,SQL_C_CHAR, SQL_VARCHAR, strlen(Code),0,Code,0,NULL); rc=SQLExecute(hstmt); Entwicklung von Datenbankanwendungen 73 Man erkennt, dass sich die ODBC-Funktionen und die OCI-Funktionen fast 1:1 entsprechen, de- shalb gilt hier im wesentlichen das im vorigen Abschnitt gesagte: Eine direkte Programmierung gegen die ODBC-API ist zu vermeiden, es sei denn, es ist zwingend erforderlich. Und zwingend erforderlich ist es praktisch nur bei der Erstellung von stärker abstrahierenden Schnittstellen. Beispiel: JDBC JDBC (Java Database Connectivity) ist die Standard-Datenbankschnittstelle von Java. Die abstrakteren Programmierschnittstellen bauen auf JDBC auf. JDBC ist letztlich eine objektorien- tierte Kapselung von ODBC. Da Java plattformunabhängig ist, entfällt der ODBC-Treibermanager, die Anwendung ist selbst für die Definition der Datenquelle zuständig. Dies erfolgt über eine Verbindungs-URL. Es gibt verschiedene Typen von JDBC-Treibern: Ein Typ-1-Treiber ist nur eine dünne Java- Schicht, die Aufrufe an einen ODBC-Treiber weitergibt. Dies war die ursprüngliche Architektur, bedingt aber erstens die Verfügbarkeit eines ODBC-Treibers und ist zweitens von der Perfor- mance her problematisch. Dieser Typ kann (und sollte nur) verwendet werden, wenn es für eine Datenbank zwar einen ODBC- aber keinen anderen JDBC-Treiber gibt. Typ-2 und Typ-3 Treiber sind heute ungebräuchlich. Ein Typ-4-Treiber ist vollständig in Java implementiert und greift direkt auf die Datenbank zu. Diese Architektur ist vorzuziehen, kann aber problematisch sein, falls die Daten durch Firewalls geleitet werden, etwa bei Applets. JDBC-Architektur, nach http://de.wikipedia.org/wiki/Java_Database_Connectivity Entwicklung von Datenbankanwendungen 74 Soll über JDBC auf eine Datenbank zugegriffen werden, so muß zunächst der passende Treiber geladen werden, hier der JDBC-Treiber für mysql. Der Name der Klasse des Treibers wird nicht hart kodiert, sondern als String angegeben: Die Klasse wird dynamisch geladen. So wird auch hier erst zur Laufzeit festgelegt, welcher Treiber verwendet, also auf welche Datenbank zugegriffen wird. Danach wird die Verbindung zur Datenbank aufgebaut, in diesem Fall auf demselben Rechner (“localhost”) zur Datenbank “url” mit der Benutzerkennung “till”und dem Kennwort “geheim”. Diese wird in einem Connection-Objekt verwaltet. Über die Methode executeQuery kann dann eine Abfrage ausgeführt werden, deren Ergebnis in Form eines ResultSet-Objekts zurückgegeben wird. Dieses stellt einen Cursor dar, der dann zeilenweise abgearbeitet wird. Mit den Methoden getString() bzw. getInt() usw. kann dann auf die einzelnen Elemente der aktuellen Zeile zugegriffen werden. Class.forName(\"com.mysql.jdbc.Driver\").newInstance(); Connection conn = DriverManager.getConnection(\"jdbc:mysql://localhost/url\",\"till\",\"geheim\"); Statement stmt = conn.createStatement (); ResultSet rset = stmt.executeQuery( \"SELECT short,long FROM links where short_url = 'ct-1204-05'\"); while (rset.next()) System.out.println(rset.getString(1) + \" \" + rset.getString(2)); Mit der Methode execute() der Klasse Statement können auch andere SQL-Anweisungen wie INSERT, UPDATE oder DELETE ausgeführt werden. Auch mit JDBC können Prepared-Statements verwendet werden, dies führt erstens zu einer besseren Performance (etwa Faktor zwei, verglichen mit dynamischem SQL) und zweitens zum Schutz vor SQL-Injection. PreparedStatement ps = conn.prepareStatement( \"select short_url, long_url from links where short = ?\"); ps.setString(1,\"ct-1204-05\"); ResultSet rset = ps.executeQuery(); while (rset.next() System.out.println(rset.getInt(1) + \" \" + rset.getString(2)); Prinzipiell sind hier die gleichen Schritte erforderlich wie bei den anderen Zugriffsverfahren, durch die Kapselung in Objekten wird der Programmcode aber übersichtlicher. Entwicklung von Datenbankanwendungen 75 Dieses Vorgehen ist einfach, hat aber zwei Probleme, die bei der Entwicklung größerer Anwen- dungen gelöst werden müssen: Der datenbankabhängige Teil einer Anwendung (und das sind alle Anweisungen, die mit SQL zu tun haben, da SQL zwar im Wesentlichen portabel ist, aber im Detail dann doch vom verwendeten Datenbanksystem abhängt) sollte möglichst isoliert werden, damit bei einer Änderung nicht die ganze Anwendung angepasst werden muss, sondern nur ein möglichst kleiner Teil. Dies kann mit einer Zwischenschicht, einem so genannten Access-Layer, realisiert werden. Problematisch ist weiterhin, dass Anwendungen heute üblicherweise objektorientiert program- miert werden, es gibt also beispielsweise in einem Webshop Klassen wie Kunde, Artikel oder Bestellung. Die Daten, die in diesen Objekten gekapselt werden, sollen in einer Datenbank gespeichert werden. Sollen die Funktionen zum Datenbankzugriff nun in diesen Klassen re- alisiert werden? Das widerspräche der eben genannten Trennung zwischen Anwendung und Datenbank-Zugriff. Eine Trennung widerspräche aber den Prinzipien der objektorientierten Programmierung oder führt zu einem sehr komplexen Objektmodell. Der Datenbankzugriff aus einem Programm heraus ist mit verschiedenen Schnittstellen möglich, die einen unterschiedlichen Abstraktionsgrad und unterschiedliche Portabilität bieten. Generell bieten objektorientierte Schnittstellen etwas mehr Komfort bei der Programmierung, aber die grundsätzliche Funktionsweise ist unabhängig von Programmiersprache und Datenbanksystem immer gleich: Verbindung zur Datenbank aufbauen, Query ausführen (ggf. vorher Prepared Statement erzeu- gen und Parameter binden) und Ergebnis in einer Schleife durchgehen. Entwicklung von Datenbankanwendungen 76 Object Relational Mapping (ORM) Wird eine Anwendung in einer objektorientierten Sprache entwickelt, deren Daten in einer Datenbank gespeichert werden sollen, stellt sich die Frage, wie die Beziehung zwischen dem Objektmodell der Anwendung (dem Domain Model) und dem Datenmodell der Datenbank aussehen soll. Grundsätzlich ist es nicht wünschenswert, den gleichen Sachverhalt in zwei Formen (nämlich in einem relationalen Modell und einem Objektmodell) auszudrücken. Eine solche Redundanz führt immer zu Mehraufwand und letztlich zu Inkonsistenzen. Deshalb ist es wünschenswert, nur ein Modell manuell zu erstellen und das andere daraus abzuleiten. Es gibt grundsätzlich zwei Möglichkeiten, eine solche Abbildung zu realisieren. Entweder wird das Objektmodell auf ein relationales Modell abgebildet, oder das relationale Modell wird auf ein Objektmodell abgebildet. Beide Verfahren haben ihre Probleme. Versucht man, das Objektmodell auf ein relationales Modell abzubilden, so zeigt sich schnell, dass es für viele Fälle keine eindeutige Abbildung gibt. Dies liegt am sogenannten “Object relational impedence mismatch”, Objekte und Tabellen passen einfach nicht zusammen. Grundsätzlich wird eine Klasse auf eine Tabelle, ein Attribut auf eine Spalte abgebildet. Problematisch wird dies etwa, wenn eine Vererbungshierarchie auf Tabellen abgebildet werden soll. Vererbung Auch hier kann jede Klasse auf eine Tabelle abgebildet werden, Student und Dozent wären dann weak Entities. Jeder Zugriff auf ein Person- oder Student-Objekt führt dann zu einem Join, dies ist der Performance nicht eben zuträglich. Bei einer komplexen Hierarchie mit vielen Spezialisierungen führt dies zu einer großen Zahl von Tabellen und bei der polymorphen Verarbeitung von Datensätzen zu einer Vielzahl von Datenbankzugriffen. Eine Alternative ist die Abbildung jeder Anwendungsklasse auf eine Tabelle, die zusätzlich die Attribute der Basisklasse erhält, im Beispiel also die Tabellen Student und Dozent. Sollen alle Personen behandelt werden, wird eine SQL-UNION verwendet, dies führt aber bei einer größeren Zahl von Klassen auch wieder zu Performance-Problemen. Wenn die Basisklasse geän- dert werden soll (etwa ein zusätzliches Attribut erhält) müssen alle Tabellen geändert werden. Das schwerwiegendste Problem tritt auf, wenn ein Objekt in mehreren Rollen auftritt: Ist eine Person sowohl Student als auch Dozent ist nicht klar, in welcher Tabelle sie gespeichert werden soll. Wird sie in beiden Tabellen gespeichert, führt dies auf lange Sicht zu Konsistenzproblemen. Eine weitere Variante ist die Speicherung aller Objekte in einer Tabelle, die Hierarchie wird Entwicklung von Datenbankanwendungen 77 flach. Die Tabelle enthält alle Attribute aller Klassen und einen Typdiskriminator, ein Attribut, das angibt, um welche Art Objekt es sich handelt. Dieses Verfahren hat den Nachteil, dass erstens die Tabelle sehr viele Spalten hat und zweitens die Konsistenzprüfung der Datenbank (etwa auf NOT NULL-Attribute) nicht mehr verwendet werden kann. Schließlich ist es noch möglich, die Daten in einem Meta-Modell zu speichern: Metamodell zur Abbildung von Vererbungsbeziehungen In einem solchen Modell können zwar beliebige Strukturen gespeichert werden, die Performance bei der Abfrage ist aber eher bescheiden. Man sieht: Schon die Fragestellung, wie eine Vererbungsstruktur auf ein relationales Modell abgebildet wird, ist nicht allgemein zu beantworten und erfordert eine detaillierte Überlegung im Einzelfall (bei Mehrfachvererbung, die etwa in C++ möglich ist, wird das dann richtig span- nend). Ähnlich komplexe Überlegungen müssen bei einer ganzen Reihe anderer Problemfelder angestellt werden, so ist etwa die Länge eines Strings bei Programmiersprachen im Allgemeinen nicht beschränkt, bei Datenbanken ist dies beim Typ VARCHAR im Allgemeinen aber durchaus der Fall. Die einfache Lösung, alle Strings auf Binary Large Objects abzubilden, funktioniert nicht, da diese nicht effizient durchsucht werden können. Schließlich muss noch die Frage geklärt werden, wo zu einem bestimmten Zeitpunkt die gültige Darstellung der Informationen liegt, in der Datenbank oder im Speicher des Client-Programms. Eine völlig andere Herangehensweise ist die Abbildung des relationalen Modells auf ein Objekt- modell. Jede Tabelle entspricht einem Objekt. Diese Zuordnung läßt sich ohne größere Probleme realisieren (die Details bezüglich Performance und Konsistenz werden am Ende des Kapitels kurz angesprochen). Diese Art der Abbildung wird nach [Fowler02] Active Record genannt: Entwicklung von Datenbankanwendungen 78 Active Record Pattern Die Domain-Klasse enthält zusätzlich zur Anwendungslogik die komplette Logik zum Zugriff auf die Datenbank. Ein Objekt der Klasse entspricht genau einer Zeile einer Tabelle. Um diese zu manipulieren, werden meist CRUD-Methoden (Create, Retrieve, Update, Delete) implementiert. Diese Art der Abbildung funktioniert dann gut (und automatisch), wenn das Datenmodell eher einfach ist, das bedeutet, wenn tatsächlich eine Tabelle in der Datenbank semantisch einem Domain-Objekt der Anwendung entspricht. Sind die Daten eines Domain-Objekts über mehrere Tabellen verteilt (etwa bei komplexen, hierarchischen Strukturen oder Vererbungsbeziehungen) sollte eine separate Klasse (manuell) erstellt werden, die die Abbildung regelt. Dies bezeichnet man als Data-Mapper-Pattern, die folgende Abbildung stammt aus [Fowler02], dort finden sich auch weiterführende Darstellungen zu diesem Thema. Das Data Mapper Patter nach Fowler Die eigentlichen Probleme liegen aber noch viel tiefer: Objekte werden normalerweise einzeln behandelt, Datensätze in einer relationalen Datenbank als Menge. Betrachten wir etwa den folgenden Ausschnitt aus dem Domain-Modell eines Webshops: Möchte man etwa die Bestellungen eines Kunden untersuchen, würde man aus Datenbanksicht eine Query der Art Entwicklung von Datenbankanwendungen 79 SELECT kde.name, best.datum, pos.menge, art.bezeichnung FROM Kunde kde INNER JOIN Bestellung best ON kde.id=best.kde_id JOIN Bestellposition pos ON best.id=pos.best_id JOIN Artikel art ON pos.art_id = art.id ausführen, also eine Anfrage, die die Daten aus allen benötigten Tabellen verbindet. Für diese Art von Abfragen sind relationale Datenbanken optimiert. Aus objektorientierter Sicht würde man zunächst das passende Kunde-Objekt suchen, zu diesem dann die zugehörigen Bestellungen, zu jeder Bestellung dann die Bestellpositionen und zu jeder Bestellposition den entsprechenden Artikel. Da die Semantik dieser Beziehungen nicht vorab bekannt ist, können nicht alle benötigten Objekte auf einmal aus der Datenbank geladen werden, sondern die Beziehungen zwischen den Objekten werden als lazy-evaluation ausgeführt, also erst beim Zugriff geladen. Das bedeutet im Beispiel, dass es eine erhebliche Anzahl von Datenbankzugriffen pro Kunde gibt: nehmen wir an, jeder Kunde hätte im Mittel 5 Bestellungen mit je 3 Bestellpositionen, führt dies zu 36 Datenbankzugriffen. Wird diese Operation für viele Kundenobjekte ausgeführt, soll beispielsweise die durchschnittliche Anzahl von Bestellpositionen ermittelt werden, müssen unter Umständen sehr viele Datenbankzugriffe ausgeführt werden. Dies führt dazu, dass ein Cache zwingend erforderlich ist, ansonsten wird die Performance sehr unerfreulich. Und selbst mit Cache müssen in der Praxis solche Abfragen oft separat von Hand kodiert werden, um eine gute Performance zu erzielen. Das wiederum kann problematisch sein, falls weitere Aufgaben wie Validierungen, Konsistenzsicherung oder Berechtigungsprüfungen in der ORM-Schicht realisiert sind. Insbesondere muss bei einem Mix aus Zugriffen über die ORM- Schicht und manuellen SQL-Anfragen die Konsistenz zwischen Cache und Datenbank gesichert sein. Dies ist in der Praxis mit relationalen Datenbanken kaum realisierbar. Um mit Martin Fowler zu schließen: “The object/relational mapping problem is hard.” [martin- fowler.com/bliki/OrmHate.html] Beispiel: ActiveRecord (ruby) Eine besonders elegante Implementierung des Active Record Patterns findet man in ruby in der ActiveRecord-Library. Das Domain-Modell wird in Form von ruby-Klassen definiert, die von ActiveRecord::Base abgeleitet werden. class Student < ActiveRecord::Base end Die Verbindung zur Datenbanktabelle students wird dadurch automatisch hergestellt. Hat diese Tabelle die Attribute Name, Matrikelnummer und email, kann über gleichnamige Methoden auf diese Attribute zugegriffen werden. Beziehungen werden durch eine einfache DSL ausgedrückt: Entwicklung von Datenbankanwendungen 80 class Student < ActiveRecord::Base has_many :courses belongs_to :year end Genau so einfach können Validierungen dargestellt werden: class Student < ActiveRecord::Base validates_presence_of :email,:matrikelnummer validates_uniqueness_of :matrikelnummer has_many :courses belongs_to :year end Der Zugriff erfolgt durch Methoden, etwa s = Student.find(4711) puts s.name s.email=\"ich@du.de s.save Für unser Beispiel, den tinyURL-Clone brauchen wir zwei Klassen Link und Visit. 1 require 'active_record' 2 require 'sinatra' 3 require 'haml' 4 class Link < ActiveRecord::Base 5 has_many :visits 6 end 7 class Visit < ActiveRecord::Base 8 belongs_to :link 9 end 10 ActiveRecord::Base.establish_connection( 11 :adapter => 'mysql2', 12 :database => 'url', 13 :username => 'till', 14 :password => 'geheim', 15 :host => 'localhost') 16 get '/' do haml :index end Entwicklung von Datenbankanwendungen 81 17 get '/url/:short' do 18 @short = params[:short] 19 l = Link.find_by_short_url(@short) 20 if (l.nil?) 21 haml :error 22 else 23 visit = l.visits.new(:url => @short, :ip => request.ip, :timestamp => Time.now) 24 visit.save 25 redirect l.long_url 26 end 27 end 28 enable :inline_templates 29 __END__ 30 @@ index 31 %h1.title Tiny URL Clone, Idee Chang Sau Sheong 32 @@ error 33 %h1.title unknown url: #{@short} In den Zeilen 10-15 wird die Verbindung zur Datenbank aufgebaut, der interessantere Teil beginnt ab Zeile 16. Hier sind sogenannte Routen definiert, wird eine entsprechende URL aufgerufen, wird der entsprechende ruby-Code ausgeführt. In Zeile 16 ist die Route “/” definiert, diese trifft für eine URL der Form http://meinserver.de zu. In dieser Route wird die Startseite mit haml, einer Template-Sprache für ruby, erzeugt. Das Template, befindet sich hier in den Zeilen 28-29. Normalerweise werden die Templates in separaten Dateien gespeichert, das ist für dieses einfache Beispiel aber nicht nötig. In den Zeilen 17 bis 27 befindet die eigentliche Funktionalität. Diese Route wird bei URLs der Form http://meinserver.de/url/xxx aufgerufen, wobei xxx die Kurzform der URL ist. Zunächst wird die Kurzform extrahiert und in der Variable @short gespeichert. In Zeile 19 wird ein entsprechender Eintrag in der Datenbank gesucht. Die hierzu nötige SQL-Anweisung wird von ActiveRecord automatisch generiert: SELECT `links`.* FROM `links` WHERE `links`.`short_url` = 'ct-1204-05' LIMIT 1 Wurde kein passender Eintrag gefunden, wird eine Fehlermeldung ausgegeben (Zeilen 20 und 21), ansonsten wird in den Zeilen 23 und 24 ein neuer Eintrag in der Tabelle “visits” angelegt. Objekte der Klasse Link haben eine Methode visits, die eine Auzählung der zugehörigen Visit- Objekte enthält. Mit dessen Methode new wird ein Visit-Objekt hinzugefügt, dass wiederum automatisch die Verbindung zum zugehörigen Link-Objekt hat. Die SQL-Anweisung wird wiederum automatisch erzeugt: Entwicklung von Datenbankanwendungen 82 INSERT INTO `visits` (`ip`, `link_id`, `timestamp`,`url`) VALUES ('127.0.0.1', 87, '2012-08-22 11:20:12', 'ct-1204-05') Schließlich wird der HTTP-Redirect zur richtigen URL ausgegeben (Zeile 25). Man sieht, dass das Programm keinen direkten Bezug zur relationalen Datenbankstruktur hat, die Zugriffe sind vollständig gekapselt. Der Entwickler der Anwendung braucht also keine SQL- Kenntnisse mehr. Ein grundlegendes Verständnis der Funktionsweise relationaler Datenbanken ist allerdings trotzdem erforderlich, sonst können eventuell ernsthafte Performance-Probleme auftreten. Unter Umständen müssen komplexe Abfragen manuell als SQL-Anweisungen erstellt werden. Dabei handelt es sich dann jedoch um einige wenige Stellen, die gut gekapselt werden können. Im Beispiel wurde nicht rails, das bekannte Web-Framework, sondern sinatra und haml verwen- det, Einführungen in rails gibt es im Internet viele, etwa http://imari.dhbw-heidenheim/rails.html Beispiel: Hibernate (Java) Hibernate ist das bekannteste ORM-Tool für Java. Die Abbildung von Java-Objekten auf Daten- banktabellen kann dabei entweder durch XML-Dateien oder im Java-Code durch Annotationen erfolgen. Der Zugriff auf die Datenbank kann über die Hibernate-API oder über die JPA (Java Persistence API) erfolgen, die im Beispiel verwendet wird. Neben dem eigentlichen Mapping besteht Hibernate aus einer Reihe von Komponenten, etwa dem Modul Hibernate Search, das Funktionalitäten zur Volltextsuche mit Apache Lucene bereitstellt. In diesem Kapitel soll nur die grundlegende Funktionsweise dargestellt werden. Hibernate Architektur Dazu implementieren wir unsere Beispielanwendung in Java und nutzen Hibernate als Persisten- zschicht. Eine solche Anwendung wird in Java normalerweise als JSP (Java Server Page) realisiert. 1 <%@page import=\"url.*\" contentType=\"text/html\" pageEncoding=\"UTF-8\"%> 2 <!DOCTYPE html> 3 <html> 4 <head> 5 <meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\"> 6 <title>tinyURL Clone</title> 7 </head> 8 <body> 9 <% Entwicklung von Datenbankanwendungen 83 10 String short_url = request.getParameter(\"url\"); 11 URL u = new URL(short_url, request.getRemoteAddr()); 12 if (u.isValid()) { 13 u.visit(); 14 response.sendRedirect(u.getLong_url()); 15 } 16 else { 17 out.println(\"<h1>unknown url: \" + short_url); 18 } 19 %> 20 </body> 21 </html> Die komplette Logik wird in einem Objekt der Klasse URL verwaltet, das aus der übergebenen Kurzform erzeugt wird (Zeile 11). Wurde ein passender Eintrag gefunden (Zeile 12) wird der Aufruf in der Datenbank protokolliert (Zeile 13) und ein HTTP-Redirect-Header erzeugt (Zeile 14). Falls nicht, wird eine Fehlermeldung ausgegeben (Zeile 17). Die Klasse URL - eine Implementierung des DataMapper-Patterns - enthält die Logik zum Zugriff auf die Datenbank: 1 public class URL { 2 private String long_url; 3 private String short_url; 4 private String remote_ip; 5 private int link_id; 6 private boolean valid = false; 7 private EntityManager em; 8 public URL(String short_url, String remote_ip) { 9 em = Persistence.createEntityManagerFactory(\"HiberTestPU\"). createEntityManager(); 10 Query q = em.createNamedQuery(\"Link.findByShortUrl\"). setParameter(\"shortUrl\", short_url); 11 List<Link> result = q.getResultList(); 12 if (result.size() > 0) { 13 valid = true; 14 Link l = result.get(0); 15 this.short_url = short_url; 16 this.link_id = l.getId(); 17 this.long_url = l.getLongUrl(); 18 this.remote_ip = remote_ip; 19 } 20 } Entwicklung von Datenbankanwendungen 84 21 public boolean isValid() { 22 return valid; 23 } 24 public void visit() { 25 Visit v = new Visit(); 26 v.setLinkId(link_id); 27 v.setIp(remote_ip); 28 v.setUrl(short_url); 29 v.setTimestamp(new Date()); 30 em.getTransaction().begin(); 31 em.persist(v); 32 em.getTransaction().commit(); 33 } Die Daten werden im Konstruktor aus der Datenbank gelesen. In Zeile 9 wird die Verbindung zur Datenbank aufgebaut, die Parameter der Verbindung wie Datenbankname und Art, Benutzer und Kennwort stehen in Konfigurationsdateien. Für verschiedene Standard-Abfragen stehen vordefinierte SQL-Anweisungen zur Verfügung, Hibernate generiert SQL-Anweisungen soweit sinnvoll nicht erst zur Laufzeit, sondern vorab. Hier wird eine dieser Abfragen (NamedQuery) geladen und der Parameter shortUrl gesetzt (Zeile 10). Das Ergebnis der Abfrage wird in einer Liste gespeichert (Zeile 11). Wenn ein passender Eintrag gefunden wurde, werden dessen Daten in Attributen des Objekts zur späteren Verwendung gespeichert (Zeilen 12-19). Das Protokollieren des Zugriffs wird in der Methode visit() realisiert, nachdem die Attribute gesetzt sind (Zeilen 26-29) wird eine Transaktion gestartet (Zeile 30), der Datensatz geschrieben (Zeile 31) und die Transaktion mit Commit beendet (Zeile 32). Die Entity-Klassen Link und Visit wurden mit einem wizard der Netbeans-IDE generiert (File- >New->Entity Classes from database). Prinzipiell ist es zwar möglich, diese manuell zu erstellen, da aber in den meisten Fällen ein Datenbankschema bereits vorliegt, ist dieser Weg oft der einfachere. Natürlich kann der generierte Code den eigenen Bedürfnissen angepasst werden. Die Abbildung der Java-Attribute auf die Datenbank wird mit Annotationen dargestellt. In Zeile 2 wird etwa definiert, dass die Daten in der Tabelle “links” gespeichert werden sollen, in den Zeilen 13-18 werden die Eigenschaften des Attributs ID festgelegt und so weiter. 1 @Entity 2 @Table(name = \"links\") 3 @XmlRootElement 4 @NamedQueries({ 5 @NamedQuery(name = \"Link.findAll\", query = \"SELECT l FROM Link l\"), 6 @NamedQuery(name = \"Link.findById\", query = \"SELECT l FROM Link l WHERE l.id = :id\"), 7 @NamedQuery(name = \"Link.findByShortUrl\", Entwicklung von Datenbankanwendungen 85 query = \"SELECT l FROM Link l WHERE l.shortUrl = :shortUrl\"), 8 @NamedQuery(name = \"Link.findByLongUrl\", query = \"SELECT l FROM Link l WHERE l.longUrl = :longUrl\"), 9 @NamedQuery(name = \"Link.findByCreatedAt\", query = \"SELECT l FROM Link l WHERE l.createdAt = :createdAt\"), 10 @NamedQuery(name = \"Link.findByUpdatedAt\", query = \"SELECT l FROM Link l WHERE l.updatedAt = :updatedAt\")}) 11 public class Link implements Serializable { 12 private static final long serialVersionUID = 1L; 13 @Id 14 @GeneratedValue(strategy = GenerationType.IDENTITY) 15 @Basic(optional = false) 16 @NotNull 17 @Column(name = \"id\") 18 private Integer id; 19 @Size(max = 255) 20 @Column(name = \"short_url\") 21 private String shortUrl; 22 @Size(max = 255) 23 @Column(name = \"long_url\") 24 private String longUrl; 25 @Column(name = \"created_at\") 26 @Temporal(TemporalType.TIMESTAMP) 27 private Date createdAt; 28 @Column(name = \"updated_at\") 29 @Temporal(TemporalType.TIMESTAMP) 30 private Date updatedAt; 31 public Link() { 32 } 33 public Link(Integer id) { 34 this.id = id; 35 } 36 public Integer getId() { 37 return id; 38 } 39 public void setId(Integer id) { Entwicklung von Datenbankanwendungen 86 40 this.id = id; 41 } 42 public String getShortUrl() { 43 return shortUrl; 44 } 45 public void setShortUrl(String shortUrl) { 46 this.shortUrl = shortUrl; 47 } Nicht immer ist die Abbildung eines Objekts auf die Datenbank so einfach wie in unserem Beispiel, dann ist es unter Umständen sinnvoll, die Entity-Klassen manuell zu erstellen. .. Nebenbemerkung: Die aktuelle Netbeans Version 7.1 erzeugt in der Entity-Klasse ungültigen Code: ´The following set of constraints on a JPA entity are contradictory: @Id @GeneratedValue(strategy = GenerationType.IDENTITY) @Basic(optional = false) @NotNull The primary key is to auto-generated and thus will be null when EntityManager.persist is invoked. But prePersist throws a BeanValidation exception because of it being null. When @GeneratedValue(strategy=GenerationType.IDENTITY), then @NotNull should not be specified. http://netbeans.org/bugzilla/show_bug.cgi?id=197845 Der NotNull-Constraint muß entfernt werden, um ein lauffähiges Programm zu erhalten. Es gibt eine Reihe von Ansätzen, um Objekte und relationale Datenbanken zusammen zu bringen. Diese versuchen, ein Problem zu lösen, das man nicht hätte, wenn es flexible, leistungsfähige Objektdatenbanken gäbe. Da hier aber keine allgemein einsetzbare Lösung in Sicht ist, muss bei jedem (größeren) Projekt abgewogen werden, wie die Anwendungsdaten in einer relationalen Datenbank gespeichert werden sollen. Hier gibt es kein allgemein gültiges Rezept. Im Bereich der Scriptsprachen hat sich das Active Record-Pattern praktisch durchgesetzt, hier werden aber im Allgemeinen auch keine komplexen Objekthierarchien aufgebaut. Im Java- Umfeld ist Hibernate der de Facto Standard. Hibernate ist ein sehr mächtiges und umfangreiches Werkzeug, erfordert aber einen erheblichen Aufwand zur Einarbeitung und Konfiguration. Dafür können auch komplexe Objektbeziehungen abgebildet werden. Ist dies nicht nötig, gibt es auch Implementierungen des Active Record-Patterns für Java. Alternativ kann der Datenbankzugriff in Data Mapper Objekten auch manuell mit JDBC realisiert werden. Dies bietet sich insbesondere an, wenn nur wenige, dafür aber komplexe Objekte abgebildet werden müssen. Erweiterte Konzepte von Datenbanksystemen Relationalen Datenbanken liegt ein überzeugend einfaches Konzept zu Grunde: Die Daten werden in unabhängigen Tabellen gespeichert und können in Abfragen beliebig kombiniert werden. Auch die möglichen schreibenden Operationen sind einfach: Anlegen eines Datensatzes, Ändern eines oder mehrerer Datensätze und Löschen eines oder mehrerer Datensätze jeweils in einer einzelnen Tabelle. Die mathematische Grundlage dieser Systeme ist die Relationenalgebra, in deren Rahmen sich beweisen lässt, dass die relationalen Operationen zur Abfrage (Selektion, Projektion und Join bzw. Produkt) als Ergebnis immer wieder eine Relation ergeben (Abgeschlossenheit) und das alle überhaupt denkbaren Abfragen durch eine Verknüpfung dieser Operationen realisiert werden können (Vollständigkeit). Diese beruhigende Art von Sicherheit ist in der Informatik selten. Als Edgar F. Codd im Jahr 1970 seinen berühmten Artikel ³ publizierte, war die Neugier groß, der Spott folgte umgehend: Es schien vielen unvorstellbar, dass ein solches System performant realisierbar sein könnte. Insbesondere Abfragen über mehrere Tabellen (Joins) schienen nur schwer effizient realisierbar. Der Wettlauf der Anbieter begann. Die ersten Akteure, System R von IBM, Oracle und Ingres sind auch heute noch wichtige Spieler im Markt der relationalen Datenbanken ⁴. Im Rückblick hatten beide Parteien recht: Die effiziente Implementierung relationaler Operatio- nen ist, durch die Forderung nach Transaktionen weiter erschwert, problematisch. Die Hersteller bieten in ihren Systemen zwar eine Vielzahl von Hilfsmitteln und Optimierungen an, ohne ein zumindest grundlegendes Verständnis der internen Funktionsweise dieser Systeme gerät ein Anwender aber leicht in eine Situation, in der die Performance plötzlich (zu) schlecht ist. Und zwar nicht um einige Prozent, sondern um Größenordnungen. Die interne Architektur relationaler Datenbanken wurde für einen Spezialfall entwickelt, näm- lich den, dass sich die Datenbank auf einem einzigen Rechner befindet und erheblich größer als der verfügbare Hauptspeicher ist. In den achtziger Jahren des letzten Jahrhunderts war dieser Spezialfall die Regel, Mainframes und/oder Supermini-Computer waren die vorherrschende Technologie in den Rechenzentren, viele gleichzeitige Anwender sorgten dafür, dass der Haupt- speicher immer zu knapp war. Heute stellt sich die Situation etwas anders dar. Es gibt zwei große Anwendungsbereiche relationaler Datenbanken, die OLTP-Systeme und die Data Warehouses. Bei ersteren handelt es sich um Systeme zur Prozessunterstützung. Typisch sind hier viele gleichzeitige Anwender, viele, kleine Datensätze und viele kleine Transaktionen, d.h. schreibende Zugriffe. Beispiele sind ³Edgar F. Codd: A Relational Model of Data for Large Shared Data Banks. In: Communications of the ACM. ACM Press, New York 13. Juni 1970, S. 377–387. ⁴Ingres weiter entwickelt als PostgreSQL, System R als DB/2 Erweiterte Konzepte von Datenbanksystemen 88 etwa ERP- oder CRM-Systeme, Buchungssysteme von Banken oder Börsen, die Systeme von Ver- sicherungen, Buchungssysteme von Fluggesellschaften usw. Typische Größen der Datenbanken liegen hier unter einem Terabyte, in der Regel eher bei einigen Gigabyte. Ganz anders bei den Data Warehouses. Hier werden historische Daten vorgehalten um Auswer- tungen zu ermöglichen. Die Anzahl der Anwender ist eher gering, die Daten werden praktisch ausschließlich gelesen, Transaktionen sind weniger wichtig. Aber die Datenbanken sind viel größer: Terabyte bis hin zu Petabyte sind durchaus üblich. Klassische relationale Datenbanken sind auf die Realisierung von OLTP-Systemen hin optimiert. Hier sind aber zwei wichtige Entwicklungen zu beachten: Erstens sind solche Systeme heute oft über die ganze Welt verteilt. Die Etablierung des weltweiten eCommerce führte dazu, dass die Zugriffe auf Systeme nicht mehr nur von wenigen Punkten aus erfolgen, sondern von überall her rund um die Uhr. Sind die Daten an einer Stelle zentralisiert, sorgen die unvermeidlichen Latenzen bei der Übertragung für eine schlechte User experience. Da dies oft nicht akzeptabel ist, müssen die Daten verteilt werden, das Paradigma einer einzelnen Datenbank hält nicht mehr. Die in den letzten Jahren immer stärker ins Interesse rückenden NoSQL-Datenbanken sind in einem solchen Szenario viel leistungsfähiger und einfacher anzuwenden. Der zweite entscheidende Aspekt ist, dass Datenbanken heute viel leichter komplett im Haupt- speicher gehalten werden können. Dies ist letztlich unvermeidlich, da sich die Größe einer OLTP- Datenbank im Lauf der Zeit mit dem Wachstum des Unternehmens (Zahl der Geschäftstransak- tionen, in der Regel also einige Prozent pro Jahr) entwickelt, die Größe des Hauptspeichers aber sehr viel stärker wächst (sich nach dem Moor’schen Gesetz alle zwei Jahre verdoppelt). Selbst für Datenbanken, die heute noch nicht in den Hauptspeicher passen, dürfte dieser Zeitpunkt bald gekommen sein. Und dann ergibt die Architektur klassischer relationaler Datenbanken keinen Sinn mehr, hier sind neue Ansätze gefragt. In Memory Datenbanken werden deshalb immer populärer. Bei den Data Warehouse-Systemen, also Systemen, die auf die Auswertung großer Datenmengen ausgelegt sind, setzen sich zwei Arten von Systemen immer mehr durch: Im BigData-Umfeld die NoSQL-Datenbanken, die mit map-reduce-jobs die Last auf viele Rechner verteilen und die Column-Stores im klassischen Warehouse Bereich: Hier werden die Daten nicht zeilenweise, wie bei den „normalen“ relationalen Datenbanken gespeichert, sondern spaltenweise. Das ist sinnvoll, da das Datenmodell von Data Warehouses meistens aus einer sehr großen Tabelle mit sehr vielen Spalten (zig bis hunderte) besteht („Star Schema“), von denen bei einer Abfrage nur eine kleine Auswahl (typisch eine einzige bis einige wenige) als Ergebnis benötigt werden. Werden die Daten spaltenweise gespeichert, werden keine unnötigen Daten gelesen und die Daten können viel besser komprimiert werden. Aus diesen Gründen, die das Leben im Bereich Datenbanken zur Zeit sehr spannend machen, wird in den folgenden Kapiteln zunächst die Funktionsweise relationaler Datenbanken dargestellt. Also die Fragen beantwortet, wie die Daten eigentlich gespeichert werden, wie Caches funktion- ieren, wie die Suche nach Datensätzen durch Indices beschleunigt wird und wie Joins realisiert werden. Daran anschließend werden Transaktionen behandelt, also die Frage beantwortet, wie es relationale Datenbanken ermöglichen, dass viele Anwender gleichzeitig auf die Daten zugreifen, ohne sich gegenseitig zu stören. Im zweiten Teil werden verteilte Datenbanken behandelt, also der Fall, dass sich die Daten nicht nur auf einem Rechner befinden. Dies führt dann zu einer Diskussion der Probleme, die Erweiterte Konzepte von Datenbanksystemen 89 relationale Datenbanken in diesem Bereich haben und der derzeitigen Entwicklungen, diese mit anderen Systemen, den NoSQL-Datenbanken zu lösen. Da die Architektur und Anwendung ein weitverzweigtes Gebiet ist, das viele Bereiche der Wirtschaftsinformatik betrifft, enthalten diese Kapitel eine Reihe von Fußnoten, die Zusatzinformationen für tiefergehend interessierte Leser enthalten. Leser, die nur einen Überblick über dieses Gebiet gewinnen wollen, können diese getrost ignorieren. Gelegentlich wird in den folgenden Kapiteln ein konkretes Beispiel gebraucht, dazu wird der folgende Ausschnitt aus einem einfachen Onlineshop verwendet: Beispiel Bestellung. Konzeptionelles ERM für die folgenden Kapitel Erweiterte Konzepte von Datenbanksystemen 90 Performance Datenbanken werden dann verwendet, wenn viele Benutzer gleichzeitig auf große Datenmengen zugreifen. Natürlich kann man Datenbanken auch dann verwenden, wenn nur wenige Daten verwaltet werden sollen und/oder nur ein Benutzer auf die Daten zugreift. Für diese Fälle existieren aber, von Ausnahmen abgesehen, bessere Lösungen ⁵. Schon deshalb ist das Thema Performance bei Datenbanken zentral. Bei relationalen Datenbanken werden die Daten auf mehrere Tabellen verteilt, die dann letztlich mit Joins wieder zusammengeführt werden müssen. Insbesondere hier ist die Performance unter Umständen problematisch. Dieses Kapitel bietet einen Überblick über die für die Performance entscheidenden Aspekte relationaler Datenbanken. Ziel ist es, ein Verständnis für mögliche Probleme zu schaffen, das hilft, beim Entwurf einer Datenbank oder Anwendung potentielle Probleme früh zu erkennen. Eine existierende Datenbank oder Anwendung muss in der Regel getunt werden, um akzeptable Performance zu bieten. Obwohl sich selbst tunende Datenbanken seit vielen Jahren Gegenstand der Forschung (und der Prospekte der Hersteller) sind, ist dies noch längst nicht Realität. Das Tuning von Datenbanken und Anwendungen ist in weiten Teilen systemspezifisch und wird deshalb hier nicht behandelt. Speicherstrukturen Die heute verfügbaren relationalen Datenbanken wie Oracle, DB/2, SQL-Server oder MySQL basieren alle auf dem gleichen Prinzip. Wenn in diesem Kapitel Details beschrieben werden, wird Oracle als exemplarisches Beispiel verwendet. Jede relationale Datenbank besteht aus einzelnen, voneinander unabhängigen Tabellen, die jeweils aus einzelnen Zeilen bestehen, die jeweils aus einzelnen Attributen zusammengesetzt sind. Die kleinste adressierbare Einheit ist eine Zeile. Da die Attribute in der Regel variable Längen haben, ist die Größe der Zeilen i.A. unterschiedlich. Da sich die Daten wegen der beschränkten Größe des Hauptspeichers (RAM) auf der Festplatte befinden (Alle diese Systeme wurden in den siebziger und achtziger Jahren entwickelt. Typische Größen waren hier einige (zig) Megabyte RAM und einige Gigabyte Festplatte. Eine Datenbank mit einer Größe von einem Gigabyte galt als sehr groß.), müssen diese Datensätze variabler Größe effizient auf Festplatten gespeichert werden. Um dies, insbesondere bei Änderungen, zu ermöglichen, werden die Datensätze in Seiten fester Größe zusammengefasst. Typische Werte sind hier etwa 2kB-8kB, manchmal auch bis zu 32kB. Da ein solcher Block mehrere Zeilen von unter Umständen mehreren Tabellen enthalten kann, wird ein komplexer Header und eine komplexe Kodierung nötig ⁶. Da Zugriffe auf die Platte vergleichsweise langsam sind, wird ein Teil des Hauptspeichers dazu verwendet, wichtige, d.h. oft benötigte Seiten zu puffern. Auch diese Funktion wird durch Seiten fester Größe erheblich vereinfacht. ⁵Andererseits wird als Standard-Speichertechnologie unter iOS die SQLite-Engine verwendet, eine – wenn auch abgespeckte – relationale Datenbank. Also sind offensichtlich nicht alle Experten der gleichen Meinung wie der Autor. ⁶Siehe etwa http://orafaq.com/papers/dissassembling_the_data_block.pdf, ein hochinteressantes Dokument, zum Zeitpunkt des Erscheinens 2005 nicht öffentlich, beschreibt tools und Techniken um den internen Aufbau einer Oracle Datenbank zu lesen und zu modifizieren: “It is intended for internal use only and the company never publishes any details about it. It is a very powerful tool but also extremely dangerous since it can change and/or corrupt data blocks of any Oracle database. […] Note that the software will not start unless a password is supplied […] The password is hardcoded by Oracle […] If you know enough about Oracle internals […] you should be able to determine the password yourself using standard reverse engineering techniques. Auch eine Art von Konsistenzsicherung ;-) Alternativ bietet auch die offizielle Dokumentation die wichtigsten Dinge im Überblick, siehe etwa http://docs.oracle.com/cd/B28359_01/server.111/b28318/logical.htm#CNCPT004 Erweiterte Konzepte von Datenbanksystemen 91 Datenbankseiten Im Wesentlichen besteht eine Datenbankseite aus einem Header, den eigentlichen Daten und einer Reserve: Würde eine Seite komplett mit Daten gefüllt, wäre bei jeder Änderung an den Daten, die zusätzlichen Platz benötigt, keiner mehr frei und es müssten Datensätze in eine andere Seite verschoben werden. Dies ist (insbesondere auf Festplatten) sehr aufwendig. Aufbau einer Seite (bei Oracle, vereinfacht) Um den (sequentiellen) Zugriff auf die Datensätze einer Tabelle zu beschleunigen, sollten diese möglichst “am Stück” auf der Platte liegen, sich also möglichst in aufeinanderfolgenden Seiten befinden. Ist dies nicht der Fall, muss der Kopf der Festplatte jeweils neu positioniert werden, bevor die Daten gelesen werden können. Dies (und das Drehen der Festplatte) dauert vergleich- sweise lang. Geht man von einer mittleren Zugriffszeit von 5msec aus, können nur 200 Seiten pro Sekunde gelesen werden, wenn diese über die Platte verteilt sind. Bei einer Größe von 4kB wären das 0,8 MByte/Sekunde. Verglichen mit einer typischen Übertragungsrate von etwa 100 MByte/Sekunde für sequentielle Zugriffe wäre dies um etwa zwei Größenordnungen langsamer. Deshalb werden Tabellen in Extents, einer sequentiellen Gruppe von Seiten, organisiert. Einzelne Seiten werden zu Extents zusammengefasst. Tabellen bestehen aus mehreren Extents Dies ist nötig, da beim Einfügen neuer Datensätze oft Zeilen in unterschiedliche Tabellen gemeinsam eingefügt werden. Nehmen wir unseren Online-shop als Beispiel. Dort wird bei jeder Bestellung ein Eintrag in der Tabelle Bestellung und einer oder mehrere in der Tabelle Bestellposition gemacht. Würden keine Extents verwendet, würden schließlich auf der Platte immer abwechselnd Seiten mit Bestellpositionen und Bestellungen liegen. Dies würde die Erweiterte Konzepte von Datenbanksystemen 92 Lesegeschwindigkeit zwar nicht um den Faktor hundert, sondern nur etwa um den Faktor zwei reduzieren (beim Lesen aller Bestellungen müssten alle Bestellpositionen mitgelesen werden), bei komplexeren Modellen würde das Problem aber immer größer werden. Cache Bei der Bearbeitung von Anfragen ⁷ müssten die Daten immer wieder von der Platte gelesen werden. Dies wäre sehr langsam. Deshalb werden die Daten im Hauptspeicher gepuffert. Die Zugriffe der Datenbank auf Seiten erfolgt generell durch den Cache: Wird eine Seite angefordert, wird geprüft, ob sie im Cache ist. Wenn nein, wird Platz im Cache geschaffen, die am längsten nicht benutzten Seiten werden ausgelagert, und einige Seiten von der Platte gelesen und an der nun freien Stelle im Cache platziert. Um zu wissen, welche Seiten häufig benötigt werden, werden entsprechende Informationen gepflegt. Für jede Seite im Cache wird gespeichert, wann das letzte Mal zugegriffen wurde (LRU). Cache einer relationalen Datenbank. Der Cache puffert einzelne Seiten der DB. Aufeinanderfolgende Seiten im Cache müssen nicht zwingend auf der Platte benachbart sein. Bei einem schreibenden Zugriff wird zunächst nur im Cache geändert, die Seite wird markiert, so dass sie (spätestens dann) auf Platte geschrieben wird, wenn sie aus dem Cache entfernt werden soll. Reale Implementierungen müssen sicherstellen, dass Zugriffe auf große Tabellen (Full Table Scans oder Zugriffe auf LOBs) nicht alle Seiten aus dem Cache verdrängen und ihn dadurch praktisch nutzlos machen. Oracle etwa markiert Seiten aus solchen Zugriffen so, dass diese beim nächsten Zugriff wieder überschrieben werden (können), also faktisch nur die Seiten einer Leseoperation im Cache bleiben und dadurch nur einen kleinen Teil des Platzes einnehmen können. Im Detail ist die Realisierung und das Tuning des Caches nicht trivial. Heute (2014) sind einige dieser Überlegungen hinfällig: Befindet sich die Datenbank auf SSD- Laufwerken, ist der Performance-Unterschied zwischen sequentiellem und zufälligem Zugriff bei weitem nicht mehr so groß, es spielt praktisch keine große Rolle mehr, an welcher Stelle die Daten gespeichert sind. Man könnte auch gleich einen Schritt weiter gehen und die Datenbank komplett im Haupt- speicher halten. Dann wäre es nicht mehr nötig, mit Seiten fester Größe zu arbeiten und der buffer Cache könnte entfallen. Für OLTP-Systeme ist auch heute noch eine Größe von einigen zig Gigabyte bis unter einem Terabyte in der Regel ausreichend. Diese Größenordung könnte heute relativ bequem im Hauptspeicher gehalten werden. Dadurch würden die Datenbanksysteme erheblich vereinfacht und die Performance massiv verbessert werden. ⁷natürlich auch beim Einfügen, denn auch hier müssen Datensätze gesucht werden, etwa um Duplikate zu vermeiden usw. Erweiterte Konzepte von Datenbanksystemen 93 Leider würde in fast allen Bereichen des Systems eine komplett andere Architektur verwendet werden müssen, das heißt, die großen Datenbankhersteller könnten ihre existierenden Systeme praktisch wegwerfen. Bekannte Vertreter dieser In-Memory-Architektur sind SAP Hana und Volt DB. Erweiterte Konzepte von Datenbanksystemen 94 Zugriffspfade und Indices Beim Zugriff auf Datensätze sind zwei wichtige Spezialfälle zu unterscheiden: Eine einzelne Zeile oder alle Zeilen einer Tabelle (ein sogenannter Full Table Scan). Beim Full Table Scan ist die bereits besprochene Organisation der Dateien auf der Festplatte und der Aufwand für das Management des Caches und die Dekodierung der Seiten entscheidend. Bei der Suche nach einem bestimmten Datensatz kommt es darauf an, ob eine Sortierung der Daten ausgenutzt werden kann. Sind die Daten etwa nach dem Primärschlüssel sortiert in der Datenbank gespeichert, kann ein schneller Algorithmus zur Lokalisierung der entsprechen- den Zeile verwendet werden, der statt linearer logarithmische Performance bietet. Da bei relationalen Datenbanken häufig eine Zeile mit einem bestimmten Primärschlüssel gesucht wird, speichern viele Datenbanksysteme die Zeilen (zumindest optional) nach diesem sortiert ab. Insbesondere beschleunigt dies die Suche bei der Verfolgung einer Relationship (Fremd- schlüssel → Primärschlüssel) und das Einfügen von Datensätzen mit fortlaufendem Schlüssel: Wird als Primärschlüssel eine aufsteigende Nummer verwendet, werden die Datensätze trotz sortierter Speicherung ohne Performance-Verlust immer hinten angehängt, gleichzeitig wird die (notwendige) Suche zur Überprüfung der Eindeutigkeit des Schlüssels sehr schnell: Es muss nur der Schlüssel des bisher letzten Datensatzes überprüft werden. Die Popularität der Verwendung solcher automatisch erzeugten IDs erklärt sich aus dieser Tatsache (und natürlich aus der Bequemlichkeit der Anwendungsentwickler). Für Suchen nach anderen Attributen ist dies aber nicht hilfreich, hier werden zusätzliche Datenstrukturen verwendet, die Indices. Ein Index funktioniert ähnlich wie der (Schlagwort- ) Index in einem Buch: Statt das gesamte Buch durchblättern zu müssen, wird der Begriff im sortierten Index gesucht. Dieser enthält einen Verweis auf die Seitennummer (im Buch wie in der Datenbank). In Datenbanken wird allerdings keine sortierte Liste der Werte verwendet, sondern meistens ein Baum. Dieser ist beim Einfügen von Werten erheblich effizienter. In praktisch allen Daten- banksystemen wird als Standard eine Variante des B-Baums verwendet. B-Baum mit n=2. Jeder Knoten außer der Wurzel hat beim B-Baum zwischen n und 2*n Nachfolgern In einem Binärbaum hat jeder Knoten höchstens zwei Nachfolger, im B-Baum eine erheblich größere Anzahl, beispielsweise n=1000. Dies ist erstens erheblich schneller bei der Suche, da die Zahl der besuchten Knoten etwa dem Logarithmus zur Basis der Nachfolgerzahl entspricht. Bei einem Binärbaum sind so zur Suche in einer Million Datensätzen etwa log2 10 6 also etwa 20 Erweiterte Konzepte von Datenbanksystemen 95 Zugriffe nötig, bei einem B-Baum etwa log1000 = 10 6 also 2 ⁸. Berücksichtigt man noch, dass sich ein Teil des Index im Cache befindet (Indices werden wie andere Datenbankobjekte auch im Buffer Cache gehalten), kann man in der Regel davon ausgehen, dass unabhängig von der Zahl der Datensätze maximal 2 Plattenzugriffe erforderlich sind, um einen Datensatz zu lesen: Einer auf den Index und einer auf den eigentlichen Datensatz. Hashtabelle der Größe N=7. Die Einträge der Hashtabelle enthalten den Schlüssel und einen Verweis auf die Seite, die den entsprechenden Datensatz enthält. Die Hashfunktion ist k mod N, z.B. wird der Schlüssel 17 an der Stelle 17 mod 7 = 3 gespeichert. 12 mod 7 ergäbe 5, dieser Platz ist bereits belegt, der Wert wird an der nächsten freien Stelle gespeichert (linear probing). Eine andere Technik zur Beschleunigung des Zugriffs auf die Datensätze ist die Verwendung von Hash-Tabellen. Hier wird der Datensatz nicht gesucht, sondern seine Seitennummer wird unter Verwendung einer Tabelle berechnet. Eine Hashfunktion H(k) liefert einen Index in eine Hashtabelle, die dann an der entsprechenden Stelle die Seitennummer enthält. Da die Hashfunktion nicht eindeutig ist (sie bildet ja gerade von einem großen Wertebereich des Schlüssels auf einen erheblich kleineren Wertebereich, nämlich die Größe der Hashtabelle, ab), also für unterschiedliche Schlüssel k und k’ den gleichen Hashwert liefern kann H(k)=H(k’), muss ein geeignetes Verfahren zur Auflösung dieser Kollisionen verwendet werden. Dieses macht die Berechnung der Anzahl der nötigen Plattenzugriffe komplizierter. Passt die Hashtabelle jedoch in den Hauptspeicher, was aufgrund der kleineren Größe pro Datensatz wahrscheinlicher ist als beim Baum, ist der Zugriff schneller. Allerdings sind keine Range- Scans, also Abfragen der Art “A>x AND A<y” möglich. Diese sind insbesondere bei Strings wichtig, wenn am Ende ein Wildcard-Zeichen verwendet wird. Diese Art von Abfragen kann durch Bäume leicht unterstützt werden (Ergebnis ist dann ein Teilbaum), eine Hashtabelle hilft hier nicht weiter. Aber insbesondere bei In-Memory Anwendungen sind Hashtabellen gebräuchlich, beispielsweise unterstützt die Memory-Engine von MySQL sowohl hash als auch B-Baum Indices. Für spezielle Anwendungen existieren weitere spezielle Indextypen. In Data Warehouses werden oft Abfragen verwendet, die eine große Anzahl von Zeilen liefern, die dann aggregiert werden. Nehmen wir an, es gäbe eine Tabelle, die für jeden Verkaufsvorgang eine Zeile enthält. Es würde dann über alle Verkäufe einer bestimmten Warengruppe (etwa Getränke) gruppiert ⁸Die genauen Zahlen hängen von den Einzelheiten ab. Im B-Baum ist jede Seite mindestens halb gefüllt (im Mittel also dreiviertel voll), im B*-Baum zu mindestens ⅔ (im Mittel also ⅚). Je nachdem, auf welcher Ebene des Baumes sich der gesuchte Schlüssel befindet (meistens auf der untersten), sind unterschiedlich viele Zugriffe nötig. Beim B+-Baum enthalten nur die Seiten der untersten Ebene Daten usw. Erweiterte Konzepte von Datenbanksystemen 96 nach Zeiträumen (etwa Monaten) summiert. Eine solche Abfrage liefert zunächst einen großen Anteil der Datensätze (vielleicht einige Prozent, wenn es sich insgesamt um Millionen oder gar Milliarden von Datensätzen handelt, ist das sehr viel). Hier unterstützt ein Baum oder eine Hashtabelle nicht, eine solche Abfrage wird schneller ausgeführt, wenn gar kein Index verwendet, sondern ein Full Table Scan durchgeführt wird. Bitmap-Index. Abbildung aus [http://de.wikipedia.org/wiki/Bitmap-Index] Name männlich weiblich ledig verheiratet geschieden Anne 0 1 0 1 0 Emil 1 0 0 0 1 Fritz 1 0 0 1 0 Hans 1 0 0 1 0 Willi 1 0 1 0 0 Eine solche Abfrage kann durch einen Bitmap-Index unterstützt werden. Jede Zeile einer Tabelle erhält hier zusätzlich zu den Daten eine Folge von Nullen und Einsen, ein Bit für jeden möglichen Wert des Attributs. Handelt es sich beispielsweise um das Geschlecht einer Person, werden zwei Bit pro Zeile benötigt, eins für “männlich” und eins für “weiblich”. Handelt es sich um die Warengruppe und gäbe es 23 verschiedene Warengruppen, werden 23 Bit je Zeile benötigt, bei der alle Bits bis auf das dem aktuellen Wert entsprechende gleich null sind. Werden nun Datensätze, etwa einer bestimmten Warengruppe, gesucht, wird passend zur Abfragebedingung eine entsprechende Bitfolge konstruiert. Diese wird dann per bitweisem UND mit der Bitfolge jeder Zeile verglichen. Diese Operation kann, je nach Anzahl der Möglichkeiten, sehr schnell ausgeführt werden, da direkt als CPU-Befehl realisiert. Typische ad-hoc Abfragen in Data Warehouses können so erheblich beschleunigt werden. Auf den ersten Blick scheint dieses Verfahren unnötig viel Speicher zu benötigen, da nur eine einzige Eins und sehr viele Nullen gespeichert werden. Dies ist aber nicht der Fall: Die Daten werden üblicherweise komprimiert - etwa per RLL - gespeichert, so dass letztlich nur sehr wenig Platz benötigt wird. An sich für Abfragen mit einer niedrigen Selektivität entwickelt, also solchen, die einen erheblichen Anteil der gesamten Datensätze zurückliefern, zeigt sich in der Praxis, dass Bitmap- Indices auch bei solchen mit einer hohen Selektivität geeignet sind: Selbst bei einem Index auf dem Primärschlüssel ist ein Bitmap-Index nicht langsamer als ein B-Baum, wie eine Untersuchung von Oracle zeigt ⁹. Allerdings sind Bitmap-Indices nicht für OLTP-Systeme geeignet, sondern nur für i.w. read-only Systeme wie Data Warehouses. Es existiert eine Vielzahl weiterer Indextypen, etwa zur Unterstützung von Abfragen in ge- ografischen Informationssystemen, zur Suche in Texten und so weiter. Die Auswahl des Indexes hängt im Wesentlichen von der Art der Abfragen ab, bei Stan- dardanwendungen relationaler Datenbanken werden von praktisch allen Systemen B-Bäume verwendet. ⁹http://www.oracle.com/technetwork/articles/sharma-indexes-093638.html Erweiterte Konzepte von Datenbanksystemen 97 Joins Bei relationalen Datenbanken werden die Daten über mehrere Tabellen verteilt. Bei Anfragen müssen diese in der Regel wieder zusammengeführt werden. Dazu werden Joins verwendet. Nehmen wir etwa an, dass wir eine Bestellung ausdrucken wollen, dann müssen die entsprechen- den Datensätze aus den verschiedenen Tabellen kombiniert werden: Für den Kopf aus Kunde und Bestellung, für die Positionen aus Bestellposition und Artikel. SELECT * FROM Bestellung INNER JOIN Bestellposition ON Bestellung.ID = Bestellposition.BestellNummer In der Relationenalgebra ist dies einfach: Zunächst wird das kartesische Produkt der Menge aller Zeilen aus Kunde und dener aus Bestellung gebildet. Dann werden mit einer Selektion diejenigen ausgewählt, für die die Join-Bedingung erfüllt ist. Dieses Vorgehen wäre allerdings in der Realität nur sehr ineffizient zu realisieren: Nehmen wir an, es gäbe 1000 Kunden mit insgesamt 2000 Bestellungen, dann müsste zunächst das Produkt beider Mengen gebildet werden, das wären 2 000 000 Datensätze, aus denen dann der eine (oder die wenigen) ausgewählt wird, bei dem die Joinbedingung gleicher Kundennummer erfüllt ist. Es ist offensichtlich, dass effizientere Realisierungen denkbar sind. Wären die Daten als Arrays im Speicher vorhanden, würde ein Join-Programm vermutlich etwa so aussehen: Bestellposition [] B; Artikel [] A; for(int i=0;i<B.length;i++) { for(int j=0;j<A.length;j++) { if (B[i].Bestellnummer == A[j].Bestellnummer) { doSomething(A[i],B[j]); … Im Prinzip genau so funktioniert der nested-loop-join. Für jeden Datensatz der äußeren Relation müssen alle Datensätze der inneren Relation gelesen und verarbeitet werden. Das ist so lange effizient, wie die beteiligten Relationen nur wenige Datensätze enthalten. Halbwegs effizient so lange, wie zumindest die äußere Relation nur wenige Datensätze enthält. Nehmen wir an, es gäbe 10000 Bestellpositionen und 100 Artikel. Dann müssten 1 Million Vergleiche vorgenommen werden. Existiert für das Join-Attribut auf der inneren Relation (hier Artikel) ein Index, wird für jeden Datensatz aus B durch Nachschlagen im Index der passende Datensatz gefunden. Erweiterte Konzepte von Datenbanksystemen 98 Bestellposition [] B; Artikel [] A; for(int i=0;i<B.length;i++) { int j = A.lookup(B[i]); doSomething(A[i],B[j]); … Die innere Schleife wird also durch einen Zugriff auf den Index ersetzt. Statt 1 Million Vergleiche werden jetzt 10000 Index-Lookups benötigt. Die dafür benötigte Zahl an Operationen hängt logarithmisch von der Zahl der Datensätze ab, ist also erheblich geringer. Dieser Fall ist in relationalen Datenbanken sehr verbreitet, nämlich dann, wenn entlang einer Relationship vom Fremdschlüssel in der einen auf den Primärschlüssel einer anderen Tabelle verwiesen wird. Deshalb ist es sinnvoll, wenn ein (B-Baum) Index für den Primärschlüssel jeder Relation angelegt wird (auch beim Einfügen ist das sinnvoll, denn dabei muss geprüft werden, ob der neue Schlüsselwert bereits existiert). Viele Datenbanken machen das deshalb automatisch. Dies gilt aber nur, wenn der Index-Lookup nur einen (oder wenige) Datensätze zurückliefert. Hat man Abfragen, die eine große Zahl an Werten zurückliefern, hilft dieses Verfahren wenig. Soll beispielsweise für jeden Artikel der mittlere, in Bestellungen vorkommende Wert (also Menge*Preis) berechnet werden, würde die Verwendung eines Indexes kaum Aufwand einsparen (eher im Gegenteil). Für solche Fälle existieren andere Join-Verfahren, etwa der sort-merge-join. Dabei werden zunächst beide Tabellen jeweils nach den Attributen der Join-Bedingung sortiert. Erweiterte Konzepte von Datenbanksystemen 99 Sort-merge-join über zwei Tabellen, Bestellung und Bestellposition In jeder der Relationen wird ein Zeiger zunächst auf den ersten Datensatz gesetzt. Dann wird der Zeiger in der rechten Relation solange weiter bewegt, bis der aktuelle Wert der linken Seite gefunden wird. Dann wird der Zeiger links weiterbewegt, bis ein neuer Wert auftaucht. Dann wieder rechts usw. Sind die Daten bereits sortiert, hängt die Laufzeit linear von der Zahl der Datensätze ab, es ist nur ein Durchgang durch beide Tabellen nötig. Sortiert sind die Daten insbesondere dann, wenn auf beiden Attributen ein Index existiert. Beim Primärschlüssel ist dies in der Regel der Fall. Es ist deshalb oft sinnvoll, auf Fremdschlüsselattributen ebenfalls einen Index zu erzeugen, wenn diese in solchen Abfragen verwendet werden. Joins über kleine Tabellen, oder auch über zwei große Tabellen können mit den beschriebenen Verfahren effizient realisiert werden. Diese stellen in der Praxis kein Problem dar. Anders sieht es aus, wenn sich ein Join über mehrere Tabellen erstreckt. Dann kann, insbesondere wenn es sich um Tabellen mit vielen Datensätzen handelt, die Performance sehr schlecht sein. In solchen Fällen ist zu überlegen, ob es nicht sinnvoller ist, das Datenmodell zu denormalisieren um die Zahl der beteiligten Tabellen zu reduzieren. Erweiterte Konzepte von Datenbanksystemen 100 Optimierung von Abfragen Zunächst wird die SQL-Anweisung analysiert und in eine Normalform transformiert. So können etwa Subqueries in Joins umgewandelt werden. Soll eine Abfrage (insbesondere über mehrere Tabellen) ausgeführt werden, gibt es verschiedene Möglichkeiten zur Realisierung, die die Performance unter Umständen dramatisch beeinflussen. Bei einem nested-loop-join etwa kann die Entscheidung darüber, welche Tabelle in der äußeren und welche in der inneren Schleife bearbeitet wird, erheblichen Einfluss auf die Performance haben. Es ist zu prüfen, ob es sich lohnt, Daten vor der eigentlichen Ausführung der Abfrage zu sortieren, ob Indices verwendet werden sollen usw. Der Optimizer analysiert die SQL-Anweisung und legt einen geeigneten Ausführungsplan fest. Dazu werden im Wesentlichen entweder regelbasierte Ansätze (“wenn ein Index auf einem Attribut existiert, wird dieser verwendet”) oder kostenbasierte Ansätze verwendet. Bei diesen versucht der Optimizer abzuschätzen, wie hoch der tatsächliche Aufwand für die Ausführung einer Abfrage ist (hinsichtlich CPU und IO) und auf dieser Basis eine optimale Aus- führungsstrategie zu wählen. Kostenbasierte Optimizer sind aufwendiger als regelbasierte und können sich bezüglich der Kosten auch irren. Deshalb ist es sinnvoll, sich den Ausführungsplan einer Abfrage anzeigen zu lassen, um diesen ggf. zu beeinflussen. Die Möglichkeiten hierfür sind je nach Datenbanksystem unterschiedlich. Zusammenfassung Relationale Datenbanken organisieren Daten in Tabellen, die zeilenweise gespeichert werden. Die Daten werden in Seiten fester Größe gespeichert und im Hauptspeicher gepuffert. Die Seiten einer Tabelle werden in Gruppen gespeichert, um die Zugriffszeit zu minimieren. Ein Datensatz kann entweder durch sequentielles Suchen in der ganzen Tabelle oder durch Nachschlagen in einem Index gefunden werden. Am gebräuchlichsten sind B*-Bäume, die eine große Anzahl von Schlüsseln je Seite enthalten. Für Spezialanwendungen existieren spezielle Indextypen. Indices sind entscheidend für die Performance von Joins. Joins ohne Verwendung von Indices sind langsam, bei größeren Tabellen und/oder komplexen Abfragen zu langsam. Erweiterte Konzepte von Datenbanksystemen 101 Recovery Klassisch wird eine Datensicherung zu festen Zeiten, etwa jede Nacht durchgeführt. Für Nutzerdokumente ist dies oft ausreichend, gibt es etwa im Lauf eines Tages einen Fehler, kann der Stand der letzten Nacht wiederhergestellt werden, die bisherige Arbeit des Tages ist verloren. Handelt es sich beispielsweise um einen Text, kann dieser meist mit vertretbarem Aufwand manuell neu erstellt werden. Datenbanken und darauf aufbauende Systeme werden in der Regel zur Speicherung von Bewegungsdaten von Prozessen verwendet. Diese stammen aus den unterschiedlichsten Quellen, teils von Mitarbeitern erfasst, teils von Kunden direkt eingegeben, teils aus anderen Systemen übernommen, teils durch Sensoren direkt eingegeben. Gehen hier Daten verloren, sind diese praktisch nicht wieder herstellbar. Deshalb müssen andere, leistungsfähigere Verfahren verwen- det werden, es muss ein kontinuierliches Backup durchgeführt werden. Logfile als Basis eines kontinuierlichen Backups Zum Zeitpunkt t=0 wird ein komplettes Backup durchgeführt, das alle Daten zu diesem Zeitpunkt enthält. Wird nun ein Datenblock geändert, wird die Änderung in einem Logfile (oder auch Journal oder Protokoll) festgehalten bevor die eigentlichen Daten geändert werden. Es handelt sich hier praktisch um ein kontinuierliches, inkrementelles Backup. Im Fehlerfall werden das letzte komplette Vollbackup sowie alle darauf folgenden Log-Einträge eingespielt (Recovery). Dieses Vorgehen liefert dann den aktuellen Stand zum Zeitpunkt des Fehlers. Dies bedeutet allerdings nicht, dass dieser Zustand konsistent ist, wie das folgende Beispiel zeigt: Nehmen wir an, es gäbe eine Tabelle Konto mit den Attributen Nummer und Kontostand. Eine Überweisung vom Konto 42 auf das Konto 4711 besteht aus zwei SQL-Anweisungen: Erweiterte Konzepte von Datenbanksystemen 102 UPDATE Konto SET Kontostand = Kontostand - 100 WHERE Nummer = 42; UPDATE Konto SET Kontostand = Kontostand + 100 WHERE Nummer = 4711; Nehmen wir an, nach Ausführung der ersten Anweisung stürzt die Datenbank ab, dann befindet sich nur die Änderung der ersten Anweisung im Logfile, nach der Recovery ist der Kontostand des ersten Kontos gesenkt, der des zweiten Kontos aber nicht erhöht worden, es ist also Geld verschwunden ¹⁰. Der Zustand des Systems ist nach der Recovery also inkonsistent. Dies muss unbedingt vermieden werden, eine solche Folge von Anweisungen (eine Transaktion) muss entweder komplett oder überhaupt nicht ausgeführt werden. Die Lösung ist (relativ) einfach: Beginn und Ende einer Transaktion werden im Log vermerkt ¹¹: Begin of transaction (BOT) UPDATE … UPDATE … End of transaction (EOT) Der Log sähe dann so aus: Log mit Transaktionen. Transaktion 42 hat BOT und EOT im Log, ist also abgeschlossen. Transaktion 43 nicht. Bei der Recovery werden zunächst alle Log-Records eingespielt. Das System merkt sich, welche Transaktionen noch offen sind ¹² und macht diese rückgängig (ROLLBACK). Um dies leisten zu können, müssen im Log zusätzlich zu den Änderungen (After Image) auch noch die Stände vor den Änderungen (Before Image) vermerkt werden ¹³. ¹⁰Natürlich ist das Geld in der Realität nicht verschwunden, sondern die Bank hat plötzlich mehr Geld. Deshalb die gewählte Reihenfolge der Anweisungen. ¹¹Eine Transaktion wird in SQL mit dem Befehl START TRANSACTION oder BEGIN [WORK] gestartet und entweder mit COMMIT [WORK] oder mit ROLLBACK [WORK] beendet. Die Ausdrücke in den eckigen Klammern sind optional. COMMIT macht die Transaktion persistent, ROLLBACK bricht sie ab. Tritt ein Fehler auf, stürzt etwa das Programm ab, wird automatisch ein ROLLBACK ausgeführt. ¹²Dazu werden alle Transaktionen nummeriert. Solche technischen Details werden in diesem Text in der Regel weggelassen, um die Darstellung nicht unnötig kompliziert zu machen. ¹³Dies kann durch before image records im Log realisiert werden. Oracle speichert diese Informationen in Rollback Segmenten als Datenbankobjekte. Dadurch sind andere wünschenswerte Eigenschaften von Transaktionen - Isolation - leicht zu realisieren. Die Konsequenz ist allerdings, dass auch für die Rollback-Segmente After-Image-Daten im Log erzeugt werden. Das bedeutet also, dass für eine Änderung eines Blocks die Daten letztlich viermal geschrieben werden müssen: Einmal tatsächlich in die Datenbank, dann in den Log, dann in das Rollback- Segment, das auch in den Log kommt. Müssen zusätzlich noch Indices aktualisiert werden, erhöht sich die Menge an zu schreibenden Daten entsprechend. Erweiterte Konzepte von Datenbanksystemen 103 Transaktionen Datenbanksysteme wurden entwickelt, um mehreren Benutzern gleichzeitigen Zugriff auf eine gemeinsame Datenbasis zu ermöglichen. Doch wie kann dies funktionieren ? Schließlich kann nur ein Benutzer gleichzeitig einen Datensatz ändern, versuchen dies mehrere gleichzeitig, gibt es Konflikte. Eine Transaktion ist eine „Logical Unit of Work“, eine Folge von Anweisungen, die gemeinsam oder gar nicht ausgeführt werden. Was passiert, wenn mehrere Transaktionen gleichzeitg ablaufen ? Stellen wir uns folgendes Szenario vor: Vom Konto 42 (Anfangssaldo 1000) sollen 100 Euro auf das Konto 4711 (An- fangssaldo 100) überwiesen werden. Gleichzeitig wird eine Einzahlung von 50 Euro auf das Konto 42 vorgenommen. Eine UPDATE-Anweisung besteht genau genommen aus zwei Schritten, zunächst wird der aktuelle Wert gelesen, dann der neue Wert geschrieben: T1 T2 READ 42 → 1000 READ 4711 → 100 READ 42 → 100 WRITE 42 → 150 WRITE 4711 → 200 WRITE 42 → 900 Die Änderung durch T2 geht verloren, es handelt sich um ein sogenanntes „lost update“¹⁴. Diese Art von Problemen wird dadurch gelöst, dass Transaktionen voneinander isoliert werden, für jede Transaktion sieht es so aus, als ob sie die einzige wäre, die auf das System zugreift. Realisiert wird dieses Verhalten durch das Sperren von Datensätzen. Will eine Transaktion einen Datensatz modifizieren, wird dieser gesperrt. Versucht nun eine andere Transaktion auf diesen Datensatz zuzugreifen, muss sie solange warten, bis sie an der Reihe ist. Die einfachste Art von Locks wäre eine exklusive Sperre der ganzen Tabelle. Im Beispiel würde T1 beim ersten READ die Tabelle Konto für alle anderen Zugriffe sperren und erst beim COMMIT wieder freigeben. Dieses, etwa bei MySQL von der MyIsam und der Memory-Engine verwendete Verfahren ist bei kurzen Transaktionen möglicherweise akzeptabel, bei längeren Transaktionen¹⁵ wird die Möglichkeit zur parallelen Bearbeitung jedoch stark eingeschränkt. Deshalb sperrt man in der Realität auf der Ebene von Seiten oder im Bedarfsfall sogar einzelnen Datensätzen¹⁶. Um mehrere Transaktionen möglichst weitgehend parallel ablaufen zu lassen, verwendet man zwei Arten von Locks, shared Locks bei Lesezugriffen und exclusive Locks bei Schreibzugriffen. Auf einem Datensatz können gleichzeitig mehrere shared Locks sein, ein exclusive Lock wird nur gewährt, wenn noch kein anderes Lock auf dem Datensatz ist. ¹⁴Es lassen sich noch andere Szenarien konstruieren, die zu anderen Problemen führen, siehe etwa http://de.wikipedia.org/wiki/Isolation_- Datenbank ¹⁵Lang laufende Transaktionen sind allerdings insgesamt als eher problematisch anzusehen, insofern ist dieses Argument, je nach Art des Systems, vielleicht nicht ganz so stark, wie es zu sein scheint. Eine vollständige Trennung von Transaktionen über längere Zeiten bedingt einen ganz erheblichen Resourcenverbrauch, der in der Praxis meist nicht akzeptabel ist. Deshalb wird der Grad der Trennung von Transaktionen in der Praxis justiert. Lost Updates müssen allerdings in jedem Fall vermieden werden. ¹⁶Diese Eskalation von Locks wird automatisch ausgeführt, und fällt deshalb dem Anwender nicht auf. Grade dieser Teil macht aber einen Lock-Manager komplex, wenn die Performance gut sein soll. Erweiterte Konzepte von Datenbanksystemen 104 Im einfachsten Fall würde im Beispiel oben T1 beim jeweiligen Lesen auf die Datensätze mit Kontonummer 42 und 4711 ein shared Lock setzen. T2 würde dann bei Lesen ein shared Lock auf Konto 42 setzen. Der Versuch von T2, ein exclusive Lock auf 4711 zu erhalten, würde nicht gelingen, so dass T2 warten würde, bis der Datensatz wieder frei ist. T1 würde dann einen exclusive Lock auf 4711 setzen und schreiben und schließlich einen exclusive Lock auf 42 setzen wollen. Was aber nicht geht, da T2 dort bereits einen shared Lock hat. Also würde T1 warten, bis der Datensatz frei ist. Das würde allerdings nie passieren, da T2 ja auf T1 wartet. Und wenn sie nicht gestorben sind, ….. Diese unangenehme Situation nennt man Deadlock. Um dieses Problem zu lösen, gibt es mehrere Möglichkeiten. Die (auf den ersten Blick) einfachste ist das Zwei-Phasen-Sperrprotokoll. Dazu werden zunächst alle für die gesamte Transaktion benötigten Locks gesetzt. Dann werden die Aktionen (lesen und schreiben) ausgeführt, dann werden die Locks wieder freigegeben. Wird dieses Verfahren verwendet, können Transaktionen beliebig geschachtelt werden. Problematisch dabei ist allerdings, dass erstens die Locks länger gehalten werden, als unbedingt nötig. Zweitens ist unter Umständen zu Beginn der Transaktion noch gar nicht bekannt, welche Locks überhaupt benötigt werden. Dann kann dieses Verfahren nicht verwendet werden. Das Zwei-Phasen-Sperrprotokoll eignet sich deshalb nur für einen Teil der Transaktionen. Deshalb wird ein weiteres Verfahren benötigt, die Deadlock Detection: Das Datenbanksystem pflegt eine Liste der Abhängigkeiten zwischen wartenden Transaktionen. Wird ein Deadlock gefunden, wird eine der beiden abgebrochen. Die abgebrochene Transaktion muss neu gestartet werden, die bisherigen Änderungen sind verloren. Anwendungen sind so auszulegen, dass sie diesen Fall, der genau genommen kein Fehler, sondern normales Verhalten ist, berücksichtigen. Änderungen, für die noch kein Commit durchgeführt wurde, sollten für andere Transaktionen nicht sichtbar sein. Dies bedingt, dass für andere Transaktionen ein konsistenter Snapshot der Datenbank für den Zeitpunkt des Beginns der Transaktion bereitgestellt werden muss. Dies kostet erhebliche Ressourcen. Deshalb kann eingestellt werden, wie gut die Transaktionen voneinander isoliert werden sollen. SET TRANSACTION ISOLATION LEVEL REPEATABLE READ; REPEATABLE READ ist der Standardwert, dies bedeutet, dass ein SELECT in einer Transaktion immer das gleiche Ergebnis liefert. Andere Werte sind READ COMMITTED, das dazu führt, dass Änderungen anderer Transaktionen dann sichtbar werden, wenn diese ein COMMIT durchführen. READ UNCOMMITTED bedeutet, dass Änderungen anderer Transaktionen sofort sichtbar sind, hier ist naturgemäß die Performance am besten. Ist eine vollständige Isolation erforderlich, kann SERIALIZABLE verwendet werden. Hier werden Datensätze auch bei einem normalen SELECT gesperrt, so dass sie von anderen Transaktionen nicht geändert werden können. Hier laufen die Transaktionen im Wesentlichen nacheinander ab, sofern sie in irgendeiner Weise die gleichen Datensätze verwenden. Wird durch eine Transaktion ein COMMIT ausgeführt, werden die Daten auf Festplatte geschrieben, so dass die Daten auch im Fall des Absturzes des Datenbankservers erhalten bleiben. Es ist aber ausreichend, wenn die Daten im Log tatsächlich auf die Platte geschrieben werden, in Erweiterte Konzepte von Datenbanksystemen 105 der Datenbank werden die Änderungen im Buffer Cache durchgeführt. Dieses als ARIES¹⁷ bezeichnete Vorgehen ist Standard bei allen kommerziellen relationalen Datenbanken, weil es erhebliche Performance-Vorteile bietet: Würden die Änderungen in der Datenbank selbst auf Platte geschrieben, müsste im Allgemeinen an verschiedenen Stellen der Platte geschrieben werden, was einen erheblich geringeren Durchsatz ergibt, als das rein sequentielle Schreiben auf den Log. Falls dessen Performance zu schlecht ist, kann durch Striping (also das parallele Schreiben auf mehreren Festplatten, RAID Level 0) der Durchsatz praktisch beliebig erhöht werden. Bei der Entwicklung von größeren Datenbankanwendungen spielt das Transaktions-Design eine erhebliche Rolle für die Performance und den Speicherbedarf. Die Operationen müssen auf sinnvolle Art und Weise zu Transaktionen zusammengefasst werden, so dass keine Deadlocks entstehen. Dazu müssen insbesondere lang laufende Transaktionen vermieden werden. Dies kann jedoch einen erheblichen zusätzlichen Entwicklungsaufwand zur Folge haben. Außerdem müssen Hotspots erkannt und vermieden werden: Nehmen wir an, ein ERP-System soll den aktuell für das Unternehmen verfügbaren Geldbetrag in einer Tabelle speichern. Nahezu jede Transaktion müßte diesen Wert ändern, was praktisch automatisch dazu führen würde, dass alle Transaktio- nen nur nacheinander ablaufen könnten. Das System wäre praktisch nicht nutzbar. Es existieren verschiedene Ansätze im akademischen Umfeld, wie diese Art von Problemen zu lösen ist, etwa Field calls oder Escrow locks, die aber in kommerziellen Produkten praktisch nicht verfügbar sind (einzige Ausnahme: Microsoft Extensible Storage Engine, die Datenbank unter Active Directory und Exchange, unterstützt Escrow-Attribute. Hier werden bei Änderun- gen durch Transaktionen nicht absolute Werte gesetzt, sondern Operationen gespeichert, die nacheinander ausgeführt werden können, etwa WERT=WERT+1). Zusammenfassung Relationale Datenbanken erlauben durch ACID-Eigenschaften (Atomicity, Consistency, Isola- tion, Durability) den gleichzeitigen Zugriff vieler Nutzer, ohne dass dabei Inkonsistenzen in den Daten entstehen. Auch bei Problemen wie Abstürzen, Hard- oder Softwarefehlern ist in der Regel eine vollständige Wiederherstellung der Daten in den letzten konsistenten Zustand (also nach erfolgreichem Commit einer Transaktion) möglich. Der Preis dafür sind Sperren und Log-Dateien und damit eine Verschlechterung der Performance. ¹⁷siehe etwa http://en.wikipedia.org/wiki/Algorithms_for_Recovery_and_Isolation_Exploiting_Semantics Erweiterte Konzepte von Datenbanksystemen 106 Verteilte Datenbanken Befindet sich eine Datenbank nicht auf einem einzelnen Rechner, sondern sind die Daten über mehrere Rechner verteilt, spricht man von einer verteilten Datenbank. Dies ist ein theoretisch ansprechendes Konzept, können doch die Daten je nach Lokalisierung der Nutzung, Organisation von Verantwortlichkeiten, Verwendungszweck oder technologischer Anforderungen auf ver- schiedene Systeme verteilt werden. Prinzipiell stellen auch verteilte Transaktionen kein Problem dar. In den neunziger Jahren wurde auf diesem Gebiet umfangreich geforscht und entwickelt¹⁸. Eine verteilte Transaktion etwa lässt sich anschaulich mit dem Two Phase Commit (2PC) Protokoll realisieren. Das Prinzip entspricht dem Vorgehen des Pfarrers bei einer Trauung: Der Pfarrer (Transaction Coordinator) fragt zunächst alle Parteien (Datenbanken oder Resource Manager) ob sie bereit sind (Prepare to commit). Antwortet eine Partei mit „Ja“, kann die Zustimmung nicht mehr rückgängig gemacht werden. Antwortet eine Partei mit „Nein“ oder überhaupt nicht, wird die Transaktion abgebrochen. Haben alle mit „Ja“ geantwortet, erklärt der Pfarrer (Transaction Coordinator) die Transaktion für erfolgreich und teilt dies allen mit (Commit). Die Probleme zeigen sich im praktischen Betrieb. Eine Transaktion kann erst dann beendet werden, wenn entweder alle Partner geantwortet haben oder ein entsprechender Timeout entscheidet, dass einer der Partner nicht erreichbar ist. Für viele praktische Belange ist diese Latenz zu groß. Deshalb werden in der Regel asynchrone Verfahren zur Kopplung verwendet ¹⁹. Aber auch ohne die Probleme verteilter Transaktionen sind verteilte Datenbanken hinsichtlich der Performance problematisch. Die Optimierung verteilter Joins ist zwar theoretisch möglich, aber in der Praxis nur schwer durchführbar. Praktisch haben sich zwei einfache Spezialfälle verteilter Datenbanken durchgesetzt, Replikation und Sharding. Bei der Replikation werden die gleichen Daten auf mehrere Datenbanken dupliziert, beim Sharding werden die Daten anhand eines Kriteriums auf mehrere Datenbanken aufgeteilt. Master-Slave-Replikation Der einfachere Fall von Replikation ist die Master-Slave-Replikation. Hier werden Schreibzu- griffe nur auf dem Master durchgeführt, die geänderten Daten werden an die Slaves weit- ergegeben, technisch in der Regel dadurch realisiert, dass die Log-Dateien des Masters kopiert werden, die Slaves befinden sich ständig im Recovery-Modus. Das ist kein Problem, da auf den Slaves nur Lesezugriffe ausgeführt werden. ¹⁸Siehe etwa Dadam, Verteilte Datenbanken und Client/Server-Systeme, Springer, 1996. ¹⁹Siehe etwa G. Hohpe, Your Coffe Shop Doesn’t Use Two-Phase Commit, IEEE Software, Volume 22 Issue 2, March 2005 Page 64-66 Erweiterte Konzepte von Datenbanksystemen 107 Diese Art der Replikation bietet einige Vorteile: Schreibzugriffe auf den Master sind genau so schnell wie bei einer einzelnen Datenbank, Lesezugriffe können durch Zugriffe auf die Rep- likate beschleunigt werden. Dies ist insbesondere interessant, wenn nur wenige Schreibzugriffe erfolgen, die Nutzer aber weltweit verteilt sind. Bei einer einzelnen Datenbank wären hier die Latenzen zum (Lese-) Zugriff auf den zentralen Datenbankserver langsam, werden die Replikate geschickt verteilt, kann dieses Problem gelöst werden ²⁰. Problematisch ist diese Art der Replikation dann, wenn entweder viele, verteilte Schreibzugriffe stattfinden oder zwar nur wenige Schreibzugriffe erfolgen, diese aber auch möglich sein müssen, wenn die Verbindung zum zentralen Datenbankserver (kurzzeitig) ausfällt („always writeable“). Der erste Fall kann in vielen Fällen durch Sharding gelöst werden, der zweite Fall ist schwierig. Schwierig bedeutet hier, dass die ACID-Eigenschaften nicht eingehalten werden können, dies ist die Aussage des CAP-Theorems. Das CAP-Theorem besagt, dass bei verteilten Datenbanken entweder Konsistenz oder hohe Verfügbarkeit erreicht werden kann, nicht beides gleichzeitig ²¹. Theoretisch gibt es auch noch die Multi-Master-Replikation, d.h. Schreibzugriffe sind auf jeder Datenbank möglich und werden auf die anderen Knoten repliziert, dies ist aber unter Beibehaltung der ACID-Eigenschaften nur in Spezialfällen mit akzeptabler Performance und Verfügbarkeit möglich. ²⁰Ganz so unproblematisch ist dieses Verfahren aber auch nicht: Liest etwa ein Client von einem der Slaves und schreibt dann auf den Master und liest dann unmittelbar darauf folgend wieder vom Slave, erhält er unter Umständen die alten, also aus seiner Sicht falsche Werte. Um dies zu vermeiden, müsste nach einem Schreibzugriff vom Master gelesen werden, um „read your writes“-Konsistenz zu erhalten. Gängige Datenbank-Schnittstellen wie ORM-Mapping-Tools bieten diese Funktionen nicht von Haus aus. Die Implementierung von Konsistenz in verteilten Datenbanken muss in der Anwendung selber realisiert werden. Selbst dann lesen verschiedene Clients von verschiedenen Slaves unter Umständen unterschiedliche Daten. ²¹Genau genommen besagt das CAP-Theorem, dass von den drei Eigenschaften Konsistenz (Consistency), Verfügbarkeit (Availability) und Unempfindlichkeit gegenüber Unterbrechungen der Netzwerkverbindung (Partition Tolerance) nur zwei gleichzeitig realisierbar sind. Zumindest gelegentliche, kurzzeitige Unterbrechungen der Netzwerkverbindung sind aber bei verteilten Systemen unvermeidbar, deshalb die einfachere Formulierung im Text. Man darf sich das aber nicht so vorstellen, dass es hier nur zwei Möglichkeiten (hohe Verfügbarkeit/keine Konsistenz und Konsistenz/schlechte Verfügbarkeit gäbe), hier ist ein Spektrum an Möglichkeiten realisierbar. Für jede Anwendung muss deshalb definiert werden, welches Maß an Konsistenz notwendig ist und welche Verfügbarkeit dadurch erreicht werden kann, oder umgekehrt, welche Verfügbarkeit gefordert ist und welche Konsistenzprobleme deshalb gelöst werden müssen. Die zweite Form ist in der Praxis häufiger als die erste. In der Regel ist diese Entscheidung auch für verschiedene Daten innerhalb einer Anwendung unterschiedlich zu treffen: Bei amazon ist beispielsweise der Warenkorb immer schreibbar, beim Auslösen einer Bestellung wird dann allerdings auf Konsistenz geachtet. Erweiterte Konzepte von Datenbanksystemen 108 Verschiedene Arten der Verteilung Beim Sharding werden gleichartige Daten auf mehrere Datenbanken aufgeteilt. Die Idee dabei ist, dass Lese- und Schreibzugriffe gemeinsam auf zusammengehörigen Datensätzen stattfinden, sich die Zugriffe aber annähernd gleichmäßig über die Gesamtdatenmenge verteilen. Insbesondere ist dies der Fall, wenn die einzelnen Nutzer jeweils auf unterschiedliche Daten, also insbesondere auf ihre eigenen, zugreifen. Ein Beispiel hierfür ist ein Messaging-System: Werden die Nutzerdaten nach Regionen auf mehrere Datenbanken verteilt, bleiben die meisten Zugriffe lokal. So kann sowohl eine im Mittel gute Performance, als auch eine hohe Verfügbarkeit erreicht werden. Die beiden Verfahren können auch kombiniert werden, d.h. die einzelnen shards werden zur Erhöhung der Verfügbarkeit und der Lese-Performance repliziert. Gerade Web 2.0 Anwendungen haben ein dieser Architektur entsprechendes Zugriffsmuster. Problematisch hierbei sind die sogenannten Hotspots, also einzelne Datensätze, die von sehr vielen Nutzern gelesen werden. Hier müssen geeignete, anwendungsspezifische Lösungen gefunden werden, eine allgemeine Lösung existiert nicht ²². Zur Verbesserung der Performance wird häufig zusätzlich zu Replikation und Sharding partition- iert, es wird eine Aufteilung nach der Art der Daten vorgenommen. Bei einem Messaging-System werden etwa in der einen Datenbank die Nutzerprofile, in einer anderen die Accounts, wieder in einer anderen die Nachrichten usw. gespeichert. Wird diese Aufteilung geschickt gewählt, kann in vielen Fällen erreicht werden, dass sich Zugriffe nicht über mehrere Datenbanken erstrecken, ²²So werden bei Twitter etwa Tweets von Nutzern nicht (nur) bei den Nutzern selber gespeichert, sondern auch bei den Followern. Ansonsten würde bei Nutzern mit sehr vielen Followern ein Performance-Problem entstehen, bei fast jedem Zugriff auf eine Timeline müssten diese durchsucht werden. Erweiterte Konzepte von Datenbanksystemen 109 sondern lokal in einer Datenbank realisiert werden können ²³. Auch hier ist eine Kombination mit den anderen Verfahren möglich: Zunächst werden die Daten nach Aggregates aufgeteilt, diese dann, etwa regional, durch Sharding verteilt und die einzelnen shards schließlich repliziert. Diese Architektur ist bei Web (2.0) Anwendungen üblich. Problematisch ist hierbei, dass bei dieser Architektur die meisten Vorteile relationaler Daten- banken nicht genutzt werden können: Keine Transaktionen mehr, keine Gewährleistung von semantischer oder referentieller Integrität, keine flexiblen Abfragen durch Joins (effizient nur noch in den Aggregates) und so weiter. Da die Daten grade zur Verbesserung der Verfügbarkeit und Performance verteilt werden, würde die Verwendung verteilter Zugriffe und Transaktionen diese Architektur konterkarieren. Letztlich wird in jeder Datenbank nur noch eine einzige Tabelle (ggf. mit einer komplexen, eher objektrelationalen Struktur, möglicherweise aufgeteilt auf einige wenige Tabellen) gespeichert. Da stellt sich schon die Frage, warum man dann noch eine relationale Datenbank braucht. Zusammenfassung Daten können über mehrere Rechner verteilt werden. Dies wird meistens zur Erhöhung der Verfügbarkeit (mehrere Kopien der Daten auf mehreren Servern), der Performance (kleinere Latenz beim Zugriff auf näher gelegene Server) oder beidem durchgeführt. Problematisch sind verteilt realisierte Schreibzugriffe, hier können leicht Inkonsistenzen entstehen, wenn die einzelnen Datenbanken nicht mittels Transaktionen synchron gehalten werden, was aber wiederum der Performance und Verfügbarkeit sehr abträglich ist. ²³Dies ist genau dann der Fall, wenn nicht nach einzelnen Tabellen, sondern nach Aggregates partitioniert wird. Diese Beobachtung stammt von Eric Evans, Domain Driven Design: In der Anwendung wird meistens nicht auf einzelne Datensätze, sondern etwa bei Bestellungen fast immer gemeinsam auf den Bestellkopf, die Bestellpositionen und die Artikeldetails zugegriffen. Diese sollten also auch zusammen verwaltet und gespeichert werden. Ausblick In den letzten Jahren sind jenseits der relationalen Datenbankmanagementsysteme andere, nichtrelationale Systeme immer stärker in den Vordergrund geraten, die sogenannten NoSQL- Datenbanken²⁴. Dies hat im wesentlichen zwei Gründe. Im letzten Kapitel wurde gezeigt, dass die Vorteile relationaler Datenbanken vor allem dann genutzt werden können, wenn sich die gesamte Datenbank auf einem Rechner befindet. Zweitens ist die Verbindung zwischen den in Relationen (Tabellen) dargestellten Daten in der Datenbank und der Darstellung in der Applikation (etwa in Form von Objekten) unterschiedlich: In der Anwendung handelt es sich eher um eine Hierarchie von Objekten, die von einer Wurzel ausgehend zahlreiche Details enthalten. Eine Bestellung stellt ein Aggregat dar, das in der Anwendung als Einheit behandelt, aber in einer relationalen Datenbank auf mehrere Tabellen verteilt wird. Abbildung aus Fowler, NoSQL distilled Beim Lesen und beim Schreiben müssen diese Darstellungen in die jeweils andere über- führt werden. Der mit diesem, oft als impedance mismatch bezeichneten Problem verbundene Aufwand lässt sich zwar teilweise mit ORM-Layern (Object Relational Mapping) wie iBATIS oder Hibernate reduzieren, aber letztlich nicht lösen²⁵. ²⁴Es gab auch schon früher nicht-relationale Ansätze, etwa die Objektdatenbanken, die sich jedoch nicht durchsetzen konnten. Der Markt in den großen Anwendungsbereichen (OLTP und Data Warehouse-Systeme) wird von den konventionellen RDBMS-Anbietern beherrscht. Diese bieten zwar mehr oder weniger stark objektorientierte Erweiterungen, die im praktischen Einsatz jedoch kaum verwendet werden. Diese Objektrelationalen Systeme sind eher dem Marketing (auf Seiten der Anbieter) und dem Wunschdenken (auf Seiten der Anwender) zuzuordnen. Ein wesentlicher Grund, warum diese Techniken nicht in der Praxis angewendet werden, ist, dass seit gut zehn Jahren der Trend stark dahin geht, Abstraktionslayer zum Zugriff auf die Datenbank zu verwenden. Dies hat verschiedene Gründe, einer – und in der Praxis einer der wichtigsten – ist, dass die Anwendung (möglichst) unabhängig vom verwendeten Datenbanksystem sein soll. Mit einem starken Vendor Lock-In haben viele Unternehmen schlechte, das heißt kostspielige, Erfahrungen gemacht. ²⁵Von Ted Neward wurde deshalb das Object Relational Mapping plakativ als „Vietnam of Computer Science“ bezeichnet, siehe http://blogs.tedneward.com/2006/06/26/The+Vietnam+Of+Computer+Science.aspx Ausblick 111 Datenbanken, die diese Strukturen direkt abbilden können, erleichtern die Entwicklung von Anwendungen unter Umständen erheblich, hier können die (Objekt-) Strukturen²⁶ ohne explizite Konvertierungen direkt serialisiert werden. Die Daten eines Aggregats (etwa einer Bestellung mit Bestellpositionen, Artikeldaten usw.) werden hierzu in geeigneter Form, heute meistens JSON, dargestellt. Die Wurzel dieser Datenstruktur wird in der Regel durch einen Schlüssel, hier etwa die Bestellnummer, dargestellt. Der Zugriff (also lesen oder schreiben) erfolgt über diesen Schlüssel. Was in der Applikation also hauptsächlich verwendet wird, sind die folgenden Funktionen: Get(key) --> JSON-Darstellung der Objekte Put(key, JSON) Natürlich lässt sich diese Art des Zugriffs auch mit relationalen Datenbanken realisieren, aber warum eigentlich? Spezielle Key-Value Stores²⁷ wie redis oder riak können dies erheblich effizienter und einfacher, vor allem aber sind diese Systeme in der Regel darauf ausgelegt, in Clustern verwendet zu werden. Replikation und Sharding sind hier Bestandteil des Konzepts und sehr einfach realisierbar. Der alleinige Zugriff über den Key reicht allerdings nicht aus, es werden zusätzliche Suchfunkto- nen, im Beispiel etwa nach Kundennummer benötigt. Deshalb bieten solche Systeme in der Regel Möglichkeiten, zusätzliche Suchpfade zu realisieren. Hier gibt es einen weiten Bereich zwischen den verschiedenen Systemen, von fast reinen Key-Value-Stores wie redis oder riak auf der einen Seite, die die eigentlichen Daten als Black Box speichern und zur Realisierung anderer Suchpfade redundante Speicherstrukturen erfordern, über dokumentzentrierte Datenbanken wie mongoDB, die komplexere Abfragen über die interne Struktur der Daten ermöglichen, bis hin zu Column Family Datenbanken wie Cassandra, die die Daten in Tabellen speichern, deren Attribute dann aber komplexe Strukturen wie Listen usw. beinhalten dürfen und Abfragen mit i.w. Standard- SQL zulassen. Je nach Anwendung wird das Leben der Applikationsentwickler durch die Wahl einer geeigneten Datenbank unter Umständen erheblich leichter. NoSQL-Datenbanken haben in der Regel kein fixes Datenmodell, sondern sind schemafrei („schemaless“). Die Beziehung zu Programmier- sprachen ist offensichtlich: Auf der einen Seite gibt es Programmiersprachen, bei denen die Attribute und Methoden von Objekten durch eine Klasse fest definiert sind, alle diese Objekte also genau die gleichen Attribute und Methoden haben. Auf der anderen Seite gibt es Sprachen, bei denen Objekte zur Laufzeit um Attribute und Methoden erweitert werden können, Objekte können sogar ganz ohne Klassen existieren. Diese Flexibilität lässt Scriptsprachen wie ruby immer populärer werden. Bei Datenbanken ist es ähnlich: Relationale Datenbanken geben ein rigoroses Schema vor, an das sich alle Datensätze halten müssen. NoSQL-Datenbanken lassen in der Regel, wenn sie denn ²⁶Meistens handelt es sich um Objektstrukturen, da objektorientierte Programmiersprachen zur Zeit am weitesten verbreitet sind. Letztlich ergibt sich dieses Problem aber bei allen Sprachen: Bei strukturierten prozeduralen Sprachen handelt es sich um Beziehungen zwischen Strukturen, bei funktionalen Sprachen um Maps und Listen. ²⁷Key Value Stores sind nichts neues, dbm etwa gibt es seit mehr als 30 Jahren. Das besondere an den NoSQL Datenbanken ist die starke Ausrichtung auf verteilte Datenspeicherung. Ausblick 112 überhaupt eine explizite innere Struktur der Datensätze zulassen und diese nicht gleich als black box behandeln, Erweiterungen und Änderungen des Schemas zu²⁸. Wenn diese NoSQL-Datenbanken²⁹ so viele Vorteile haben, stellt sich die Frage, warum sie sich nicht schon längst durchgesetzt haben, die dahinter stehenden Ideen gibt es zum Teil bereits seit langer Zeit. Ein nicht unwichtiges Argument ist, dass relationale Datenbanken keine Aggregation bevorzu- gen, das heißt, keine „Richtung“ vorgeben, entlang derer man auf die Daten zugreifen kann. In unserem Beispiel der Bestellungen heißt das, dass es bei relationalen Datenbanken kein Problem ist, eine Abfrage aus der Richtung der Artikel zu machen, also etwa eine Liste mit den Umsätzen je Artikel. Mit aggregatorientierten Datenbanken ist dies nur mit erheblichem Aufwand möglich: Die Daten (also alle Daten!) müssen zunächst geeignet umsortiert werden. Dies ist einer der Gründe, warum relationale Datenbanken die hierarchischen Datenbanken abgelöst haben, diese hatten das gleiche Problem. Ein weiteres Argument, warum grade derzeit das Interesse an diesen nicht-relationalen Systemen so groß ist, stammt von Martin Fowler³⁰: Integration auf der Datenebene In der Vergangenheit wurden relationale Datenbanken oft als Mittel zur Integration auf der Datenebene verwendet. Das bedeutet, dass verschiedene Anwendungen auf die gleichen Daten zugreifen und so ein einziger konsistenter Zustand existiert. Diese Art der Integration ist mit relationalen Datenbanken sehr leicht möglich, sie bieten, je nach Anwendung, verschiedene Sichten auf die Daten, es können entsprechende Zugriffsrechte für verschiedene Anwendungen definiert werden, die Datenbank kann unabhängig von einzelnen Anwendungen die Konsistenz sichern und so weiter und so weiter. All dies können NoSQL-Datenbanken nicht leisten. Aber diese Art der Integration ist heute immer weniger wichtig: Die Integration auf der Datenebene funktioniert insbesondere dann gut, wenn alle Anwendungen selber entwickelt werden. Dieses, zu Großrechner-Zeiten populäre Vorgehen ist heute sehr viel weniger wichtig: Durch die Verwendung von Standardsoftware und Best of Breed³¹ Ansätzen ist das Datenmodell eines großen Teils der Anwendungen außerhalb der Kontrolle des Unternehmens. Deshalb wird ²⁸Allerdings bedeutet dies, dass die Anwendung überprüfen muss, ob die Attribute, die verarbeitet werden sollen, vorhanden sind und was passieren soll, wenn nicht. Diese Flexibilität hat also ihren Preis, wie im übrigen bei den Programmiersprachen auch. ²⁹Es gibt noch weitaus mehr Möglichkeiten, Daten zu speichern, als hier besprochen wurde. Zu den NoSQL-Datenbanken zählen beispielsweise auch die Graph-Datenbanken wie etwa Neo4j, die die Modellierung und (im Gegensatz zu relationalen Datenbanken auch effiziente) Abfrage entlang komplexer Beziehungen erlauben. Andere Technologien, die zwar in der Regeln nicht zu den NoSQL-Datenbanken gezählt werden, wie etwa Objektdatenbanken, Multimedia-Datenbanksysteme oder die Speicherung von Dateien im Filesystem haben auch interessante Eigenschaften, obwohl sie derzeit nicht so populär sind wie die NoSQL-Datenbanken. Aber uns interessiert hier das Prinzip, nicht so sehr die genaue technische Realisierung. ³⁰Fowler, Sadalage, NoSQL distilled. Ein sehr empfehlenswertes Buch für diejenigen, die an einer knappen Darstellung der NoSQL-Bewegung aus der Perspektive der Software-Architektur interessiert sind. Dieses Kapitel folgt in Teilen der Argumentation von Fowler. ³¹Nicht alles von einem Anbieter kaufen, sondern für jeden Bereich den besten Anbieter wählen. Ausblick 113 heute auf anderen Wegen integriert, etwa über nachrichtenorientierte Middleware und EAI- Infrastrukturen oder, moderner, auf der Ebene von Services. Basiert die Architektur auf gekapselten Services, spielt es plötzlich eine sehr viel geringere Rolle, welche Technologie für die Datenhaltung verwendet wird³². Dies wird allerdings kaum dazu führen, dass relationale Datenbanken komplett durch andere Systeme ersetzt werden. Die Zukunft wird wohl eher so aussehen, dass zusätzlich zu den relationalen Datenbanken, je nach Anwendung, auch andere Systeme verwendet werden. Fowler bezeichnet diese Art der Architektur als „polyglot persistence“. Zusammenfassung Relationale Datenbank Management Systeme zeichnen sich dadurch aus, dass sie: • Daten in Tabellen anordnen, die zeilenweise in Seiten fester Größe auf Festplatten gespeichert werden • Zugriffe auf die Festplatte cachen • die Suche nach Datensätzen durch B-Bäume beschleunigen • Änderungen in einem Logfile protokollieren, bevor die Daten in die eigentliche Datenbank geschrieben werden • ACID-Transaktionen durch locks synchronisieren • Master-Slave-Replikation unterstützen Diese Systeme funktionieren bei OLTP-Anwendungen sehr gut, sofern die Systeme auf einem einzelnen Server betrieben werden können. Zur Erhöhung der Verfügbarkeit bzw. zur Verbesserung der Performance bei Lesezugriffen unter gewissen Umständen können Datenbanken repliziert werden. OLTP-Datenbanken sind allerdings in der Regel nicht so groß, als dass es heute noch zwingend nötig wäre, die Daten auf Massenspeichern zu halten, heutige Systeme könnten die Daten auch im Hauptspeicher verwalten. Damit wären zahlreiche Einschränkungen hinfällig. Der Trend zu In Memory Datenbanken wird deshalb im OLTP-Bereich immer stärker werden. Weltweit verteilte Datenbanken erfordern eine andere Betrachtung von Konsistenz als die klassischen ACID-Eigenschaften, hier gibt es aber leider keine allgemein gültigen Modelle, die in einer verteilten Umgebung mit allen Anwendungen funktionieren. Die Logik zur Auflösung von unvermeidbaren Inkonsistenzen ist anwendungsabhängig, die Technologie zur Umsetzung solcher Systeme findet sich bei den NoSQL-Datenbanken. Data Warehouses erreichen durch die immer stärkere Fokussierung auf sehr große Datenmengen („Big Data“) eine Größe, die die Bearbeitung auf einem einzelnen Rechner immer schwieriger macht. Hier sind zwei Trends auszumachen: Column Data Stores, also relationale Datenbanksys- teme, die Daten spaltenweise speichern und so eine erhebliche Verbesserung der Performance bei Lesezugriffen bieten und stark verteilte NoSQL-Systeme, die Anfragen als map-reduce-jobs über eine große Anzahl von Rechnern verteilen. ³²Es ist allerdings nicht so, dass es keine Rolle mehr spielt. Von vielen Autoren aus dem Bereich der Software-Architektur werden Anforderungen des Betriebs wie etwa Datensicherung und zum verlässlichen, effizienten Betrieb nötiges know-how gerne vernachlässigt. Ich möchte natürlich nicht unterstellen, dass dies daher kommt, dass viele dieser Autoren ihr Geld mit Beratung verdienen und nach dem Ende der Entwicklung nicht arbeitslos sein wollen ;-)","libVersion":"0.3.2","langs":""}