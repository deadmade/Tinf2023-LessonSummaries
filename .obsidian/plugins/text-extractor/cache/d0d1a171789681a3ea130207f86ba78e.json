{"path":"DHBW Heidenheim/2025 WiSe/Datenbanken/Skript/Patterns in Data Management.pdf","text":"Patterns in Data Management — A Flipped Textbook Jens Dittrich October 19, 2017 Also check out the ebook version of this print book. As of March 2016 it is available for about 10$ at http://amzn.to/1Ts3rwx. Imprint Copyright 2016 by Jens Dittrich. No part of this book or its related materials may be reproduced in any form without the written consent of the copyright holder. Notice that most of the images used in ﬁgures in this book are copyrighted material and you must not use this material without getting a license from the copyright holder, see the Credits section at the end of this book. Notice that the youtube videos produced by Jens Dittrich and pointed to in this book are freely available on http://youtube.com/jensdit and may be used for classrooms at no additional costs. The same holds for the slides that were used to produce these videos in the ﬁrst place. All slides are freely available under a CC-BY-NC license. Checkout my site http://datenbankenlernen.de and my youtube channel http://youtube.com/jensdit for a full list of videos. Or simply follow the pointers in this book. Those sites also contain about 80 additional introductory videos on database concepts in German. Disclaimer. All material contained in this book has been researched with great care and the author has used his best eﬀorts in preparing this book. Yet, the author and publisher make no warranty of any kind with regard to the concepts, techniques, methods, programs, and software described in this book. In particular, there is no guarantee with respect to correctness or ﬁtness for a particular purpose. The author and publisher shall not be liable in any event for incidental or consequential damages in connection with, or arising out of, the furnishing, performance, or use of the concepts, techniques, methods, programs, and software contained in this book. So, again, this is a textbook using abstractions (=we leave away details here and there) and generalizations (=we group diﬀerent techniques under one umbrella by focussing on commonalities and we may leave away details in the descriptions) to explain general concepts. Often details are crucial in making a particular concept work in a speciﬁc scenario and setting. Hence, as in all science and engineering, applying technical concepts blindly to a particular scenario and/or software without considering the particular circumstances is a bad idea. So, you are still reading this boring disclaimer? I am wondering: shouldn’t you rather be studying the technical content of this book? book version: October 19, 2017 The author can be reached at: Prof. Dr. Jens Dittrich Campus E1 1 66123 Saarbrücken Germany jens.dittrich@cs.uni-saarland.de http://www.infosys.uni-saarland.de http://datenbankenlernen.de http://youtube.com/jensdit ISBN-13: 978-1523853960 (CreateSpace-Assigned) ISBN-10: 1523853964 BISAC: Computers / Database Management / General This print book and ebook were prepared with LATEX, tex4ht, and Ruby scripts employing Nokogiri, written by Armando Fox (and heavily tweaked by Jens Dittrich); cover design by Christine Tophoven (http://www.tophoven-design.de). Contents Preface . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 Acknowledgments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 How to use this Book . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 0 Introduction 15 0.1 Course Overview and Motivation . . . . . . . . . . . . . . . . . . . . . . . 15 0.1.1 The Truth about Databases . . . . . . . . . . . . . . . . . . . . . . 15 0.1.2 Architecture of a DBMS . . . . . . . . . . . . . . . . . . . . . . . . 17 0.2 History of Relational Databases . . . . . . . . . . . . . . . . . . . . . . . . 20 0.2.1 A Footnote about the Young History of Database Systems . . . . . 20 0.2.2 Relational Database — A Practical Foundation of Productivity . . 23 1Hardware and Storage 25 1.1 Storage Hierarchies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 1.1.1 The All Levels are Equal Pattern . . . . . . . . . . . . . . . . . . . 30 1.1.2 Multicore Storage Hierarchies, NUMA . . . . . . . . . . . . . . . . 30 1.2 Storage Media . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34 1.2.1 Tape . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34 1.2.2 Hard Disks: Sectors, Zone Bit Recording, Sectors vs Blocks, CHS, LBA, Sparing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35 1.2.3 Hard Disks: Sequential Versus Random Access . . . . . . . . . . . 39 1.2.4 Hard Disk Controller Caching . . . . . . . . . . . . . . . . . . . . . 42 1.2.5 The Batch Pattern . . . . . . . . . . . . . . . . . . . . . . . . . . . 45 1.2.6 Hard Disk Failures and RAID 0; 1; 4; 5; 6 . . . . . . . . . . . . . . 47 1.2.7 Nested RAID Levels 1+0; 10; 0+1; 01 . . . . . . . . . . . . . . . . 51 1.2.8 The Data Redundancy Pattern . . . . . . . . . . . . . . . . . . . . 53 1.2.9 Flash Memory and Solid State Drives (SSDs) . . . . . . . . . . . . 55 4 CONTENTS 1.2.10 Example Hard Disks, SSDs and PCI-connected Flash Memory . . 58 1.3 Fundamentals of Reading and Writing in a Storage Hierarchy . . . . . . . 60 1.3.1 Pulling Up and Pushing Down Data, Database Buﬀer, Blocks, Spa- tial vs Temporal Locality . . . . . . . . . . . . . . . . . . . . . . . 60 1.3.2 Methods of the Database Buﬀer, Costs, Implementation of GET . 63 1.3.3 Pushing Down Data in the Storage Hierarchy (aka Writing), update in-place, deferred update . . . . . . . . . . . . . . . . . . . . . . . 64 1.3.4 Twin Block, Fragmentation . . . . . . . . . . . . . . . . . . . . . . 67 1.3.5 Shadow Storage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69 1.3.6 The Copy On Write Pattern (COW) . . . . . . . . . . . . . . . . . 71 1.3.7 The Merge on Write Pattern (MOW) . . . . . . . . . . . . . . . . 73 1.3.8 Diﬀerential Files, Merging Diﬀerential Files . . . . . . . . . . . . . 76 1.3.9 Logged Writes, Diﬀerential Files vs Logging . . . . . . . . . . . . . 80 1.3.10 The No Bits Left Behind Pattern . . . . . . . . . . . . . . . . . . . 83 1.4 Virtual Memory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 86 1.4.1 Virtual Memory Management, Page Table, Preﬁx Addressing . . . 86 1.4.2 Retrieving Memory Addresses, TLB . . . . . . . . . . . . . . . . . 89 2Data Layouts 93 2.1 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93 2.2 Page Organizations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95 2.2.1 Slotted Pages: Basics . . . . . . . . . . . . . . . . . . . . . . . . . 95 2.2.2 Slotted Pages: Fixed-size versus Variable-size Components . . . . . 99 2.2.3 Finding Free Space . . . . . . . . . . . . . . . . . . . . . . . . . . . 102 2.3 Table Layouts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104 2.3.1 Data Layouts: Row Layout vs Column Layout . . . . . . . . . . . 104 2.3.2 Options for Column Layouts, Explicit vs Implicit key, Tuple Re- construction Joins . . . . . . . . . . . . . . . . . . . . . . . . . . . 106 2.3.3 Fractured Mirrors, (Redundant) Column Grouping, Vertical Parti- tioning, Bell Numbers . . . . . . . . . . . . . . . . . . . . . . . . . 109 2.3.4 PAX, How to choose the optimal layout? . . . . . . . . . . . . . . 115 2.3.5 The Fractal Design Pattern . . . . . . . . . . . . . . . . . . . . . . 119 2.4 Compression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122 2.4.1 Beneﬁts of Compression in a Database, Lightweight Compression, Compression Granularities . . . . . . . . . . . . . . . . . . . . . . . 122 CONTENTS 5 2.4.2 Dictionary Compression, Domain Encoding . . . . . . . . . . . . . 124 2.4.3 Run Length Encoding (RLE) . . . . . . . . . . . . . . . . . . . . . 128 2.4.4 7Bit Encoding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 130 3 Indexes 133 3.1 Motivation for Index Structures, Selectivities, Scan vs. Index Access . . . 133 3.2 B-trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137 3.2.1 Three Reasons for Using B-tree Indexes, Intuition, Properties, ﬁnd(), ISAM, ﬁnd_range() . . . . . . . . . . . . . . . . . . . . . . 137 3.2.2 B-tree insert, split, delete, merge . . . . . . . . . . . . . . . . . . . 142 3.2.3 Bulk-loading B-trees or other Tree-structured Indexes . . . . . . . 146 3.2.4 Clustered, Unclustered, Dense, Sparse, Coarse-Granular Index . . 148 3.2.5 Covering and Composite Index, Duplicates, Overﬂow Pages, Com- posite Keys . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 153 3.3 Performance Measurements in Computer Science . . . . . . . . . . . . . . 156 3.4 Static Hashing, Array vs Hash, Collisions, Overﬂow Chains, Rehash . . . 163 3.5 Bitmaps . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167 3.5.1 Value Bitmaps . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167 3.5.2 Decomposed Bitmaps . . . . . . . . . . . . . . . . . . . . . . . . . 169 3.5.3 Word-Aligned Hybrid Bitmaps (WAH) . . . . . . . . . . . . . . . . 171 3.5.4 Range-Encoded Bitmaps . . . . . . . . . . . . . . . . . . . . . . . . 173 3.5.5 Approximate Bitmaps, Bloom Filters . . . . . . . . . . . . . . . . . 175 4 Query Processing Algorithms 181 4.1 Join Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 181 4.1.1 Applications of Join Algorithms, Nested-Loop Join, Index Nested- Loop Join . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 181 4.1.2 Simple Hash Join . . . . . . . . . . . . . . . . . . . . . . . . . . . . 184 4.1.3 Sort-Merge Join, CoGrouping . . . . . . . . . . . . . . . . . . . . . 186 4.1.4 Generalized CoGrouped Join (on Disk, NUMA, and Distributed Systems) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 189 4.1.5 Double-Pipelined Hash Join, Relationship to Index Nested-Loop Join194 4.2 Implementing Grouping and Aggregation . . . . . . . . . . . . . . . . . . 198 4.3 External Sorting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 201 4.3.1 External Merge Sort . . . . . . . . . . . . . . . . . . . . . . . . . . 201 4.3.2 Replacement Selection . . . . . . . . . . . . . . . . . . . . . . . . . 207 6 CONTENTS 4.3.3 Late, Online, and Early Grouping with Aggregation . . . . . . . . 213 5 Query Planning and Optimization 217 5.1 Overview and Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . 217 5.1.1 Query Optimizer Overview . . . . . . . . . . . . . . . . . . . . . . 217 5.1.2 Challenges in Query Optimization: Rule-Based Optimization . . . 220 5.1.3 Challenges in Query Optimization: Join Order, Costs, and Index Access . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 224 5.1.4 An Overview of Query Optimization in Relational Systems . . . . 228 5.2 Cost-based Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . 231 5.2.1 Cost-Based Optimization, Plan Enumeration, Search Space, Cata- lan Numbers, Identical Plans . . . . . . . . . . . . . . . . . . . . . 231 5.2.2 Dynamic Programming: Core Idea, Requirements, Join Graph . . 234 5.2.3 Dynamic Programming Example without Interesting Orders, Pseudo-Code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 237 5.2.4 Dynamic Programming Optimizations: Interesting Orders, Graph Structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 240 5.3 Query Execution Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . 243 5.3.1 Query Execution Models, Function Calls vs Pipelining, Pipeline Breakers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 243 5.3.2 Implementing Pipelines, Operators, Iterators, ResultSet-style Iter- ation, Iteration Granularities . . . . . . . . . . . . . . . . . . . . . 248 5.3.3 Operator Example Implementations . . . . . . . . . . . . . . . . . 251 5.3.4 Query Compilation . . . . . . . . . . . . . . . . . . . . . . . . . . . 253 5.3.5 Anti-Projection, Tuple Reconstruction, Early and Late Material- ization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 255 6 Recovery 263 6.1 Core Concepts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 263 6.1.1 Crash Recovery, Error Scenarios, Recovery in any Software, Impact on ACID . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 263 6.1.2 Log-Based Recovery, Stable Storage, Write-Ahead Logging (WAL) 266 6.1.3 What to log, Physical, Logical, and Physiological Logging, Trade- Oﬀs, Main Memory versus Disk-based Systems . . . . . . . . . . . 269 6.2 ARIES . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 274 Bibliography 285 CONTENTS 7 Credits 289 Index 293 CV Jens Dittrich 299 8 CONTENTS Preface When I was an undergraduate student I attended the course Introduction to Databases. Iquicklyconcludedthat databasesare probablythesingle mostboringand dullesttopic in the world. Consequently, after a couple of weeks I dropped the course. And I was wondering: is that what computer science is about? Managing data rows and their relationship across tables? Reasoning about the “normal form” of a table? Writing a database “trigger”? Could there be anything less exciting in the universe? Maybe in some parallel universe? Why would such universe exist then? Shouldn’t I better switch ﬁelds? And even the term “Database”: it reminded me of dusty ﬁle cabinets crammed with worn-out cardboards. All of that hidden in dark aisles, somewhere downstairs in a for- gotten ﬂoor even way below to what people call “the basement”, probably observed by some weird librarian who had never seen the light of the day. Nevertheless, I continued studying computer science, but I focussed on other topics like software engineering. Eventually, I had to write a Diploma Thesis (a predecessor to what is now called an M.Sc. Thesis). And I had to make a decision: which of those two software engineering topics that a software engineering professor suggested to me would I want to work on for the following nine months or so? Well, great question! It seemed, Ihad ﬁnallyfoundsomethingevenmoreboringthan databases1. Out of a mere gut feeling I headed straight for the database professor’s oﬃce to ask him about possible Diploma Thesis topics. He let me in and explained two topics to me on the white board. And, surprisingly, they did not sound at all like “managing rows”, “determining normal forms of tables” or “writing triggers”. Quite in contrast, a whole new world of exciting algorithmic problems opened up. The Diploma topic he suggested would be about designing new algorithms, and comparing them with existing algorithms published in related work at top international conferences. In the months that followed I learned a lot, and I understood how exciting computer science, and in particular databases, can be. 1This was not so much the fault of software engineering as a ﬁeld, but of the speciﬁc subtopic that software engineering professor suggested. Software engineering, to me, is actually another extremely exciting and useful area in computer science. 10 CONTENTS With this book I am trying to share my excitement for the ﬁeld of data management. Saarbrücken, February 2016 Prof. Dr. Jens Dittrich Acknowledgments This book wouldn’t have been possible without the support of a great team of students including my Ph.D. students: Stefan Schuh, Endre Palatinus, Stefan Richter, and Felix Martin Schuhknecht. They delivered ideas for exercises and helped on the quizzes. They also peer-reviewed some of the material and discussed plans for videos with me. Marcel Maltry proofread the book. I would like to thank Christine Tophoven for designing the cover of this book. My secretary Angelika Scholl-Danopoulos helped typesetting this book. I am also grateful to the tutors and students of my database systems classes of summer 2014, winter 2014/15, and winter 2015/16 who were the ﬁrst to play with the material presented in this book. I would also like to thank my subscribers on youtube for the encouraging positive comments. 12 CONTENTS How to use this Book This book is not a standard textbook. This book was written extending and comple- menting preexisting educational videos I designed and recorded in winter 2013/14. The main goal of these videos was to use them in my ﬂipped classroom “Database Systems” which is an intermediate-level university course designed for B.Sc. students in their third year or M.Sc. students of computer science and related disciplines. Though in general my students liked both the ﬂipped classroom model and (most of) the videos, several students asked for an additional written script that would allow them to quickly lookup explanations for material in text that would otherwise be hard to re-ﬁnd in the videos. Therefore, in spring 2015, I started working on such a course script which more and more evolved into something that I feel comfortable calling it a book. One central question I had to confront was: would I repeat all material from the videos in the textbook? In other words, would the book be designed to work without the videos? I quickly realized that writing such an old-fashioned text-oriented book, a “textbook”, wouldn’t be the appro- priate thing to do anymore in 2015. My videos as well as the accompanying material are freely available to everyone anyways. And unless you are sitting on the local train from Saarbrücken to Neustadt, you will almost always have Internet access to watch them. In fact, downloading the videos in advance isn’t terribly hard anyway. This observation changed the original purpose of what this book would be good for: not so much the primary source of the course’s content, but a diﬀerent view on that content, explaining that content where possible in other words. In addition, one goal was to be concise in the textual explanations allowing you to quickly re-ﬁnd and remember things you learned from the videos without going through a large body of text. Therefore I came up with a structure for this book where each section, or learning unit if you wish, is structured into: 1. Material. This is the primary content I recommend you to study to reach the learning goals of this unit. Typically this material is one (rarely more) short videos. In addition, we provide links to slides as QR-codes. Like this you may easily point your device to this text to open and study the material. 2. Additional Material. This is secondary material which you do not necessarily have to study. Yet it might help you in getting a diﬀerent explanation in other words or an extended explanation of (more or less) the same content. The additional material is split into two parts again. In “Literature” we list material focussing on 14 CONTENTS the actual content of this learning unit whereas. In contrast, in “Further Reading” we list material that is related to this learning unit but goes way beyond the learning goals. All materials are provided either as bibliographic references or as QR-codes. 3. Learning Goals and Content Summary. This is a textual summary of the content of this learning unit. I decided to not write it as one ﬂow text, but rather phrase all content as Q&As — we all know people’s short attention span these days: what did I write at the beginning of this sentence? This Q&A-style serves multiple purposes: (a) Precise Learning Goals. Each question phrases one learning goal of this learning unit. By just reading the questions, you know what you should have learned. After having studied the material, you should be able to answer each question yourself. So after having worked with the material, e.g. a video, you may test your knowledge by hiding the answer and comparing your answer with the one written down. (b) Many Entry Points. While writing this text my goal was to make each an- swer concise and as much independently understandable as possible (wherever that was possible, this wasn’t always possible as a lot of material builds upon each other). Like that it becomes easier for you to enter the text somewhere in the middle. (c) Diﬀerent Views. In the videos I try to connect the dots wherever possible by motivating why a particular technique is important, to which other techniques it is related, and where it may be applied. While writing this text, I felt that here and there I wanted to extend my explanations from the videos a little bit or wanted to provide yet another view on the material in the videos. So in some answers you will see that I grabbed the opportunity to clarify explanations or come up with yet another view on the same content. For instance, in Section 2.3.4, I added a small formalism to deﬁne linearization which I believe helps a lot here to understand data layouts. Again, you may perceive this on ﬁrst sight as adding even more material. However, actually these alternative explanations make your life easier. They are all designed to help you understand the material better and grasp the material faster. 4. Quizzes. This is an excerpt of the electronic quizzes we use for my inverted class- room “Database Systems”. In that class, every week, I ask my students to study some material. Then they have to answer these quizzes in an electronic lecture tool —we use Moodle forthat. Onlyafter that, we meet in class tostart workingon the weekly exercises. 5. Exercises. This is a collection of exercises we use for my inverted classroom “Database Systems”. These exercises are taken from the weekly assignments. We change them a bit every year. The students start working on these exercises in the “class”, I call it “the LAB”. During that time my tutors and me walk through the aisles to consult the students. Chapter 0 Introduction 0.1 Course Overview and Motivation 0.1.1 The Truth about Databases Material Video: Original Slides: Inverted Slides: Additional Material Literature: [LÖ09], Database Management System Learning Goals and Content Summary Why are database systems a vertical topic? vertical topic Database systems covers topics from multiple ﬁelds. These ﬁelds (or topics) are typically treated in separate textbooks and often investigated separately. However, in order to really understand and design a complex system (like a database system), we have to understand all relevant ﬁelds (and what is important in those ﬁelds for data management) and in addition understand their interactions. Which are those topics? All systems layers (aka levels): hardware, operating system, software, and even how users interact with the system. In the context of this book we are particularly interested in hardware, data layouts, algorithms, data structures (referred to as indexes in this book), and how the diﬀerent components interact in a complex system. Is this book only about database systems or software in general? Whether you are using a database management system or not: most of the software artifacts we design manage data in one way or the other. Therefore, the techniques 16 IntroductionDatabase Systems = Data Layout Algorithms Hardware Systems Users Data Structures + + + + + Figure 1: diﬀerent aspects of database systems we learn about in this book (and the videos accompanying it) are not only used inside database systems, but may be used in other more general software. The impact of these techniques may be in terms of performance, data consistency or both. This book is designed to teach best practices in data management. This means, we will not focus on special cases, but rather spend most of our time discussing general techniques that have been identiﬁed in the past four decades of database research to be extremely useful and eﬃcient. We will also phrase some of those techniques as ‘data design patterns’ (similar to the standard software design patterns presented by Gamma et.al. [GHJV95]). 0.1 Course Overview and Motivation 17 0.1.2 Architecture of a DBMS Material Video: Original Slides: Inverted Slides: Learning Goals and Content Summary Indexes Data Layouts Algorithms DBMS Indexer Store Query Optimizer System AspectsScheduler Figure 2: Layers of a database system What are the diﬀerent layers of a database system? layers The layers are hardware, store (aka storage), indexer, and query optimizer. Hardware may or may not be considered to be part of the database system. Usually, it makes sense to consider it to be part of the system in order to fully exploit the performance 18 Introduction optimizations possible on a particular hardware platform. Some database systems are even sold as a package of software and hardware (aka appliance). How do they relate to the diﬀerent vertical topics?vertical topic The query optimizer is mostly about algorithms, the indexer mostly about data structures (indexes), and the store mostly about data layouts. So basically each layer in a database system corresponds to one of the vertical topics. What are the major tasks of store, indexer, and query optimizer?store indexer query optimizer The main task of the store is to manage ﬁne-granular data items (rather than just ﬁles as done by a ﬁle system). The indexer provides data structures that allow us to quickly ﬁnd data items in the store (rather than just scanning through all items available). The query optimizer takes an incoming query (typically an SQL-statement created by an application program) and translates it into an eﬃcient program. SQL is declarative, i.e. it deﬁnes WHAT needs to be retrieved. In contrast, the query optimizer has to ﬁnd out HOW to retrieve that data. What are system aspects of database systems?system aspects Some aspects of the database system are hard to comprehend when investigating one of the layers alone. These ‘system aspects’ are cross-layer aspects that often need to be treated holistically. A good example for this is scheduling, e.g. what to do if the system is allowed to handle multiple queries/transactions concurrently? The consistency and performance problems implied by this should be handled considering their impact across all layers. What is the conﬂict of computation versus data access about? And does this conﬂictcomputation data access relate to the diﬀerent layers of a database system? Traditionally, many courses in computer science focus on the computational aspects of algorithms, e.g. the runtime complexity of an algorithm or data structure which is of- ten modeled along the number of CPU operations. In databases, very often the actual computation time of an algorithm is not the most critical aspect of an algorithm or sys- tem. Rather, the time it takes to retrieve or store (read or write) data items may have a much higher impact on the overall runtime of a program than the computational eﬀort. Therefore, depending on which layer of a database architecture we are talking about, the eﬀects of data access may outweigh the computational eﬀects. In general, the closer we get to the store, the higher are the data access eﬀects, e.g. particular layouts or random vs sequential access. Vice versa, the closer we get to the query optimizer the stronger are the computational eﬀects, e.g. computational eﬀort of join enumeration, eﬃciency of query unnesting. Quizzes 1. What can be considered architectural layers of a DBMS? (a) Hardware (b) BIOS 0.1 Course Overview and Motivation 19 (c) Operating system (d) Store (e) Indexer (f) Query optimizer (g) Application (h) User 2. The part of a DBMS that is concerned most with data layouts is: (a) The query optimizer (b) The indexer (c) The scheduler (d) The store 3. The part of a DBMS that is responsible to determine how to compute results to queries: (a) The store (b) The indexer (c) The scheduler (d) The query optimizer 4. The part of a DBMS that provides physical organizations speeding up selective access to tuples: (a) The store (b) The indexer (c) The scheduler (d) The query optimizer 5. Aphysicaldesign advisoris used for: (a) Designing high-performance database hardware (b) It is actually a job role for database administrators. (c) Tuning the knobs of a DBMS to achieve better performance 6. Which component is coordinating the diﬀerent actions inside a DBMS? (a) The user (b) The physical design advisor (c) The scheduler (d) The query optimizer 20 Introduction 7. The layer of a DBMS that is focusing mainly on data access and not so much on computation is: (a) The indexer (b) The query optimizer (c) The store (d) The scheduler Exercise Discuss: (a) What additional layer could be considered part of the database system? What would be the advantage of considering that layer? (b) What if you implement the DBMS as shown in the slides (and bypass the layer discussed in (a)? List at least one concrete advantage of doing this. (c) Some vendors implement parts of the DBMS in hardware (be it conﬁgurable FPGAs or as real chips). Examples include Netezza (acquired by IBM). What would be the advantages/disadvantages of this approach? 0.2 History of Relational Databases 0.2.1 A Footnote about the Young History of Database Systems Material Video: Original Slides: Inverted Slides: Learning Goals and Content Summary Why would it make sense to use a database system anyway? Almost every program needs to manage data at one point or another. So, why not bundle the data managing functionality in a separate software component which can then be developed, tested, and maintained separately. This has the advantage that software developers do not have to constantly re-invent the wheel in terms of data management. How did the development of database systems start? Since the early days of computer science people tried to come up with eﬃcient data managing solutions. The early approaches like navigational and hierarchical database systems had the problem that they did not support real physical data independence, physical data independence i.e. the developer had to know how the data was organized in the database system in order to be able to phrase queries. In other words: the WHAT (which data do I want?) and the HOW (how is that data actually retrieved by the system?) were not clearly 0.2 History of Relational Databases 21 separated. In the early 70s, this problem was solved with the relational model, invented by E. Codd. WHATHOW „Codd made relations, ... all else is the work of man.“ Figure 3: from non-relational to relational database systems What was a major change introduced by the relational model? relational model The major change was providing physical data independence, again: this separated the WHAT from the HOW. However note, that this separation is not fully preserved by modern database systems. For instance, still inside relational database management systems, the database administrator (DBA) has some inﬂuence on the HOW part by deciding which indexes to create, e.g. CREATE INDEX...,or how to assign data to physical storage, e.g. through tablespaces. What were major developments in database history? database history 22 Introduction The development of systems implementing the relational model. These systems are some- times referred to as relational database management systems (RDBMS). Then SQL and its standardization, several extensions to SQL and database systems in general like ob- ject orientation, parallel databases, analytical databases (OLAP), support for XML and JSON, data stream management and column stores. Since 2010, many database tech- niques have been revisited (again) in the context of ‘big data’ and so-called NoSQL systems. It is also worth noting that the ﬁeld of database systems, even though relational systems have been developed since the 70ies, is still facing major innovations. This is also underlined by the high number of new start-ups and acquisitions of start-ups through big, established database companies every year. Quizzes 1. What is the main distinguishing feature of pre-relational and relational database management systems? (a) XML support (b) Physical data independence (c) Big data support (d) Object orientedness 2. What was the ﬁrst commercial relational DBMS? (a) Oracle Database (b) IBM DB2 (c) Informix (d) Knoppix (e) IBM System R (f) MySQL 3. What does physical data independence mean? (a) The logical organization of the data is independent from the database schema. (b) The physical organization of data is independent from the database schema and the data in it. (c) Creating multiple copies of the database to speed-up query processing. 4. Which of the following types of DBMSs are the most important ones today? (a) Relational (b) Object-oriented (c) Object-relational 5. Having read-mostly, complex queries is also referred to as: (a) Big data 0.2 History of Relational Databases 23 (b) OLAP (c) Object-oriented DBMS (d) OLTP 6. What is an appliance? (a) Abundle of software and hardware (b) Astorage server (c) ADBMS (d) Acloud computingsoftware 0.2.2 Relational Database — A Practical Foundation of Produc- tivity Material Literature: [Cod82], Sections 0 to 4 Additional Material Literature: [Bac73] Learning Goals and Content Summary What was the problem with data management in the 60s? There was no sharp distinction between the logical view on the data (the WHAT part) and its physical representation (the HOW). Hence, the programmer had to know how data was stored rather than focussing on the what part. What is associative addressing in the context of a database system? associative addressing Associative addressing overcomes positional addressing. In positional addressing data items are identiﬁed by their position. In contrast, in associative addressing, data items are identiﬁable solely based on the triple (relation name, primary key, attribute name). What is a data model? data model Codd’s deﬁnes a data model to contain at least three components: 1. a structural part,e.g. domains,relations,attributes,tuples,candidateand primary keys, 2. a manipulative part, e.g. algebraic operators like select, project, and join which transform input relations into output relations, and 3. a integrity part, e.g. integrity constraints which impose restrictions on the data instances that may be represented in the structural part. 24 Introduction In the relational model, does the order of the columns or rows in a schema matter?relational model column row schema No, the order of columns in a relation is irrelevant. Obviously, the same applies to the order of rows. This is a feature of associative addressing. What is the primary goal of relational processing? A goal of relational processing was loop-avoidance which was assumed to boost program- mer productivity. This is in so much true that the actual SQL-statement is loop-free and declarative, i.e. the programmer does not have to write down loops, for instance to specify ajoin operation. However notethatas soon astheresultof an SQL-statementisfetched from a database into a programming language, e.g. through JDBC, the result is typically treated in a loop again. Is relational algebra intended to be used as a language for end-users?relational algebra end-users No, it is not. It is meant as a starting point to design appropriate sublanguages supporting these set operations. In a modern database system (as of 2015), (enriched) relational algebra is typically used as an intermediate language in particular for query processing, see Chapter 4, and query optimization, see Chapter 5. Quizzes 1. In the late sixties the DBMS failed to boost productivity due to the lack of (a) separation of logical and physical views of the data (b) support for views (c) support for stored procedures (d) set processing commands (e) iterative processing commands 2. Codd proposed to access data (a) by positions (b) by associative addressing (c) with indexes (d) in text format 3. Which of the following are not part of the relational model: (a) domains, relations, attributes (b) algebraic operators (c) integrity rules (d) indexes Chapter 1 Hardware and Storage 1.1 Storage Hierarchies Material Video: Original Slides: Inverted Slides: Additional Material Literature: [LÖ09], Memory Hierarchy [LÖ09], Main Memory [RG03], Section 9.1 Further Reading: [LÖ09], Storage Management [LÖ09], Storage Security [LÖ09], Write Once Read Many Wdisk and SSDperformance charts WRAM performance charts [PH12], Section 5 Learning Goals and Content Summary What are the properties of ideal computer memory? memory It would have unlimited capacity and bandwidth, zero random access times. It should be for free and persistent. In addition, it should not trigger any read errors whatever happens. What is the core idea of the storage hierarchy? storage hierarchy The core idea of a storage hierarchy is to approach ideal memory in terms of performance however with dramatically reduced costs. What is the relationship of storage capacity and access time to the distance to the storage capacity access timecomputing core? computing coreThe closer a storage layer gets to the computing core, the faster random access, at the same time the smaller the storage capacity. 26 Hardware and Storage costs/Byte, bandwidth capacity, access time core Registers L1 L3 main memory flash/hard disk L2 The Storage Hierarchy Typical Access Times access time core Registers L1 L3 main memory flash/hard disk L2 4cyc 10cyc 60cyc 60ns 5ms 1cyc Figure 1.1: A simple storage hierarchy and typical access times What is the relationship of costs/ bandwidth to the distance to the computing core?costs bandwidth The closer a storage layer gets to the computing core, the faster sequential bandwidth, the more expensive the memory (when calculated per Byte). What are typical access times of the individual storage layers? How does this translate to relative distances? The term “typical access time” indicates already some vagueness here. The actual access time depends a lot on the type of the device. As of 2015 it is fair to assume that accessing L1 takes ⇠4cycles, L2, ⇠10 cycles, and L3 ⇠60 cycles. Following memorybenchmark.net the latency for DDR3 and DDR4 RAM modules are in an interval of 15–116 ns. The video assumes slightly older RAM with 60 ns latency. It is fair to assume that modern 1.1 Storage Hierarchies 27 “L1 cache is like grabbing a piece of paper from your desk (2 second),Factor 2.5Factor 45 Interesting-Phrase Mining for Ad-Hoc Text Analytics Srikanta Bedathur†, Klaus Berberich†, Jens Dittrich‡, Nikos Mamoulis†\u0000, Gerhard Weikum† †Max-Planck-Institut für Informatik ‡Saarland University Saarbrücken, Germany Saarbrücken, Germany {bedathur,kberberi,nmamouli,weikum}@mpi-inf.mpg.de jens.dittrich@cs.uni-saarland.de ABSTRACT Large text corpora with news, customer mail and reports, or Web 2.0 contributions offer a great potential for enhancing business-intelligence applications. We propose a framework for performing text ana- lytics on such data in a versatile, efﬁcient, and scalable manner. While much of the prior literature has emphasized mining key- words or tags in blogs or social-tagging communities, we empha- size the analysis of interesting phrases. These include named en- tities, important quotations, market slogans, and other multi-word phrases that are prominent in a dynamically derived ad-hoc sub- set of the corpus, e.g., being frequent in the subset but relatively infrequent in the overall corpus. We develop preprocessing and in- dexing methods for phrases, paired with new search techniques for the top-k most interesting phrases in ad-hoc subsets of the corpus. Our framework is evaluated using a large-scale real-world corpus of New York Times news articles. 1. INTRODUCTION With the dramatic growth of business-relevant information in various textual sources, such as user-interaction logs (web clicks etc.), news, blogs, and Web 2.0 community data, text analytics is getting a key role in modern data mining and Business-Intelligence (BI) for decision support. Analysts are often interested in examin- ing a set of speciﬁcally compiled documents, to identify their char- acteristic words or phrases or discriminate it from a second set. Tag clouds and evolving taglines are prominent examples of this kind of analyses [2, 6, 18]. While there is ample work on this topic for word or tag granularities, there is very little prior research on mining variable-length phrases. Such interesting phrases include names of people, organizations, or products, but also news head- lines, marketing slogans, song lyrics, quotations of politicians or actors, and more. In this paper, we focus on the analysis of interesting phrases in ad-hoc, dynamically derived document collections, for example, by a keyword query or metadata-based search from a large document corpus. Interestingness can be deﬁned with the help of statistical \u0000on leave from the University of Hong Kong Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for proﬁt or commercial advantage and that copies bear this notice and the full citation on the ﬁrst page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission and/or a fee. Articles from this volume were presented at The 36th International Conference on Very Large Data Bases, September 13-17, 2010, Singapore. Proceedings of the VLDB Endowment, Vol. 3, No. 1 Copyright 2010 VLDB Endowment 2150-8097/10/09... $ 10.00. measures that compare the local frequency of a phrase in the ad- hoc collection with its global frequency in the entire archive. For example, consider the results of keyword query “Steve Jobs” on a news archive. The most interesting phrases may include “apple chief executive”, “mac os x”, “the computer maker”. The ratio local/global frequency of these phrases is high, therefore they are deemed appropriate in characterizing the query results. In [23] a phrase inverted index is developed for ﬁnding the most interesting phrases in an ad-hoc subset D\u0000 of the overall corpus D. As a preprocessing step, for each phrase, identiﬁers of documents in D that contain the phrase are collected into an index list, built in an IR-style inverted-ﬁle fashion [28]. In order to compute the frequencies of the phrases in D\u0000, the inverted lists are accessed and intersected with D\u0000. An approximate counting technique that inter- sects only a sampled subset of each list with D\u0000 is proposed; still, a very large number of lists has to be accessed – potentially as large as the number of phrases, regardless of the size of D\u0000. As news, blogs, and web-usage corpora become rapidly larger, the phrase- inverted-index method becomes practically infeasible for interac- tive analytics. In fact, the experiments in [23] only reported results on a corpus of 30,000 publications. In this paper, we develop an efﬁcient alternative to [23] with much better scalability. We pre-process the documents in the entire corpus D and extract all phrases (above some minimum-support threshold). We then encode and index the phrases contained in each document in a forward index list. Given a subset D\u0000 \u0000 D, in order to determine the frequencies and compute the interesting- ness of the phrases there, we scan and merge the forward index lists of the documents in D\u0000. We propose several variants of this approach, based on different ways of ordering and compressing the phrases in the lists. These variants in turn lead to alternative algo- rithms for the phrase mining, with different capabilities for pruning the search space. As the number of phrases that are contained in D\u0000 can be very large, we focus on ﬁnding the top-k interesting phrases. We offer a systems-level solution that scales to very large corpora D. Our methods are evaluated using a corpus of nearly two million articles from the New York Times archive. Our problem setting differs from classic sequence mining [27] by the ad-hoc nature of the subset D\u0000 of D: D\u0000 is dynamically derived from queries and we gear for this novel situation by judicious indexing of D. The rest of the paper is organized as follows. Section 2 deﬁnes a representative interestingness measure for phrases in an ad-hoc subset of a corpus. In Section 3, we present alternative meth- ods for indexing the document corpus and searching for interesting phrases, including the framework that we propose in this paper. We experimentally demonstrate the efﬁciency and scalability of our ap- proaches in Section 4. Section 5 reviews related work and Section 6 concludes the paper. Metal Balls in Bowery Jack Greenhorn jack@village1.com #42Relative Distances! 2013 by George Orwell L2 cache is picking up a book from a nearby shelf (5 seconds), DRAM is taking a walk down the hall to buy a Twix bar (90 seconds).“ L3 cache is picking up a book from the next room (30 seconds),Factor 15Factor 3,750,000 “hard disk is like walking from Saarland to Hawaii.“ 7,500,000 seconds of walking! = 86.8 days! Figure 1.2: Access times translated to a real-life scenario: picking up something from your desk vs walking to Hawaii RAM has about ⇠20ns latency only. What are typical sizes of the diﬀerent storage layers (aka storage levels)? How do they storage layer storage leveltranslate to relative distances? Again, this is a parameter that improves quickly every year. It depends on your CPU, the number of CPUS and the prize you pay for your server. As a rule of thumb as of 2015, as we pick a typical CPU, say an Intel Haswell L1 is 64 KB per core (32 KB for data, 32 KB for code), L2 256 KB per core, and L3 may be in-between 8–25MB (shared on the CPU). DRAM is measured in the hundreds of GBs. Having a server with 1 TB of RAM is not uncommon and aﬀordable even for small companies. Due to these numbers most relational database systems ﬁt comfortably into DRAM. 28 Hardware and Storage L1 L3Factor 8 DRAMFactor 524,288 Relative Sizes! L2Factor 256 L1 DRAM Zoom out: L2 L3 Figure 1.3: Relative Sizes of the diﬀerent caches in a storage hierarchy: L1–L3 caches vs DRam What are the tasks of each level of the storage hierarchy? Localization of data objects, caching of data from lower levels (typically inclusion), im- plementation of data replacement strategies and writing of modiﬁed data (possibly syn- chronization with other caches) diﬀerent strategies (write through and write back). How would we use a storage hierarchy to cache only reads?cache If you use the write through strategy, write operations are never cached. This means, the storage hierarchy caches only data w.r.t. read but not to writes. How do we use a storage hierarchy to cache both reads and writes? If you use the write back strategy, write operations are cached as well. This means, the 1.1 Storage Hierarchies 29 storage hierarchy caches data w.r.t. both read and writes. Extra care has to be taken to persist changes performed by write operations. What is inclusion? inclusion Inclusion means that data available on a higher level is a subset of data on a lower level. This does not have to hold strictly. For instance, main memory typically includes data structures that are not necessarily existent on disk. What is a data replacement strategy? data replacement strategy Whenever a storage layer has to store some data, yet there is no free slot available for storing that data, some data has to be evicted from that storage layer. The data replacement strategy decides which data item to remove. Obviously it has quite some eﬀect on the performance of a storage hierarchy. Quizzes 1. What level of the storage hierarchy can be accessed the fastest? (a) L1 Cache (b) Memory (c) Disk (d) Register 2. What of the following levels of the storage hierarchy can typically store the most data? (a) L1 Cache (b) Memory (c) Disk (d) Register 3. The L2 Cache includes the L1 Cache. Does the hard disk include the whole main memory? (a) Yes, if we strictly apply the inclusion property. (b) No, inclusion forbids this. (c) Often, but in practice inclusion is not strictly applied between these layers. 4. How does the write through strategy on any level of the storage hierarchy work? (a) Amodiﬁed itemis written back tothe lowerlevel of the storage hierarchy just before it gets evicted. (b) Every modiﬁcation is immediately written back to the lower level in the storage hierarchy. (c) Every modiﬁed item is written immediately to disk. 5. How does the write back strategy on any level of the storage hierarchy work? 30 Hardware and Storage (a) A modiﬁed item is written to the lower level of the storage hierarchy just before it gets evicted. (b) Every modiﬁcation is immediately written back to the lower level in the storage hierarchy. (c) Every modiﬁed item is written back to disk after it has been evicted. 1.1.1 The All Levels are Equal Pattern Material Video: Original Slides: Inverted Slides: Learning Goals and Content Summary What is the central observation of the All Levels are Equal Pattern? No mater what layer of the storage hierarchy we are talking about, the techniques and algorithms used a very similar. Those algorithms may need some tweaking, yet their central idea is often the same. What does All Levels are Equal mean for practical algorithms? How can I exploit it to solve a problem on a speciﬁc layer of the storage hierarchy?storage hierarchy If you have an algorithm X that works for one speciﬁc layer Y of the storage hierarchy, you should consider adapting X to work for storage layer Y. How to misunderstand the All Levels are Equal pattern?pattern You misunderstand it if you take it literally. The statement “All levels are equal” is incorrect in the strict sense that the exact same techniques can be used, however the statement is correct as a high-level observation: the core ideas of the algorithms are often the same, just the granule, i.e. the layer(s) an algorithm operates on is exchanged. Compare also the fractal design pattern in Section 2.3.5. 1.1.2 Multicore Storage Hierarchies, NUMA Material Video: Original Slides: Inverted Slides: Additional Material Further Reading: [KSL13] 1.1 Storage Hierarchies 31 costs/Byte, bandwidth capacity, access time core Registers L1 L3 main memory flash/hard disk L2 A Single-Core Storage Hierarchy costs/Byte, bandwidth capacity, access time core Registers L1 L3 main memory flash/hard disk L2 A Single-Core Storage Hierarchy CPU board Figure 1.4: A single-core storage hierarchy and how it is mapped to a CPU and the board Learning Goals and Content Summary How are the two terms core and CPU related? core CPUA CPU may contain several cores (separate arithmetic logical units). What does a typical multicore storage hierarchy look like? What is diﬀerent compared to multicore storage hierarchy the storage hierarchy? It looks similar to a standard storage hierarchy except that each core has separate L1 and L2 caches. Why isn’t L1 shared as well among multiple cores? L1 For two major reasons: the access times would go down (for physical reasons) and too 32 Hardware and Storage L3 main memory flash/hard disk core Registers core Registers core Registers core Registers A Multicore Storage Hierarchy CPU board L3 main memory flash/hard disk L3 main memory flash/hard disk L3 main memory flash/hard disk Non-Uniform Memory Access (NUMA) board Figure 1.5: A multicore storage hierarchy vs NUMA many locking conﬂicts would occur. What is a Non-Uniform Memory Access (NUMA) architecture? Non-Uniform Memory Access In NUMA,main memory is not considered as one uniform unit but rather split into NUMA regions. Each region is typically attached to one CPU. This implies that a CPU has NUMA-local memory as well as NUMA-remote memory. Access to NUMA-local memory is slightly faster than accessing NUMA-remote memory. Though the term NUMA is typically used w.r.t. memory, similar eﬀects may happen on any layer of the storage hierarchy. What is the diﬀerence of NUMA compared to the multicore architecture? multicore architecture In a multicore architecture we use local L1 and L2 caches in NUMA we add local main 1.1 Storage Hierarchies 33 memories. We could also say: in a multicore architecture we have non-uniform L1 and L2 access as access to non-local L1 and L2 is slightly more expensive. What does NUMA imply for accesses to DRAM? DRAM Memory accesses to non-local memory (be it main memory or caches or whatever) is slightly more expensive. This should be factored in into algorithm design. However, be aware that the overheads of remote-access may be small (depending on your concrete application). How again does this relate to The All Levels are Equal? It is fair to say that a main memory system with diﬀerent RAM banks as well as a shared- nothing system are both NUMA, the former w.r.t. RAM, the latter w.r.t. disks. So again, if a phenomenon exists for one layer of the storage hierarchy, it very likely also exists for other layers. Quizzes 1. Which components are replicated in a multicore architecture? (a) L1 Cache (b) L2 Cache (c) L3 Cache (d) Registers (e) Hard Disk 2. Which components are replicated in a multi socket (NUMA) architecture? (a) L1 Cache (b) L2 Cache (c) L3 Cache (d) Registers (e) Hard Disk (f) Motherboard 34 Hardware and Storage 1.2 Storage Media 1.2.1 Tape Material Video: Original Slides: Inverted Slides: Additional Material Literature: [LÖ09], Storage Devices Learning Goals and Content Summary What are the major properties of tape?tape Tape has (very) slow random access due to winding (approximately 100 sec) and high bandwidth (about 100MB/sec). It is good for archival storage. Tape storage is typically available as separate tape cartridges which are accessed by a tape drive. Why would tape still be important these days? It is still heavily used for archives and backups, but it will get more and more replaced by hard disks. What is a tape jukebox? How does it work? What does this imply for access times?tape jukebox access time A jukebox is an automated library of tapes, a tape drive and a robot. The robot fetches tapes and places them in the tape drive. This adds some additional delay to the random access time. Therefore, random access on a tape cartridge in a jukebox is the sum of the times for fetching the tape cartridge, placing it in the tape reader, and winding the tape to the right position. Quizzes 1. Why are there still tape drives out there? (a) Fast random access (b) Cheap space for archival purposes (c) Super high bandwidth 2. Tape Jukeboxes allow for faster random access compared to a single tape reader. (a) False (b) True 3. Tape Jukeboxes allow for near-line access to an archive. Why is that the case? (a) The robot works 24/7. (b) The robot replaces the librarian who is expected to be slower. 1.2 Storage Media 35 1.2.2 Hard Disks: Sectors, Zone Bit Recording, Sectors vs Blocks, CHS, LBA, Sparing Material Video: Original Slides: Inverted Slides: Additional Material Literature: [LÖ09], Disk Video, a Hard Disk in action Further Reading: [LÖ09], Active Storage Learning Goals and Content Summary What are the major components and properties of hard disks? Disks are at their core mechanical devices: they are build around a stack of magnetic platters rotating on a spindle, platters can be read from/written to on both sides. Why would hard disks still be important these days? hard disk in Industry is slow-moving, major db products still disk-based. Disks are also required for datasets exceeding main memory. In addition, many legacy database systems are still in use. Even for main-memory centric systems, it is also important to persist changes for durability and recovery purposes (like in ARIES recovery). What is a platter? platter A platter is a magnetic disk used for storage. Platters are stacked on a spindle which rotates with constant speed, e.g. several thousand rotations per minute. What is a diskhead and a disk arm? diskhead disk armA diskhead reads from and writes to exactly one side of one platter. A disk arm contains one disk head for each side of each platter. The disk arm may be moved and positioned on diﬀerent cylinders. Only one of those disk heads may be active at any as the position of the arm must be ﬁne-tuned for each side of a platter separately. This means, within a particular cylinder there is still some ﬁne-tuning involved to position the currently active disk head. How do tracks and cylinders relate? track cylindertrack on a platter contains all points having the same distance to the center of the platter, i.e. it corresponds to a circle with that radius around the center. The tracks of all platters having the same distance to the center are grouped into a cylinder. What is the diﬀerence of a circular sector from a HD sector? circular sector HD sectorA circular sector denotes the surface of the disk enclosed by two radii and an arc. The size of the sector is deﬁned by the angle among the two radii. In contrast, a hard disk sector is a consecutive sequence of bits on any track. Typically, a hard disk sector has a ﬁxed size of 512 Bytes or bigger. 36 Hardware and Storage Hard Disks arm (disk) heads platter cylinder = set of similar tracks for all platters spindle (virtual) tracks Circular Sectors vs. HD Sectors hard disk sector: 4K block of data circular sector: angle Figure 1.6: The principal architecture of a hard disk; circular vs HD sectors What is zone bit recording?zone bit recording The circumference of a track grows with its radius. Therefore the larger the radius the more HD sectors ﬁt on a track. Tracks containing the same number of HD sectors are grouped into a zone. In other words: a zone is a set of adjacent tracks having the same number of HD sectors. What does this imply for sequential access?sequential access Assuming a constant rotation speed of the platters, sequential read and write performance is higher the closer the track is positioned to the edge of the platter. Where are self-correcting blocks used? self-correcting block 1.2 Storage Media 37 Zone Bit Recording zone HD Sector vs. OS Blocks hard disk sector: 4K block of data operating system block: 8K block of data Figure 1.7: Zone bit recording; HD sectors vs operating system blocks Some drives use internal error-correction codes to be able to detect erroneous blocks and faulty reads, i.e. the data read is diﬀerent from what was written before. What is the diﬀerence of HD sectors and operating systems blocks? operating systems block Blocks used by the operating system are typically larger, i.e. a multiple, of a HD block. What is physical cylinder/head/sector-addressing? physical cylinder/head/sector- addressing This is an old addressing scheme for hard disks exposing the physical properties of the disk. Blocks are addressed using a compound key of cylinder, head, and sector. What is logical cylinder/head/sector-addressing? How does it relate to the former? logical cylinder/head/sector- addressing This addressing scheme has a similar interface as in physical cylinder/head/sector- 38 Hardware and Storage addressing. However, block addresses are mapped to diﬀerent physical positions by the hard disk controller (a small computational device which is part of the disk, see also Section 1.2.4). This implies the device mimics a certain architecture, i.e. X cylinders, Y heads, and Z sectors. In reality, that drive may have completely diﬀerent physical properties. What is logical block addressing? logical block addressing In LBA a simple integer key domain is used for addressing blocks. As these ad- dresses are mapped to physical positions by the hard disk controller (just like in logical cylinder/head/sector-addressing), there is no need to expose an artiﬁcial cylinder/head- /sector to the interface anyway. What is sparing?sparing Erroneous blocks may be remapped to diﬀerent physical locations by the hard disk con- troller. What does this imply for a sequential access?sequential access It increases the likelihood of random accesses (yet this remapping shouldn’t happen too often). Quizzes 1. Why are hard disk systems in use today? (a) Durability (from ACID) (b) To provide enough storage space for very large datasets (c) Many legacy systems are still disk-based. 2. If a disk rotates with 15,000 RPM (rotations per minute), how long does it take to rotate exactly once? (a) trot = 15, 000/3600sec (b) trot =1/(15, 000/60)/3600sec (c) trot =1/(15, 000/60)/60sec (d) trot =1/(15, 000/60)sec 3. What zone has the highest transfer rate? (a) They are all equal. (b) The inner zone has the highest transfer rate. (c) The outer zone has the highest transfer rate. 4. Which of the following addressing methods are supported by hard disk controllers? (a) physical addresses (b) Logical Block Addressing (c) Logical CHS 1.2 Storage Media 39 5. Which of the following are true? (a) Atrack consists of spindles. (b) Aplatter has severaltracks. (c) Aspindle is connected to severalplatters. (d) Atrack has severalsectors. (e) There is a separate arm for each cylinder. 6. Which of the following are true? (a) OS blocks are always the same as HD sectors. (b) OS blocks can be composed of several HD sectors. (c) HD sectors always contain several OS blocks. 7. How many HD sectors are contained in a circular sector? (a) That depends on the angle and the track. (b) An HD sector is a synonym for circular sector. Exercise Consider a disk with 2,000 cylinders where the cylinders are numbered from 0 to 1,999 (numbering start from the outermost track). For simplicity, let’s assume that a track on cylinder i has 50 \u0000bi/100c blocks per track, 5 double-sided platters, and a block size of 4096 bytes. Suppose that a ﬁle contains 1,000,000 records with 200 bytes each where no record is allowed to span more than one block. 1. How many zones does this disk have? 2. How many records ﬁt into one block? 3. How many blocks are required to store the entire ﬁle? If the ﬁle is arranged se- quentially on disk (ﬁlling up one cylinder after the other), how many cylinders are needed (when starting storing the ﬁle in the outermost zone)? 4. How many records of 200 bytes can be stored using this disk? 5. Assume that the time to move the arm between 2 tracks is at least 2 ms whereas the head switch time is 1 ms, the average seek time is 6 ms, the disk platters rotate at 10,000 rpm: how much time is required to read the entire ﬁle sequentially? Further assume that the data is laid out on disk with head skew and track skew in mind. 1.2.3 Hard Disks: Sequential Versus Random Access Material Video: Original Slides: Inverted Slides: 40 Hardware and Storage Hard Disk Evolution for Enterprise Disks 0 75 150 225 300 2000 2003 2006 2008 2010 2013 Seq. Read [MB/s] Seq. Write [MB/s] 0 4 8 12 16 2000 2003 2006 2008 2010 2013 Random Read [ms] Source: http://www.tomshardware.com/charts/hard-drives-and-ssds,3.html Figure 1.8: Evolution of sequential read and writ bandwidth and random access over time Learning Goals and Content Summary What exactly is a random access and what are its major components?random access If we read from or write to a sector that requires the disk drive to reposition its disk arm, we call this type of access a random access. A random access on hard disk has two major components: 1. the actual arm movement to position a particular disk head on the right track, and 2. rotational delay, i.e. we have to wait that the HD sector are interested in becomes available under the disk head. Though we should also count in the costs for transferring the actual block, the random access time is dominated by these two components. What is a sequential access? and how do we estimate its costs?sequential access If we read from or write to a sector that does not require the disk drive to reposition its disk arm, we call this type of access a sequential access. A sequential access initially has the same costs as a random access (if the disk arm is not already in the correct position). After that it the costs are dominated by the transfer time. Ideally, a sequential scan will ﬁrst read/write all HD sectors on the same track. Then it will switch to another platter on the same cylinder. It will switch through all platters until all platters for this cylinder have been considered. Only after that it will switch to an adjacent cylinder. Switching disk heads takes some time (about 1ms), as well as switching to an adjacent cylinder (about 1ms). What is track skewing?track skewing The idea of track skewing is to position HD sectors such that rotational delay fully overlaps with switch times. This means, HD sectors on tracks on the same cylinder (as well as on adjacent cylinders) are slightly shifted in their position on the platter. The shift is done such that the head switch time (or the cylinder switch time) corresponds to the rotational 1.2 Storage Media 41 delay. Like that no additional delay occurs once the disk head is in place. How did hard disks evolve over the last decades? hard disk In general random access times evolve slowly. For end-user disks they may even get worse. In contrast, sequential access times increase considerably every year. What do we learn from that? Accessing data sequentially gets cheaper every year. In contrast, accessing data randomly does not improve much. Bonus Question: What does this imply for index structures? index This has major impact on indexing decisions (discussed later on in this course). As scans become cheaper, for certain types of queries indexing does not pay oﬀ anymore. We will get back to this in Chapter 3. Quizzes 1. What type of accesses are supported by hard disks? (a) Temporal Access (b) Random Access (c) Sequential Access (d) Microsoft Access 2. What are the major components of random access time? (a) tr:time needed to rotate to the correct angle (half a rotation on average) (b) ts:time to move the arm to the right track (c) ttr:time to transfer the data (d) top:time the operating system needs to send the read command to the disk controller 3. How do you compute the costs to randomly read all blocks of a whole ﬁle from disk? Assume the ﬁle consists of x blocks. Given the rotational delay tr,the seek time ts and the time needed to transfer a single block ttr. (a) tran = x · (tr + ts + ttr) (b) tran = tr + ts + x · ttr 4. When sequential reading and switching from one head or track to another you always have to wait on average half a rotation to start reading. (a) False (b) True 5. The random access times have improved tremendously over the last decades. (a) False 42 Hardware and Storage (b) True 6. In 1970, what percentage of a ﬁle could be read randomly in the time needed to read the ﬁle fully, when reading sequentially? (a) 25 percent (b) 50 percent (c) 1percent 7. With modern hard drives, what percentage of a ﬁle can be read randomly in the same time needed to read the ﬁle fully, when reading sequentially? (a) 25 percent (b) 50 percent (c) 1percent Exercise Suppose you have 2 20 blocks of size 8 KB each sequentially laid out on the device: S-ATA disk: Average Seek Time = 2 ms Rotational Speed = 15,000 RPM Sustained Transfer Rate = 150 MB/s Maximum Transfer Rate = 600 MB/s The maximum transfer rate describes the bandwidth that can be observed on the outer zone if no head switch or movement needs to be done. For simplicity, you can assume, that the whole ﬁle ﬁts in the outer zone. In contrast, the sustained transfer rate is achieved when large consecutive portions of the disk are read. This is a simpliﬁcation and includes all the needed head switch and arm movement times involved in a sequential scan. Give the percentage of blocks that should be accessed in a full table scan to outperform random I/O. 1.2.4 Hard Disk Controller Caching Material Video: Original Slides: Inverted Slides: 1.2 Storage Media 43 costs/Byte, bandwidth capacity, access time core Registers L1 L3 main memory flash/hard disk L2 The Hard Disk Cache disk cache Figure 1.9: The relative position of the hard disk cache within the storage hierarchy Learning Goals and Content Summary What exactly is the hard disk cache? hard disk cache The hard disk cache is a small amount of volatile memory (typically about 128MB) managed by the hard disk controller. How is the hard disk cache related to the storage hierarchy? storage hierarchy It is like adding a mini storage hierarchy inside the level ‘ﬂash/hard disk’. What do we gain by using the hard disk cache? What do we lose? We see the same eﬀects as for every situation when a faster layer in a storage hierarchy caches data from a layer above: read requests already available in that cache (in this case the hard disk cache) do not have to be fetched from underneath (in this case the actual platters). For write requests: once we write data to the cache (in this case the hard disk cache) from a storage layer above (say main-memory) we do not necessarily write it through immediately to the layers underneath (in this case the actual platters). The latter optimization obviously also has some risks: if data was only written to the volatile cache but not to the persistent disk, we may lose that data. What does this imply when storing data on disk? We must make sure to understand when data written to disk is actually forced to the platters. This can typically be enforced by calling a separate flush-command. What is the elevator optimization? elevator optimization If the hard disk controller receives requests for multiple tracks, it may reorder the exe- cution order of those requests. So rather than executing requests in the same order as they arrive, the controller may improve the overall throughput of the device by handling requests which can be visited on the way. This is similar to an elevator which will stop 44 Hardware and Storage on every ﬂoor where the button got pressed. Quizzes 1. What is typically cached in the disk cache? (a) Just the requested block. (b) The whole track that was read to serve the read request. 2. For what types of requests can the disk cache be used? (a) Only read requests. (b) Both read and write requests. 3. Why is it important to ﬂush after writing to disks? (a) The data might still be in the disk cache and not yet written to disk. (b) The data is only kept in main memory till a call to ﬂush occurs. 4. Accesses to the hard disk is always served in the order of the request arrivals. (a) False (b) True 5. Accesses to the hard disk can be reordered by the disk controller to increase the throughput of the hard disk. (a) False (b) True Exercise Assume we extend the HD-interface to allow for the retrieval not only of a block having a particular logical block-ID, but a conditional retrieval where the block is only returned if it contains a particular byte-sequence (of any size smaller than the block size). For instance, rather than sending something like: getBlock(42) ! block_contents, which returns the contents of block 42, we want to have something like this: getBlock(42, 0x4304F 04F 04C) ! block_contents. Only if block 42 contains that byte-sequence, the contents of that block are returned (just like before), otherwise an error code NOT_FOUND is returned. Notice that this may be extended to something like: getBlocks(0, 420000, 0x4304F 04F 04C) ! {block_contents}. 1.2 Storage Media 45 Latency versus Throughput time0 1 2 3 4 5 6 7 8 9 non-batched: f() f() f() f() f() f() f() f() f() f() time0 1 2 3 4 5 6 7 8 9 batched: f() f() Figure 1.10: Balancing latency and throughput when batching data “get me all blocks in range 0 to 420000 including having that byte-sequence”. Notice that this call returns a set of blocks containing the qualifying blocks only. (a) Discuss why such an idea does not make sense. Find arguments why this increases costs in terms of the overall system costs and makes the implementation of ﬁlter functionality on top of this device more diﬃcult and less eﬃcient. (b) Discuss why such an idea does make sense. Find arguments why this saves costs in terms of the overall system costs and makes the implementation of ﬁlter functionality on top of this device easier and more eﬃcient. 1.2.5 The Batch Pattern Material Video: Original Slides: Inverted Slides: Learning Goals and Content Summary What is the central observation here of the batch pattern? batch pattern If you are in a situation where the same function f(item) is applied to a set of items individually, it often makes sense, to collect k items in a batch and then apply a modiﬁed function f(batch) on each batch of items. What are the advantages? The costs for processing k items in a batch may be cheaper than processing k times f(item). The throughput is likely to increase (which is typically good). 46 Hardware and Storage What are the disadvantages? The latency for an individual request is likely to increase (which is typically bad). What does this mean for practical algorithms? We have to ﬁnd the speed spot: collect as many items as to keep the latency low enough. What this means exactly is typically deﬁned by the application in terms of service-level agreements, aka SLAs. What are possible applications? Possible applications are queries in a database system ;-), requests to HD sectors, and requests to data items. Quizzes 1. Why should you use the batch pattern? (a) To increase throughput. (b) To decrease latency. 2. In the batch pattern several function calls on single items are combined to a single function call on multiple items. (a) False (b) True 3. Please mark all the following examples that use the Batch Pattern. (a) Single Instruction Multiple Data (SIMD) (b) Elevator Optimization (c) Batch ﬁles in Windows (.bat) 4. Algorithm A processes 10 items in 5 seconds and B processes 25 items in 10 seconds. What can be said about the performance of the algorithms? (a) Ahas ahigherthroughputthan B (b) Bhas ahigher throughputthan A (c) both have the same throughput 1.2 Storage Media 47 1.2.6 Hard Disk Failures and RAID 0; 1; 4; 5; 6 Material Video: Original Slides: Inverted Slides: Additional Material Literature: [LÖ09], RAID [CLG +94] [PH12], Section 6.9 [RG03], Section 9.2 Further Reading: [SG07] [JFJT11] Learning Goals and Content Summary Why should I worry about hard disk failures? hard disk failures Hard case failures are common and may have several reasons. As a hard disk failure may result in loosing data you should design your system in a way that it can survive one or multiple hard disk failures. What is the core idea of RAID? RAID The core idea of RAID is to combine multiple hard disks in a Redundant Array of Inex- pensive Disks. Like that RAID is able to survive single disk failures (depending on the RAID-level used). RAID may be implemented in software and in hardware. What is the impact on performance of RAID 0? RAID 0 simply stripes data across the disks without introducing redundancy. Therefore, we do not gain anything w.r.t. reliability. However, we gain in terms of I/O performance: read and write requests may be executed in parallel by the diﬀerent drives. Is my data safer by using RAID 0? Not at all (see above). What is RAID 1? RAID 1 replicates blocks across all drives. If you have n disk drives in RAID 1, you have n copies of the same data. Hence, even if you lose any n \u0000 1 drives, you still have one drive left to read the data from. In terms of read I/O you may gain for requests that may be split up such that one portion of the data is read from one drive and another portion from another drive. Those read requests may be executed in parallel. In terms of write I/O notice that every block written to a RAID 1 system has to be written on each of the n drives. Yet these requests may be executed in parallel. What is the diﬀerence of RAID 4 and RAID 5? 48 Hardware and Storage RAID 0 B2 B4 B6 B8 B1 B3 B5 B7 RAID 1 B1 B2 B3 B4 B1 B2 B3 B4 Figure 1.11: RAID 0 vs RAID 1 RAID 4 is like RAID 0 with n \u0000 1 disks plus one additional disk for parity. The parity on that last disk is computed by XORing the striped blocks from the ﬁrst n \u0000 1 disks. Now, if any disks fails, the contents of that disk may be reconstructed by XORing the remaining disks. RAID 4 may survive the failure of one disk (no matter which one). A problem with RAID 4 is that the parity disk may become a bottleneck, i.e. any write operation also eﬀects the parity. Hence whatever you write you also have to touch the parity disk. Therefore a better solution is RAID 5 which distributes the parity blocks in around robin fashion. How many disks do I need for a RAID 4 or 5 system? At least three disks are required in both cases. 1.2 Storage Media 49 RAID 4 B1 B4 B7 B9 B12 B3 B6 Stripe4 parity Stripe2 parity Stripe1 parity Stripe3 parity B10 B8 B11 B2 B5 RAID 5 B1 B4 B7 B3 Stripe4 parity Stripe2 parity Stripe1 parityB2 B5 Stripe3 parity B8 B11B10 B6 B9 B12 Figure 1.12: RAID 4 vs RAID 5 Which sequential read performance can I expect in a system with n disks? For RAID 4 and 5 you can expect a speed-up of factor n \u0000 1. And which write performance? For single block writes there is no speed-up (in parallel you have to write the parity block as well!) For sequential write operations you may have similar gains as for reads (if the RAID controller is able to group write operations aﬀecting the same parity into a single write operation to that parity). How many disk failures may a RAID 4 or 5 system survive? They may both survive failure of a single disk (any of them). What is the diﬀerence between RAID 6 and RAID 5? RAID 5 uses a single parity whereas RAID 6 uses double parity. RAID 6 requires at least 4disks, butmaysurvive two diskfailures. 50 Hardware and Storage Quizzes 1. Why should you use multiple disks instead of a singe disk? (a) To decrease the likelihood of a failure of a single hard disk device. (b) To increase the likelihood of a failure of a single hard disk device. (c) To increase the mean time to failure of the I/O subsystem as a whole. (d) To decrease the mean time to failure of the I/O subsystem as a whole. 2. Please mark all properties of RAID 0. (a) The system survives a single disk failure. (b) The system can read from all disks in parallel (if the blocks you are interested in are uniformly distributed over all disks). (c) The system can split the write eﬀort to all disks in parallel (if the blocks you are interested in are uniformly distributed over all disks). 3. Please mark all properties of a RAID 1 system with n disks. (a) The system survives n-1 disk failures. (b) The system can read from all disks in parallel (if enough continuous blocks are requested). (c) The system can split the write eﬀort to all disks in parallel (if enough contin- uous blocks are written). 4. How many disk failures can a RAID 4 or RAID 5 system with n disks survive? (a) 1 (b) n \u0000 1 (c) n \u0000 2 (d) n/4 5. RAID 5 improves over RAID 4 by distributing the parity information. Therefore the parity disk is no longer the write bottleneck (a) False (b) True 6. How many disks are needed for the minimal RAID 6 system? (a) 4 (b) 3 (c) 2 1.2 Storage Media 51 1.2.7 Nested RAID Levels 1+0; 10; 0+1; 01 Material Video: Original Slides: Inverted Slides: Learning Goals and Content Summary RAID 1+0 or RAID 10 B1 B3 B5 B7 B1 B3 B5 B7 B2 B4 B6 B8 B2 B4 B6 B8 RAID 1 RAID 1 RAID 0 RAID 0+1 or RAID 01 B2 B4 B6 B8 B1 B3 B5 B7 B2 B4 B6 B8 B1 B3 B5 B7 RAID 0 RAID 0 RAID 1 Figure 1.13: RAID 10 (=1+0) vs RAID 01 (=0+1), Note: nested RAID-levels are read from bottom to top What is RAID 10? And why would we call RAID 10 a nested RAID? nested RAID RAID 10 (aka RAID 1+0) combines RAID-levels 1 and 0. It hierarchically combines (applies) both RAID-levels. On the leaf-level it combines multiple disks into a RAID 1 system (the leaf). Multiple leafs are then combined into a RAID 0 system. See Figure 1.13. And what is RAID 01 then? RAID 01 (aka RAID 0+1) combines RAID-levels 0 and 1. It hierarchically combines (applies) both RAID-levels. On the leaf-level it combines multiple disks into a RAID 0 52 Hardware and Storage system (the leaf). Multiple leafs are then combined into a RAID 1 system. See Figure 1.13. Is it possible, in principal, to combine any kinds of RAID-levels? Yes. What are the properties of diﬀerent nested RAID-levels? This depends on the concrete nesting. In general, nested RAID-levels inherit properties from the non-nested RAID-levels. However, in which level we apply the non-nested RAID- levels makes a diﬀerences. For instance, let’s compare RAID 10 and RAID 01 both four four disks. See Figure 1.1. They have the same properties w.r.t. performance and failover. This changes if we use more disks per group, see Figure 1.2. RAID 10 RAID 01 2⇥2 2⇥2 ((.,.),(.,.)) ((.,.),(.,.)) max seq. read speed ⇥4 ⇥4 max seq. write speed ⇥2 ⇥2 max disk failures 2 2 redundancy 2 2 Table 1.1: Comparison of RAID 10 and RAID 01 using four disks each. RAID 10 RAID 01 RAID 10 RAID 01 2L⇥3R 2L⇥3R 3L⇥2R 3L⇥2R ((.,.),(.,.),(.,.)) ((.,.),(.,.),(.,.)) ((.,.,.),(.,.,.)) ((.,.,.),(.,.,.)) max seq. read speed ⇥6 ⇥6 ⇥6 ⇥6 max seq. write speed ⇥3 ⇥2 ⇥2 ⇥3 max disk failures 3 4 4 3 redundancy 2 3 3 2 Table 1.2: Comparison of RAID 10 and RAID 01 using six disks each. L means leaf-level, Rmeansrootnode. Quizzes 1. How does a minimal RAID 10 setup look like? (a) You need four disks. Two disks form a mirror (RAID 1). On top of the two RAID 1 systems data is striped (RAID 0). (b) You need four disks. Two disks use striping (RAID 0). On top of the two RAID 0 systems data is mirrored (RAID 1). 2. How many disk failures can a minimal nested RAID system survive? (a) At least one. (b) Two depending on where the second failure occurs. (c) Up to three. 1.2 Storage Media 53 (d) At least two. 3. How many disks are needed at least to create a RAID 50 system? (a) 6 (b) 4 (c) 3 4. In a RAID 50 conﬁguration with 6 disks, how much data can be stored on the system? (a) The capacity of the smallest disk times four. (b) The capacity of the smallest disk times ﬁve. (c) The capacity of the smallest four disks (even if the disks have diﬀerent capac- ities). Exercise Discuss each of the RAID conﬁgurations below in terms of (a) read performance, (b) write performance, and (c) reliability (minimal and maximal number of disks that may fail). Assume twelve disks are used for both conﬁgurations and you are reading or writing sequentially from or to large ﬁles. 1. RAID 5 + 5 using three RAID 5 subsystems consisting of four disks each 2. RAID 6 1.2.8 The Data Redundancy Pattern Material Video: Original Slides: Inverted Slides: Additional Material Literature: [LÖ09], Replication for High Availability [LÖ09], Replication Further Reading: [LÖ09], Partial Replication 54 Hardware and Storage Learning Goals and Content Summary Why is data redundancy good?redundancy If we keep multiple physical copies of the data, we may aﬀord loosing all but one of them. Then, we are still able to recover the data. Similarly, if we keep error correction codes/checksums that allow us to reconstruct faulty data, we may be able to recover some of the data. Why is data redundancy bad? Data redundancy creates storage overhead. How much depends on the type of redundancy, i.e. the RAID-level used or for RAID 1 the number of replicas used. Can you list three examples where data redundancy is used for good? hard disks (RAID), datacenter redundancy, backups Quizzes 1. What are the beneﬁts of keeping redundant copies of your data on diﬀerent devices? (a) It decreases the likelihood of losing all devices with all copies of the data. (b) You can write in parallel and get higher throughput. (c) When reading, it is just ﬁne to read from one of the copies. 2. What are the drawbacks of keeping redundant copies of your data on diﬀerent devices? (a) additional write eﬀort (b) additional resource consumption (c) additional parity computation eﬀort (d) additional read eﬀort to retrieve the data 3. Which of the following are examples of the Data Redundancy Pattern? (a) RAID (b) Hot Standby Server (c) Database Partitioning (Sharding) 4. On what levels can you apply the Data Redundancy Pattern? (a) to combine multiple devices (b) to combine multiple systems (c) to combine multiple data centers 5. What kind of hazzard can the Data Redundancy Pattern on disk level solve? (a) Fire or Flood. (b) Defect in the disc. (c) Programming errors. 1.2 Storage Media 55 1.2.9 Flash Memory and Solid State Drives (SSDs) Material Video: Original Slides: Inverted Slides: Learning Goals and Content Summary Flash Memory Solid State Disk (SSD) Hard Disk (HD) Figure 1.14: Flash memory comes in diﬀerent form factors, when it comes in the shape of a hard disk we call it solid state disk (SSD). What are the major properties of ﬂash memory? ﬂash Flash memory is non-volatile. In contrast to hard disks it is also robust as it does not have any moving parts. In addition, many devices allow you to access data in parallel, 56 Hardware and Storage i.e. in contrast to a hard disk which can only read one sector at a time, some types of ﬂash memory can serve multiple requests concurrently. What is an SSD?SSD A solid state disk is ﬂash memory in the form factor of a hard disk. It is connected to the computer via a hard disk interfaces such as S-ATA or SAS. Like that it may simply replace a hard disk in a computer system. What is the major diﬀerentiator over hard disks w.r.t. performance?hard disk Amajorfeature of ﬂash memoryis thatrandomaccess is byafactorof 100faster —or even more: this depends on the speciﬁc device. In an SSD, what are blocks and superblocks?block superblock Superblocks contain a set of blocks. What does this mean for write operations? An SSD cannot simply overwrite a block. Therefore, if you want to overwrite the same physical block, you ﬁrst have to erase its superblock. However, in practice, with every write to a logical block you may map that logical block to a diﬀerent (already erased) physical block; this is similar to what you exploit in copy-on-write and like that you do not have to wait for the erase. What is write ampliﬁcation?write ampliﬁcation Write ampliﬁcation is a measure for the write overhead triggered by superblock erasure, garbage collection, and internal RAID. How would this aﬀect the storage layer of a database system?storage layer As hard disks pages can simply be overwritten, a hard disk does not have to tell the hard disk if it considers a particular page to be unused. This is a problem for SSDs as it cannot immediately erase that page. Only when the next write to that page occurs, the SSD will COW that page and eventually erase the old page. Therefore, in terms of performance it helps, if the storage layer of the DBMS informs the SSD about pages that are unused or will soon be overwritten. This can be done by calling trim() for particular blocks of the SSD. How does it relate to RAID?RAID Internally, many SSDs use a RAID-like conﬁguration of their memory banks to increase both performance and reliability, e.g. RAID 5. This is another example of Fractal Design, see Section 2.3.5. How does ﬂash memory relate to volatile memory?volatile memory SSDs typically contain a volatile cache just like hard disks. The same problems as with hard disk caches occur, in particular lost writes. Which sequential bandwidth can we expect these days? sequential bandwidth This depends a lot on the quality of the ﬂash memory and the interface used to connect it to the computer. For an SSD a sequential bandwidth of 500 MB/sec is realistic. 1.2 Storage Media 57 And which random access time? random access time About 0.05 ms Quizzes 1. Please mark all properties of SSDs. (a) SSDs are non-volatile (b) SSDs need some power source to keep the data persisted (c) SSDs are more robust than hard drives (d) SSDs can be accessed faster than hard drives, especially when accessed ran- domly. (e) SSDs provide parallel accesses 2. Asuperblockconsists of several blocks of typically 8KB (a) False (b) True 3. How are blocks written on an SSD? (a) One can only write into empty, freshly erased blocks. (b) If no block is empty, the system simply erases a block and writes into that block. (c) Erase can only happen at the superblock level. If no block is empty, the system has to erase a whole superblock and write into one of the erased blocks. 4. Why does an SSD sometimes physically write more data than logically required by the system? (a) super block erasure (b) garbage collection (c) blocks are always stored redundantly 5. To improve the performance of database systems that use SSDs the system should send trim()-commands to the SSD for a block, as soon as the block has been written. (a) False (b) True Exercise Assume a block storage interface allowing you to store logical blocks numbered from 0 to N . Each block can be identiﬁed by its logical ID termed LBID 2 0,. .., N . Internally, the device maps these blocks to k internal physical devices. This means, the device implements a mapping 58 Hardware and Storage LBID 7! {(DID, IBID, isPARITY)}. This means logical block IDs are mapped to a set of internal locations. Each internal location is a triple consisting of the ID of the internal device, termed DID 2 {0,. .., k \u00001}; the block ID on that device, termed IBID; and a ﬂag isPARITY indicating whether this internal location reﬂects parity information about this logical block. For instance, for a RAID 0 using k disks, assign_RAID_0(LBID, k)is implemented as: assign_RAID_0(LBID, k){ DID = LBID MOD k; IBID = LBID DIV k; isPARITY = false; return {(DID, IBID, isPARITY)}; } Recall, that if the set contains more than one pair-wise diﬀerent entry where isPAR- ITY==false, that block is stored twice. So, when calling assign_RAID_0(LBID, k) with an ascending sequence of LBIDs, and assuming three disks (k =3), we obtain: (0,3) 7! {(0, 0, false)}, (1,3) 7! {(1, 0, false)}, (2,3) 7! {(2, 0, false)}, (3,3) 7! {(0, 1, false)}, (4,3) 7! {(1, 1, false)}, (5,3) 7! {(2, 1, false)}, . . . (a) Implement assign functions for RAID 1, 4, and 5. (b) Implement assign functions for RAID 10, 01, and 51 (ﬁrst number is the type of the nested subsystem, i.e. for RAID 10, RAID 1 is used on the leaf-level and RAID 0 on the root) (c) What is the relationship of assign() to LBA and disk sparing in a single hard disk? (d) What is the relationship of assign() to wear-leveling and trim() on an SSD? 1.2.10 Example Hard Disks, SSDs and PCI-connected Flash Memory Material Video: Original Slides: Inverted Slides: 1.2 Storage Media 59 Learning Goals and Content Summary What are typical performance characteristics of hard disks and SSDs? hard disk SSDThey have similar throughput, random access times, however are roughly by a factor 100 better for SSDs. Would you buy an SAS (Serial attached SCSI) disk for your PC? SAS Probably not, as it is relatively expensive and loud. What is a PCI ﬂash drive? PCI ﬂash drive This is ﬂash memory connected to the computer by PCI. Why would using a PCI ﬂash drive be a good idea? Like that much higher bandwidth becomes possible. Why do I get considerably more random read operations per second than one divided by the random access time? random access time Many ﬂash devices can handle multiple requests in parallel. Quizzes 1. How can SSDs ﬁx your performance problems? (a) They provide much faster random reads and writes in comparison to HDDs. (b) They have higher storage capacities. (c) They allow for better cache-locality. 2. Why is the number of I/O-operations per second higher than 1 divided by the random access time for SSDs? (a) Several I/O operations can be served in parallel by SSDs. (b) This result is obtained when connecting several SSDs to the system. (c) The SSDs use more data buses simultaneously. 60 Hardware and Storage 1.3 Fundamentals of Reading and Writing in a Storage Hierarchy 1.3.1 Pulling Up and Pushing Down Data, Database Buﬀer, Blocks, Spatial vs Temporal Locality Material Video: Original Slides: Inverted Slides: Additional Material Literature: [RG03], Section 9.4.1 and 9.4.2 [LÖ09], Buﬀer Manager [LÖ09], Buﬀer Pool [LÖ09], Memory Locality Further Reading: [LÖ09], Buﬀer Management Learning Goals and Content Summary What does pulling up data mean?pulling up data This means that data is transferred from any layer of the storage hierarchy to a layer closer to the CPU(s). What does pushing down data mean?pushing down data This means that data is transferred from any layer of the storage hierarchy to a layer further away from the CPU(s). This is only necessary if that data was modiﬁed or newly created. Did you see the database buﬀer anywhere?database buffer The database buﬀer sits in-between main memory and hard disk. It controls which pages from hard disk are cached in main-memory and which pages are replaced and written back. What is temporal locality?temporal locality Given a set of address references A1,. .., An and some i and j where 1  i< j  n. If Ai = Aj and the distance of i and j is “small”, we coin this temporal locality. In other words: the same memory address is referenced twice within a “short” period of time. What “small” and “short” means, depends on the context. And what is spatial locality then?spatial locality Given a set of address references A1,. .., An and some i and j where 1  i< j  n. If distance(Ai,Aj)< \u0000 and the distance of i and j is “small”, we coin this spatial locality. In other words: a similar memory address is referenced twice within a “short” period of time. What “small” and “short” means, depends on the context. 1.3 Fundamentals of Reading and Writing in a Storage Hierarchy 61 core Registers L1 L3 main memory flash/hard disk L2 Pulling Up and Pushing Down DataReadingWriting Where is the Database Buffer? hardware, core Registers L1 L3 main memory flash/hard disk L2 hardware, hardware, software DB BufferReadingWriting Figure 1.15: Reading data is equivalent to pulling up data in the storage hierarchy. Writing data is equivalent to pushing down data in the storage hierarchy. What is the relative position of the DB-buﬀer? How are the two related? Spatial locality is a generalization of temporal locality. Temporal locality only considers memory addresses that are equal and reasons about their distance in time. In contrast, spatial locality considers similar memory addresses (including equality as a special case) and reasons about their distance in time. Quizzes 1. In disk-based database systems the database buﬀer (a) is involved in pulling up data 62 Hardware and Storage (b) is involved in pushing down data (c) is a hardware component (d) resides on the hard disk (e) is a dedicated special memory chip on DRAM 2. Which of the following are correct in the context of database buﬀers? (a) Blocks are loaded with higher speed than pages due to spatial locality. (b) Multiple pages form a block. (c) Blocks reside on hard disk. (d) Pages reside in main memory. (e) Blocks are pulled up, and pages are pushed down. 3. Temporal locality (a) is exploited by the database buﬀer (b) means accessing the same pages more than once in a given short amount of time (c) means accessing similar pages more than once in a given short amount of time (d) means accessing pages that where last modiﬁed nearly at the same time point (e) means actually pulling up the same blocks again and again from disk in a given short amount of time (f) apalindromehas temporal localityin acharacterstream 4. Spatial locality (a) is exploited by the database buﬀer (b) means accessing the same pages more than once in a given short amount of time (c) means accessing similar pages more than once in a given short amount of time means (d) accessing pages that where last modiﬁed nearly at the same time point (e) means actually pulling up the same blocks again and again from disk in a given short amount of time (f) apalindromehas thehighestpossible spatiallocalityin a characterstream 5. Prefetching, e.g. read ahead, done by hard disk controllers exploits: (a) spatial locality (b) temporal locality (c) database buﬀers (d) non-monotonous distance functions 1.3 Fundamentals of Reading and Writing in a Storage Hierarchy 63 Implementation of GET Get(Px): 1. If (not PAGE_IN_BUFFER(Px)):\t // check whether already exists 2. \t if (no empty slot available in buﬀer):\t // is there space to load a page? 3. \t\t S = Pi = CHOOSE_PAGE();\t // choose a page to kick out 4. \t\t if (Pi is dirty):\t // did anyone change this page? 5. \t\t \t ﬂush Pi to external memory;\t // oops, got to write it out ﬁrst 6. \t else:\t\t \t // we have space left anyway... 7. \t\t S = getFreeSlot();\t // pick a free page 8. \t read(Px, S);\t // read Px into free slot 9. ﬁx(Px);\t\t \t // ﬁx page Px 10. return Px;\t\t \t // return a reference to Px Figure 1.16: The implementation of the get method in the database buﬀer 1.3.2 Methods of the Database Buﬀer, Costs, Implementation of GET Material Video: Original Slides: Inverted Slides: Learning Goals and Content Summary What are the most important methods of the database buﬀer? database buffer get(), ﬁx(), unﬁx(), page_in_buﬀer(), and choose_page(). What does get(Px) do? get(Px) Ireturns areference/pointertoPage Px. What may happen when you request a page that is not in the buﬀer? page If all slots in the buﬀer are full, we ﬁrst have to choose a page for eviction (choose_page()). If the page to be evicted is dirty, we ﬁrst have to write it back to the storage layer underneath. Only after that we may load the page actually requested. What are the costs involved? costs In the worst case, two random I/O-operations may be triggered: one for writing back the dirty page to be evicted, the second for reading the page actually requested. If the page to be evicted is not dirty, we can obviously simply discard it. Then no write back is necessary. Who would evict a page? evict Page eviction is triggered by the DB-buﬀer every time no empty slot is available anymore. Other than that there is no reason to evict a page. Still, a DB-buﬀer should run an 64 Hardware and Storage additional background thread to regularly write back dirty pages in the background. This avoids the extra costs for writing back dirty pages at page eviction time, and it also improves recovery time. see also Section 6. Quizzes 1. The get-method of the database buﬀer may perform the following operations: (a) Check if the page is already in the buﬀer. (b) Read the page from disk. (c) Evict a page from the buﬀer. (d) Flush dirty pages to disk. 2. The operations of the database buﬀer trigger the following costs: (a) 2 disk seeks when it has to ﬂush a dirty page to disk before reading the new page. (b) Evicting pages from the buﬀer does not always necessitate a disk seek. (c) 2 disk seeks when it has to ﬂush a clean page to disk before reading the new page. 3. Why do we have to be careful when ﬂushing dirty pages to disk? (a) It is a signiﬁcant computational eﬀort to choose which page to ﬂush to disk, because of the high costs of generating random numbers. (b) It involves 1 random I/O operation. (c) It involves 2 random I/O operations. (d) To avoid reading a dirty page again into the free slot created by ﬂushing the page. 4. What is the role of the ﬁx and unﬁx methods? (a) to prevent a page from being evicted while a process is still working on that page (b) to prevent writing modiﬁed content back to disk (c) to prevent concurrent processes to read from the ﬁxed page 1.3.3 Pushing Down Data in the Storage Hierarchy (aka Writ- ing), update in-place, deferred update Material Video: Original Slides: Inverted Slides: 1.3 Fundamentals of Reading and Writing in a Storage Hierarchy 65 Learning Goals and Content Summary What is a direct write? direct write Assume a block A is modiﬁed in main memory and becomes A’. Further assume that A’ is written back to the storage layer underneath, say the hard disk, overwriting (and replacing) the old version A. Then we call this a direct write. In other words, the new version of the block is written over the old version of that block. What is an indirect write? indirect write Assume a block A is modiﬁed in main memory and becomes A’. Further assume that A’ is written back to the storage layer underneath, say the hard disk, but not overwriting (or physically replacing) the old version A. In contrast, the old version of the block A is kept. The new version A’ is written to a diﬀerent place. Then we call this an indirect write. In other words, the new version of the block is not written over the old version of that block, but rather kept in a diﬀerent place. What are their pros? In direct write, if anything goes wrong while writing back the new version A’ (e.g. a hard disk problem), you may end up in an inconsistent state, e.g. the ﬁrst half of the block represents the new version A’, the second half the old version A. In other words, you do not have a fully consistent version of that block anymore. In direct write this may not happen, as if anything goes wrong while writing the new version A’, you still have the consistent version A. What are their cons? Indirect write introduces a level of indirection. This typically implies fragmentation which deteriorates the sequential layout of data on the storage medium. Direct writes do not suﬀer from this problem. The storage layout is not aﬀected by direct writes. Quizzes 1. Direct write means that (a) we apply updates directly on disk, bypassing the database buﬀer. (b) we apply updates individually and ﬂush the dirty pages to disk immediately. (c) we simply overwrite the old block on hard-disk if the page is dirty. (d) we use special hardware components to accomplish the update operations, thus the name update in-place. 2. Direct write (a) alone cannot ensure a consistent state of the database. (b) can ensure a consistent state of the database if we keep old blocks. (c) overwrites log records for the given page, i.e. updates them in-place. (d) ensures a consistent state of the database. 3. Direct write without logging violates the following ACID properties: 66 Hardware and Storage (a) durability, since a power failure could result in dirty pages not being completely written to disk. (b) isolation, since the updates of diﬀerent transactions cannot be identiﬁed any- more on the blocks that have been overwritten. (c) consistency, since the blocks are overwritten on disk before the transaction commits. (d) atomicity, since a power failure could result in dirty pages not being completely written to disk. (e) atomicity, since a power failure could result in some dirty pages completely written to disk, but other dirty pages not written to the disk at all. 4. Indirect write (a) creates a copy of the old block before applying each update to the page. (b) keeps a copy of the old block until the transaction has committed. (c) keeps a backup copy of all blocks as a hot stand-by in case the DBMS crashes. 5. Deferred updates means that (a) updates are collected and applied to a page at once. (b) updates are not applied immediately on the consistent version visible to other transactions, but only when the transaction has committed. (c) updates are only visible to other transactions if the transaction has committed. Exercise Assume a DB-buﬀer having slots for 4 pages only (sic! to allow you to draw the solution more easily). The DB buﬀer implements LRU (least recently used), i.e. the page that was referenced the longest time ago among all entries will be evicted. Pages numbered from 0,. .., N . The database currently runs two types of queries (“type” means: queries of the same type trigger the same page reference sequences): Q1: references pages 0, 1, and 2 Q2: references pages 4, 5, 6, and 7 Notice that each query references the pages in exactly the order speciﬁed. This also means that the DB-buﬀer does not see all page references done by a query at the same time at the beginning of the query (in fact the concept of a query is not understood by the DB-buﬀer), i.e. the DB-buﬀer sees individual page requests one by one and has to make sure that the page requested is or becomes available in main memory. Whenever a query accesses a page that is not available in the DB-buﬀer, we count this as one cost unit, otherwise we assume no costs for accessing that page. (a) Let’s assume we start with an empty DB-buﬀer. Q1 and then Q2 is executed. What is the state of the DB-buﬀer after these two queries? Notice that the state of the DB-buﬀer is the set of pages currently kept in the DB-buﬀer. What were the costs? 1.3 Fundamentals of Reading and Writing in a Storage Hierarchy 67 (b) Let’s assume we start with an empty DB-buﬀer. Assume that Q1 is executed fre- quently in this system whereas Q2 is executed rarely. For instance, Q1-type queries are executed x times in a row. Then a Q2-type query is executed exactly once. This pattern is then repeated N times. What is the state of the DB-buﬀer after these (x + 1) · N queries? What were the costs? (c) What other methods (at least two algorithmically diﬀerent methods, possibly having the same eﬀect on the pages evicted from the DB-buﬀer) could you think of to lower the costs computed in (b)? What is the state of the DB-buﬀer after these (x + 1) · N queries? What were the costs? 1.3.4 Twin Block, Fragmentation Material Video: Original Slides: Inverted Slides: Learning Goals and Content Summary What is the core idea of twin block? twin block The core idea of twin block is to keep two versions of each block. Like that for each block we have an a-version and a b-version. One of these versions is considered consistent and read-only, the other version is considered possibly inconsistent and may be modiﬁed by an ongoing transaction. The roles of the a and b versions change over time. A global switch indicates which of the two versions is currently considered the consistent version. What are its pros? We do not need extra helper data structures, undo of changes (e.g. aborting a transaction) is easy, and there is no fragmentation introduced by the method. What are its cons? The storage requirements are doubled. Quizzes 1. In case of twin block (a) each block is stored twice physically on disk. (b) each block has two backup copies. (c) backup copies of blocks are only created for dirty pages. (d) backup copies are only created for blocks read into the database buﬀer. 2. In case of twin block without concurrent transactions (a) we direct all read requests to the consistent version of the block. (b) we direct all read requests to the new version of the block. 68 Hardware and Storage Modify B3 T1 B0a B0b B1a B1b‘ B2a B2b B3a B3b B4a B4b B1b‘ B3b‘ pointer to consistent version a Write Back B3 T1 B0a B0b B1a B1b‘ B2a B2b B3a B3b‘ B4a B4b B1b‘ B3b‘ pointer to consistent version a Figure 1.17: Twin block: modifying version b of block B3 to B3’ and writing it back to storage. (c) we are dealing with an in-place update method. (d) we are dealing with an indirect write method. 3. The method switching between the consistent and (possibly) inconsistent versions of the blocks has to be: (a) in-place (b) atomic (c) asynchronous (d) write-through 1.3 Fundamentals of Reading and Writing in a Storage Hierarchy 69 4. When ﬂushing out dirty pages to disk at any time before committing transactions using twin block: (a) the fragmentation of the disk increases due to the free space created by deleting the twin block. (b) we actually need to write the page twice when ﬂushing it to disk. (c) we are only allowed to overwrite the inconsistent version of the block. (d) after switching versions, all blocks changed by the transaction have to be copied to the inconsistent version. 5. The beneﬁts of twin block are: (a) undoing changes is easy (b) storage requirements increase only moderately (c) needs only an A-B version toggle per page (d) does not require complex helper data structures 1.3.5 Shadow Storage Material Video: Original Slides: Inverted Slides: Learning Goals and Content Summary What is the core idea of shadow storage? shadow storage We only keep two versions of those blocks currently being modiﬁed, i.e. those blocks that are part of an uncommitted transaction. Indirection from logical blocks to physical blocks is organized using two mapping tables. One of these mapping tables is considered consistent and read-only, the other version is considered possibly inconsistent and may be modiﬁed by an ongoing transaction. The roles of the two mapping tables may change over time. A global switch indicates which of the two mapping tables is currently considered the consistent version. Shadow storage is an example of an indirect write method. What are its pros? We do not double the storage overhead (as in twin block), but rather only double the storage requirements for the modiﬁed blocks. Undoing changes is easy. What are its cons? The indirection introduced by the mapping table may lead to fragmentation. The helper data structures may become big. Where is this used outside databases? It is used in ﬁle systems like ZFS. In addition, virtual memory is basically an implemen- tation of shadow storage. See also Section 1.4. 70 Hardware and Storage 0 1 2 3 4 5 0 1 2 3 4 5 B0 B1 B2 B3 B4 B5 B6 B7 B8 B9 Insert and Update a version a file version b Ma Mb pointer to consistent version 0 1 2 3 4 5 0 1 2 3 4 5 B0 B1 B2 B3 B4 B5 B6 B7 B8 B9 Crash a version a file version b Ma Mb pointer to consistent version Figure 1.18: Shadow paging: inserting and updating data vs handling a crash Quizzes 1. In shadow storage we keep (a) two versions of each modiﬁed block (b) two versions of each block (c) two versions of each block in the database buﬀer (d) two versions of each block that is ﬁxed in the database buﬀer 2. The mapping table in shadow storage is used for: (a) translating array indexes to logical block addresses. (b) translating the dirty page numbers to physical block addresses. (c) translating logical block numbers to physical block addresses. (d) translating block numbers of inconsistent blocks to the physical address of the consistent copy of the given block. 3. In case of shadow storage 1.3 Fundamentals of Reading and Writing in a Storage Hierarchy 71 (a) in case the transaction crashes, the new transactions will read the consistent version of the block. (b) in case the transaction crashes, the new transactions will read the newest version of the block. (c) we are dealing with an in-place update method. (d) we are dealing with an indirect write method. 4. To abort a running transaction and undo its updates: (a) we toggle the version pointer, so that it points to the consistent version of the mapping table. (b) we just have to use the mapping table pointed to by the version pointer, and copy its contents over the other mapping table (c) we need to erase the inconsistent versions of the blocks. (d) we need to overwrite the inconsistent versions of the blocks with their corre- sponding consistent version. 5. Assume, the currently consistent version is B, the inconsistent version containing some changes is A. The proper order of actions to persist the changes of a transaction that did changes to A are: (a) ﬂush the dirty pages, ﬂush Ma,toggle the global version pointer,copy the contents from Ma to Mb. (b) ﬂush the dirty pages, ﬂush Ma,toggle the global version pointer,copy the contents from Mb to Ma. (c) ﬂush Ma,toggle the global version pointer,copy the contents from Ma to Mb, ﬂush the dirty pages. (d) ﬂush the dirty pages, toggle the global version pointer, copy the contents from Ma to Mb,ﬂush Ma. 6. The beneﬁts of shadow storage are: (a) undoing changes is easy (b) storage requirements increase only moderately (c) needs only a version toggle per page (d) does not require complex helper data structures compared to twin block 1.3.6 The Copy On Write Pattern (COW) Material Video: Original Slides: Inverted Slides: 72 Hardware and Storage Learning Goals and Content Summary When is the Copy On Write Pattern (COW) applicable? Copy On Write Pattern COW Whenever there is computer memory that may be partitioned into units, we may apply COW. What is the core idea? Assume you have two users who both operate on large portions of data (whether that data is in main memory or on disk does not matter). Assume that data is partitioned into units (e.g. pages or anything else). Let S1 be the set of pages from user 1, S2 respectively. Let dup = S1\\S2 contain the pages that are byte-equivalent across S1 and S2. Then, it does not make sense to store the pages that are contained in dup twice. It is more eﬃcient to only store those pages once and make both users share those pages. However, what happens if any of the two users, say user 1, wants to modify one of the pages in dup? In that case, user 2 would also see the changes. This is typically not what we want. Therefore in exactly this situation COW kicks in: if user 1 modiﬁes a page in dup that page is removed from dup and duplicated. Now, each user, i.e. sets S1 and S2, has a private version of that page and can modify that page. Where is it applied? In all kinds of places. It is used as a method for indirect writes in databases. It is also used in virtual memory management: if a process spawns a child process their memory is organized by COW. Quizzes 1. Using the copy-on-write pattern the same logical block addresses (a) always translate to diﬀerent physical addresses. (b) might translate to the same physical address. (c) might translate to diﬀerent physical addresses. (d) always translate to the same physical address. 2. Using the copy-on-write pattern we can (a) share physical blocks among database transactions (b) share physical blocks among operating system processes (c) hide logical addresses (d) have fewer physical blocks allocated than logical blocks 3. Given multiple transactions operating on a set of logical blocks. Using the copy-on- write pattern we keep (a) asingle physicalcopy of each logical block (b) one physical copy of each logical block per transaction 1.3 Fundamentals of Reading and Writing in a Storage Hierarchy 73 (c) we create a new physical copy of a logical block for each transaction modifying that block 4. The beneﬁts of using the copy-on-write pattern are: (a) storage requirements are considerably increased over twin blocks (b) isolation of transactions (c) does not require complex helper data structures (d) reduces fragmentation of data 1.3.7 The Merge on Write Pattern (MOW) Material Video: Original Slides: Inverted Slides: Learning Goals and Content Summary When is Merge on Write Pattern (MOW) applicable? Merge on Write Pattern MOWIn similar situations as COW. What is the core idea? Assume you have two users who both operate on large portions of data (whether that data is in main memory or on disk does not matter). Assume that data is partitioned into units (e.g. pages or anything else). Let S1 be the set of pages from user 1, S2 respectively. Let dup = S1\\S2 contain the pages that are byte-equivalent across S1 and S2. We already considered this use-case for COW, see Section 1.3.6. However, what happens if any of the two users, say user 1, wants to modify one of the pages in S1 \\ dup such that after that change the contents of that page are byte- equivalent to a page in S2 \\ dup? This is exactly this situation where MOW kicks in: if user 1 modiﬁes a page in S1 \\ dup,that page is moved from S2 \\ dup to dup. Now, the two users, share that page. Where is it applied? In data deduplication. What is the relationship to COW? COW Again, MOW is the inverse operation to COW. Quizzes 1. The merge-on-write pattern is (a) the inverse of the copy-on-write pattern. (b) the anti-pattern of the copy-on-write pattern. 74 Hardware and Storage 2. Given multiple transactions operating on a set of logical blocks. Using the merge- on-write pattern (a) we keep a single physical copy of each logical block (b) we keep a single physical copy of all logical blocks with the same contents (c) we create a new physical copy of a logical block for each transaction modifying that block (d) we might free the memory used by a physical copy of a block after updating that block 3. Using the merge-on-write pattern diﬀerent logical block addresses (a) always translate to diﬀerent physical addresses. (b) might translate to the same physical address. (c) might translate to diﬀerent physical addresses. (d) always translate to the same physical address. 4. Using the merge-on-write pattern two diﬀerent logical addresses can point to the same physical address after: (a) performing speciﬁc updates to a block (b) calling the ﬁx method of the database buﬀer (c) copy-on-write 5. The beneﬁts of using the merge-on-write pattern are: (a) storage requirements are considerably increased over twin blocks (b) it can reduce storage requirements (c) it does not require complex helper data structures (d) it reduces fragmentation of data 6. The merge-on-write pattern (a) is related to compression (b) cannot be used together with the copy-on-write pattern (c) cannot be applied to shadow storage (d) is applied in twin-blocks Exercise Assume pages of size 4 Bytes (sic! to allow you to draw the solution more easily) and a sequence of write operations belonging to transactions executed as shown below. Initially, all bits in all pages are set to 0. We assume shadow storage implementing both copy-on- write and merge-on-write. At all times only one transaction is running (no concurrency). Each transaction writes a 4 Byte integer to a page (in other words: the entire page is overwritten). 1.3 Fundamentals of Reading and Writing in a Storage Hierarchy 75 1. begin 2. P2 w(1) 3. P1 w(10) 4. P7 w(12) 5. commit 6. begin 7. P7 w(7) 8. P2 w(8) 9. P0 w(5) 10. commit 11. begin 12. P2 w(10) 13. P6 w(7) 14. abort 15. begin 16. P2 w(10) 17. P6 w(7) 18. P8 w(7) 19. commit (a) Show the mappings stored in the page table(s) after performing each of the above operations. (b) What is the maximum number of physical pages allocated at any given time? (c) Assume a method Twin Block++ which works as follows: it keeps 2 ⇤ n blocks and two separate bit lists of n bits each. If bit i is set in a bit list, that means version A is considered the consistent version for this block, otherwise B is the consistent version for this block. We keep a global bit to signal which of those bit lists is considered to reﬂect the consistent state. If a new transaction comes in, the inconsistent bit lists (initially a copy of the con- sistent bit list) is modiﬁed as follows: for each block i that needs to be changed, initially the globally consistent version is copied over the inconsistent version, bit i is ﬂipped, and then changes are performed on the inconsistent version. If the trans- action commits, all changed blocks are ﬂushed to disk, the global pointer is ﬂipped, ... Answer (a) and (b) for this method. 76 Hardware and Storage (d) What are pros and cons of Twin Block++ vs. Twin Block and Shadow Storage? 1.3.8 Diﬀerential Files, Merging Diﬀerential Files Material Video: Original Slides: Inverted Slides: Learning Goals and Content Summary Publishing a Book page 23: “datbase“ → “database“ page 345: “idex“ → “index“ page 77: “idex“ → “index“ changes:1st edition Databases by Jens Dittrich Create a 2nd Edition 2nd edition Databases 2nd edition by Jens Dittrich changes: Figure 1.19: Publishing a 1st edition of a book and collecting changes vs eventually creating a second edition What is the main analogy from real life for diﬀerential ﬁles?differential ﬁles The main analogy of diﬀerential ﬁles is publishing books. Publishing houses publish a 1.3 Fundamentals of Reading and Writing in a Storage Hierarchy 77 Create a 2nd Edition page 75: “kamera“ → “camera“ page 143: “big date“ → “big data“ new chapter on “tools“ 2nd edition Databases 2nd edition by Jens Dittrich changes: Create a 3rd Edition 3rd edition Da ta ba ses NEW: 3rd edition! by Jens Dittrich changes: Figure 1.20: Collecting changes over the second edition vs eventually creating a 3rd edition 1st edition of a book. Then they collect errors, typos, and other suggestions to improve the book in a list of changes. Eventually, they merge the 1st edition of the book and everything they collected to create a second edition. This analogy is implemented in diﬀerential ﬁles where editions are coined ﬁles and the list of changes is coined diﬀ ﬁle. To which other patterns does this relate? pattern It relates to The Batch Pattern (Section 1.2.5) and The All Levels are Equal Pattern (Section 1.1.1). What are the pros and cons of diﬀerential ﬁles? In the diﬀerential ﬁles method, write operations are relatively cheap. In addition, the diﬀ ﬁle corresponds to an incremental backup. The “editions” (or ﬁles) may be considered 78 Hardware and Storage RW DiffFile 0 RW File v0 frame 1 time frame 0 frame 2 frame 3 Example: Merging DiffFiles RW DiffFile 0 RW File v0 frame 1 time frame 0 frame 2 frame 3 RW File v1 RW DiffFile 1 Example: Merging DiffFiles Figure 1.21: Diﬀerent phases in the merge process: the situation before starting the merge (a read-only ﬁle v0 and its DiﬀFile 0) vs the situation during the merge (the DiﬀFile 0 is set to read-only as well. Both the read-only ﬁle v0 and the DiﬀFile 0 are merged into a new ﬁle v1. Concurrently we collect changes in a new DiﬀFile 1.) snapshots, whereas the entries in the diﬀ ﬁle provide you with an incremental backup which allows you to move the state of the database forward from the most recent snapshot (the ﬁle). Moreover, if you perform merges regularly to create new read-only ﬁles from old read-only ﬁles and growing diﬀ ﬁles, this is an opportunity to defragment the storage layout. How to merge diﬀerential ﬁles with the read-only DB without halting the database?read-only DB This merge operation can be performed without halting incoming read and write opera- tions. To merge an existing ﬁle v0 with diﬀ ﬁle 0, simply switch diﬀ ﬁle 0 to read-only and start a new writable diﬀ ﬁle 1. All write operations are now directed to diﬀ ﬁle 1. 1.3 Fundamentals of Reading and Writing in a Storage Hierarchy 79 All read operations must consider ﬁle v0, diﬀ ﬁle 0, and diﬀ ﬁle 1 to obtain the most recent state of the database. Now, you merge ﬁle v0 with diﬀ ﬁle 0 to form a new ﬁle v1. This can be done in a background thread. Once that merge operation is ﬁnished, all read operations may be redirected to only consider ﬁle v1 and diﬀ ﬁle 1. Then diﬀ ﬁle v0 and diﬀ ﬁle 0 may be deleted. Now, we are back at the initial situation with one ﬁle and one diﬀ ﬁle. Eventually, once diﬀ ﬁle 1 has grown, we may repeat the entire process. Quizzes 1. Using Diﬀerential ﬁles, we (a) create a new ﬁle by merging the original ﬁle and another ﬁle, storing the found diﬀerences in a separate ﬁle. (b) collect the changes in a separate ﬁle, and eventually merge with the original ﬁle. (c) merge two ﬁles together, if their contents diﬀer. (d) collect the diﬀerences of two ﬁles and create a merged ﬁle not containing these diﬀerences, i.e. the complement of the symmetric diﬀerence. 2. For diﬀerential ﬁles, the following pattern applies: (a) all levels are equal pattern (b) write-back pattern (c) batch pattern (d) data redundancy pattern (e) copy on write pattern (f) merge on write pattern 3. The diﬀerential ﬁle is (a) at the same time the incremental backup ﬁle as well. (b) at the same time the snapshot backup ﬁle as well. (c) the buﬀer for the updates. (d) the buﬀer for the most recently read pages. 4. Merging the diﬀerential ﬁle (a) corresponds to creating a snapshot of the data. (b) increases the fragmentation of the data. (c) can only be applied on the ﬁle level. 5. To retrieve data from a table organized with diﬀerential ﬁles: (a) we have to read one read-only ﬁle only. (b) we have to read one read-only ﬁle and one read-write ﬁle. 80 Hardware and Storage (c) we have to read one read-write ﬁle only. (d) we have to read two read-only ﬁles and one read-write ﬁle as well. (e) we have to read up to two read-only ﬁles and one read-write ﬁle as well. 6. In the Diﬀerential File method a diﬀerential ﬁle can be (a) read-only (b) read-write 1.3.9 Logged Writes, Diﬀerential Files vs Logging Material Video: Original Slides: Inverted Slides: Learning Goals and Content Summary What is the diﬀerence of logging and diﬀerential ﬁles?logging differential ﬁles The major diﬀerence is that in logging the ﬁle is not read-only and kept up-to-date at all times. The log is a redundant mechanism to trace all changes applied to the ﬁle. What are its pros and cons? Similarly to diﬀerential ﬁles, logging may be applied at any storage granule: entire databases, ﬁles, indexes, tables, blocks (in particular for media that has faster reads than writes like ﬂash memory), etc. Moreover, this method can also be applied on diﬀerent layers of the storage hierarchy, not only in-between disks and main memory. In contrast to diﬀerential ﬁles, in logging read-operations are relatively cheap as at all times only one structure has to be considered: the ﬁle. And just like in diﬀerential ﬁles, the log corre- sponds to an incremental backup that can be archived or shipped, e.g. in a distributed system to synchronize the state across multiple databases. If the log is never pruned, the log contains all changes ever send to the database, i.e. “the log is the database”. In that case we could regard the ﬁle (or the database if that is the granule used) as a compressed version of the log. We will look in more detail at logging in Section 6.1.2. Is it possible to combine logging and diﬀerential ﬁles? Absolutely, one use-case is to use logging where the read-write ﬁle is (internally) imple- mented using diﬀerential ﬁles. Quizzes 1. Logging is a technique where we can apply the: (a) all levels are equal pattern (b) write-back pattern (c) batch pattern (d) data redundancy pattern 1.3 Fundamentals of Reading and Writing in a Storage Hierarchy 81 Publishing a Book page 23: “datbase“ → “database“ page 345: “idex“ → “index“ changes:1.2nd edition Databases by Jens Dittrich 1.2nd edition 3rd edition Da ta ba ses NEW: 3rd edition! by Jens Dittrich changes: page 23: “datbase“ → “database“ page 345: “idex“ → “index“ page 77: “idex“ → “index“ page 75: “kamera“ → “camera“ page 143: “big date“ → “big data“ new chapter on “tools“ Publishing a Book Figure 1.22: Publishing a book using logged writes: changes are not merely collected but every change is also applied to create a new (sub-)edition. (e) copy on write pattern (f) merge on write pattern 2. To obtain all data belonging to a table: (a) it is suﬃcient to read the log ﬁle only. (b) it is suﬃcient to read the database only. (c) we have to read the database and the log as well. (d) we ﬁrst have to merge the log into the database. 3. To obtain all data from a table it is usually more eﬃcient: 82 Hardware and Storage (a) to read the log ﬁle. (b) to read the database. (c) to read the database merged with the tail of the log. 4. In case of logging we write sequentially to the: (a) log (b) database 5. In case of logging we route updates to the: (a) log (b) database 6. The log is: (a) asnapshot of thedatabase (b) an incremental backup 7. The beneﬁts of logging over diﬀerential ﬁles are: (a) smaller storage space requirements (b) no random I/O (c) the possibility of restoring the database to any point of time 8. When combining logging and diﬀerential ﬁles: (a) we collect the updates in the log and in the diﬀerential ﬁle(s) as well. (b) we collect the updates in the diﬀerential ﬁle(s), and merge them with the log, before applying them to the database. (c) we direct the results of the merging to the log. (d) we get a non-functioning system. 9. The log ﬁle can be (a) read-only (b) read and allow for writing data at any position (c) read and allow for appending data Exercise Consider a read-only database using diﬀerential ﬁles to handle updates to tables on a granularity of records, i.e. if a record changes, the new version of the record is stored in a diﬀerential ﬁle. You are given a dataset of 1 TB already loaded in the read-only DB. Notice: 1 KB = 1024 Bytes. In addition, 1.3 Fundamentals of Reading and Writing in a Storage Hierarchy 83 • All I/O-operations are sequential (no seek time, no rotational delay, ignores random I/O, yet simpliﬁes your calculation). • I/O-operations can be done at a speed of 500 MB/s. • Capacity of the diﬀerential ﬁle DF0 is 64 MB. • Data access is sequential in diﬀerential ﬁles. • Record size is 128 Byte. • Only updates to records in the existing RO-DB happen! Updates are uniformly distributed over the records in the (initial) RO-database, Neither inserts of new records nor updates to records existing already in some diﬀerential ﬁle will happen (however, this may not be exploited in the calculations of the merge strategies and also the algorithm must not rely on this property, in particular Strategy 2). Now consider the following merge strategies: Strategy 1: When the diﬀerential ﬁle DF0 is full, merge the read-only DB and the diﬀerential ﬁle DF0 immediately and start a new diﬀerential ﬁle DF 0 0. Strategy 2: Keep a sequence of diﬀerential ﬁles DF0,. .., DFN where the capacity of DFi+1 is twice the capacity of DFi.Whenever the capacity of a diﬀerential ﬁle DFi is reached, it is merged into DFi+1. If DFi+1 is empty, the storage space used by DFi is simply reassigned to DFi+1 without performing an actual merge. For a suitable N> 0, DFN can be considered the read-only DB. Assume 960 MB of updated database records have been inserted. For both strategies (1)&(2) determine the following: (a) number of merges for each diﬀerential ﬁle, (b) size of the diﬀerential ﬁle(s), (c) size of the read-only DB, (d) total merge time, (e) best, worst, and the average query time. 1.3.10 The No Bits Left Behind Pattern Material Video: Original Slides: Inverted Slides: 84 Hardware and Storage main memory flash/hard disk main memory Example with Wasted Bits 12 42 4212 main memory flash/hard disk main memory Example with Better Data Layout 4212 12 Figure 1.23: Wasting bits by distributing hot data over diﬀerent pages vs keeping hot data on the same page Learning Goals and Content Summary Why shouldn’t we leave any bits behind? Or in other words: why should we avoid wasting bits?wasting bits The main goal here is to avoid loading cold data into any layer of the storage hierarchy. Cold data refers to data that is actually not required to perform a particular operation. Hot data refers to the data requested by an ongoing operation. Typically, this loading of cold data happens when hot and cold data sits on the same storage granule, e.g. a page, and for whatever reason both the hot and the cold data is loaded both to a higher layers of the storage hierarchy. For instance, as data from disks can only be loaded in the granule of pages, and if that page contains cold and hot data at the same time, we 1.3 Fundamentals of Reading and Writing in a Storage Hierarchy 85 still have to load the entire page into the database buﬀer. This is wasting storage space in the DB-buﬀer. What does this mean for data layouts? data layouts We should (try to) layout data in a way such that we avoid loading cold data. Obviously, this cannot always be achieved easily as it also highly depends on the workload. Why would I cluster data with spatial locality on the same virtual memory page, disk page, spatial locality virtual memory page disk page disk sector, cache line? disk sector cache line To ﬁx the problem of wasting storage for cold data explained above: addresses that are referred to within short periods of time should minimize their spatial distance. This implies that they should also minimize the number of boundaries across storage granules. For instance, if two rows are referenced very often at the same time, however those rows reside on diﬀerent pages, it is worth considering to place them on the same page. Like that for these two rows their is no boundary w.r.t. pages anymore. Still within that page, the two rows may still not reside on the same disk sector (recall: a page is typically amultipleof adisksector). So, eventuallyyou maydecreasethedistanceof thetwo rows even further by placing them on the same disk sector. Still, the two rows may be loaded into diﬀerent cache lines. So again, eventually you may further decrease their distance, i.e. by placing them within a cache line granule. So, basically, the storage layout may be adjusted dynamically to follow the data references of your program (or database management system). See also Section 1.3.1. Why would I keep data with little spatial locality on diﬀerent virtual memory pages, disk pages, disk sectors, cache lines? If data has little spatial locality, there is no use in keeping that data on the same storage granule. Quizzes 1. How can you avoid to load data into main memory containing at the same time data items that are heavily used by the CPU and data items that are only infrequently used by the CPU? (a) Rearrange data items on disk such that frequently used data items are clustered on disk. (b) Rearrange data items on disk such that frequently and infrequently used data items are uniformly distributed on disk. (c) Compress data as much as possible. 2. How can you make better use of the available bandwidth between memory and caches (or disk and memory) ignoring CPU costs? (a) Compress data as much as you can. (b) Only read the bytes you are interested in from a cache line (or memory page). 3. When accessing a single byte in main memory, ... 86 Hardware and Storage (a) the whole cacheline containing the byte is brought into the caches. (b) only the surrounding word (64 bits) is loaded from main memory. 4. Assume we have a database with a buﬀer manager and data is in row layout. Ac- cessing a single tuple of a relation ... (a) will bring the whole page containing that tuple from disk into the database buﬀer. (b) will only load the needed tuple from disk. 1.4 Virtual Memory 1.4.1 Virtual Memory Management, Page Table, Preﬁx Address- ing Material Video: Original Slides: Inverted Slides: Additional Material Literature: [PH12], Section 5.4 Learning Goals and Content Summary How are virtual memory addresses translated to physical addresses?virtual memory Virtual memory addresses are translated to physical addresses by a combination of soft- ware and hardware components. The entire process is also called address virtualization. address virtualization The central software components are the page table which is maintained in main-memory just as any other data. However, some of its entries are cached in a special hardware cache coined translation lookaside buﬀer (TLB), see also Section 1.4.2. Address trans- lation is supported by hardware through a memory management unit (MMU). Similar memory mapping problems and techniques are used in the storage layer of a DBMS when implementing rowIDs. As a rowID must identify the location of a particular data item, it makes sense to not store actual physical oﬀsets to memory but rather virtualize those addresses through a page table similar to the one used in virtual memory management. How does virtual memory address translation work? A virtual memory address is translated into a physical memory address as follows: a virtual address of k bits is split into two parts: (1) a preﬁx having l bits and (2) a suﬃx having k \u0000 l bits. The preﬁx is interpreted as a virtual pageID. The suﬃx is interpreted as the oﬀset inside the physical page pointed to by that virtual page. Therefore, in order to translate a given virtual address, we need to replace the preﬁx by the preﬁx of the 1.4 Virtual Memory 87 Virtual Memory Management #01000011 virtual memory addresses physical memory addresses #1101001 Preﬁx Addressing (Multi-level Tree) 0 1 2 ... 15 0 1 2 ... 15 0 1 2 ... 15 0 1 2 ... 15 0 1 2 ... 15 0 1 2 ... 15 0 1 2 ... 15 ... 2 3 0 1 152 1000010111... 1011110001... 0 1 Figure 1.24: Virtual memory management maps virtual address to physical address. This mapping is often implemented using a (multi-level) preﬁx tree. physical page ID actually pointed to. This yields a new physical address which has the same suﬃx like the virtual address, but a new preﬁx. What is the role of the page table? page table The page table maps virtual preﬁxes to physical preﬁxes. It may be organized in diﬀerent ways and is kept in DRAM, however some of the data is cached in TLB. What is preﬁx addressing? preﬁx addressing In preﬁx addressing the preﬁx of the virtual memory address is interpreted as the path into a radix tree. For instance, in a multi-level tree (see Figure 1.24), if the preﬁx has l = 10 bits, we may further divide this preﬁx into 2+4+4 bits. We start at the root 88 Hardware and Storage node which has at most 2 2 =4 entries. The ﬁrst 2 bits (yellow numbers) of the suﬃx determine the child node. In that child, which has at most 24 = 16 entries, the next 4 bits (blue numbers) determine the address of the next child to inspect. Again in that child, which has at most 2 4 = 16 entries, the last 4 bits of the suﬃx (green numbers) contain the actual physical address. That physical address is the value stored at that particular oﬀset within this node. What exactly is an oﬀset?offset An oﬀset deﬁnes the distance from a starting address. In the context of this video, preﬁx of a certain length deﬁne such a starting address, the remaining suﬃx then deﬁnes the oﬀset to that starting address. For instance, if you consider the ﬁrst two bits to deﬁne a starting address, the preﬁx tree translates this to the starting address of one of the four segments. The remaining 16 bits deﬁne the oﬀset from that starting address within that segment. Recursively, if you address a particular page within a segment using a 6 bit preﬁx, that 6 bit-suﬃx lead you to the starting address of a particular page within that segment (again through the preﬁx tree). Any other address within that page (the oﬀset) can be addressed by the remaining 12 bit suﬃx. Quizzes 1. Why do operating systems use virtual memory? (a) To separate the address spaces of diﬀerent processes. (b) To allow for compiled code to contain ﬁxed addresses, even though the physical location in memory is only known after the process was started. (c) To allow for a larger address space than the actually available main memory. 2. How are virtual memory addresses translated to physical addresses, if using a single level of translation, 32 bit addresses, and 4KB pages? (a) The ﬁrst 20 bits are used to ﬁnd the page table entry, that contains the physical page number and that physical page number is appended by the 12 bit suﬃx of the virtual address. (b) The ﬁrst 20 bits are used to ﬁnd the page table entry, that contains an arbitrary physical address (i.e. an address not necessarily aligned to the page size) and the 20 bit suﬃx has to be added to the physical address. (c) The ﬁrst 12 bits are used to ﬁnd the page table entry, that contains the physical page number and that physical page number is appended by the 12 bit suﬃx of the virtual address. (d) The ﬁrst 12 bits are used to ﬁnd the page table entry, that contains an arbitrary physical address and the 20 bit suﬃx has to be added to the physical address. 3. Given 8-bit virtual addresses and a page size of 64 bytes. How many virtual pages are there per virtual address space? (a) 1.4 Virtual Memory 89 4. How do modern CPUs support virtual memory? (a) They provide memory management units that perform the translation in hard- ware. (b) They provide a special cache, the TLB, to speed up address translation of addresses frequently accessed virtual pages. (c) They provide a special co-processor to speed up address translation of addresses in the same virtual page. (d) They provide a special cache, the TLB, to speed up address translation of addresses frequently accessed physical pages. 5. How many bits from the 64-bit virtual addresses are actually used by current x86(64) CPUs to translate address preﬁxes from virtual pages to physical pages? Notice that only 48-bits of the 64-bit address space is used in any case, i.e. how many of those 48 bits are used for the page translation preﬁx for a given memory page size? (a) 12 for 4KB memory pages (b) 36 for 4KB memory pages (c) 27 for 2MB memory pages (d) 18 for 1GB memory pages 1.4.2 Retrieving Memory Addresses, TLB Material Video: Original Slides: Inverted Slides: Learning Goals and Content Summary What happens when referencing a speciﬁc virtual memory address? virtual memory The TLB is inspected whether it has a cached version of that address. If that is not the case, the page table has to be searched for that address. What kind of translation lookaside buﬀer (TLB) misses and cache misses may occur? translation lookaside buffer TLB cache miss If a particular address is not available in the TLB, looking up the page table may lead to cache misses (at multiple levels of the storage hierarchy). Under the assumption that the page table is entirely available in main memory, an address lookup may therefore require the time it takes to randomly fetch data from main memory. Only after that, and using the physical address just retrieved, the actual data is retrieved. This may again lead to cache misses. Bottom line: in a storage hierarchy using virtual memory address translation, we do not only observe cache misses due to fetching data, but additionally due to fetching addresses. 90 Hardware and Storage Task: Read Page 32 core Registers L1 L3 main memory L2 page 56 virtual pageID physical pageID 42 11 32 56 77 23 33 34 virtual pageID physical pageID 77 23 33 34 .. .. TLB Figure 1.25: Translation lookaside buﬀer (TLB) and TLB-misses Quizzes 1. What is the translation look-aside buﬀer used for? (a) It stores mappings from virtual page addresses to physical page addresses. (b) It stores mappings from physical page addresses to virtual page addresses. 2. How many last level cache misses can occur when reading a single value from mem- ory if three-level address translation is used? Notice: if an architecture has L1, L2, and L3, then L3 is called the last level cache (LLC). (a) 3. How can the pressure on the TLB be reduced? (a) By using larger pages. (b) By disabling address translation (assuming this is possible). (c) By addressing a given page at most once. Exercise A current 64-bit UNIX operating system uses only the lowest 47 bits for memory addressing, the other 17 bits are ﬁlled with 1s. Let’s assume your OS uses 64 KB mem- ory pages and uses shadow-storage (without merge-on-write) as the strategy for indirect writes. Assume an empty page table initially, and that write operations to unmapped virtual addresses allocate a new physical page (numbered by P1,..., PN ). Recall that a read- operation to a page that was never written to before will be redirected to a system-wide, 1.4 Virtual Memory 91 globally visible read-only page ﬁlled with zeros. Only if the ﬁrst write operation happens to a particular physical page, a page fault happens and a writable page is alocated and an entry inserted into the page table. Also notice that whenever a process forks a child process that child process receives a copy of its parent’s page table. Consider the following sequence of operations to the speciﬁed virtual memory ad- dresses executed by processes 1 and 2 in that order. Notice that 0xFFFF,FEAD,0001,0000 is a multiple of 0x1,0000 (=64 K base 1024). (1) process 1: write 0xFFFF,FEAD,0001,0004 (2) process 1: read 0xFFFF,FEAD,0002,0100 (3) process 1: write 0xFFFF,FEAD,0002,0004 (4) process 1 forks child: process 2 (5) process 2: write 0xFFFF,FEAD,0001,0002 (6) process 2: write 0xFFFF,FEAD,0001,0004 (7) process 2: write 0xFFFF,FEAD,0007,1234 (8) process 1: write 0xFFFF,FEAD,0001,0008 (9) process 1: write 0xFFFF,FEAD,0002,0142 (10) process 1: read 0xFFFF,FEAD,0007,4321 (11) process 1: write 0xFFFF,FEAD,0007,4321 Your task is to show the mappings stored in the page table(s) after performing each of the above operations. 92 Hardware and Storage Chapter 2 Data Layouts 2.1 Overview Material Video: Original Slides: Inverted Slides: Learning Goals and Content Summary What are the principal mapping steps to map a relation to a device? mapping steps relation device The diﬀerent mapping steps as displayed in Figure 2.1 are linearize(1a) linearize:Two-dimensional relations (sets of tuples) are mapped to a one- dimensional sequence of values, serialize (1b) serialize:a one-dimensional sequence of values is mapped to bytes on virtual pages, devirtualize (2) devirtualize:virtual pages are mapped to physical pages, materialize (3) materialize:physical pages are mapped to storage devices. Which of those steps is related to data layout? data layout Steps 1a and 1b. Why linearize values? Arelation is a setand hence does notdeﬁne an order. Arelation can be considered a two-dimensional address space, i.e. each attribute value can be uniquely addressed by the pair (rowID, attribute name). In contrast, the address space in memory (be it on physical or virtual pages) is a one-dimensional sequence of bytes, it has an order. Therefore we have to force tuples’ attribute values into a particular order. How we deﬁne this order may have a tremendous impact on query performance in the database system. Why serialize values? 94 Data Layouts Mapping Relations to Devices? page ID 32 page ID 77 page ID 56 page ID 23 virtual pages physical pages \t (23, Albert, 45000) \t (42, Rob, 37000) \t (77, Peter, 50000) Employees = 1b 2 3 (23, Albert, 45000) (42, Rob, 37000) (77, Peter, 50000) 1a Mapping Relations to Devices? page ID 32 page ID 77 page ID 56 page ID 23 virtual pages physical pages \t (23, Albert, 45000) \t (42, Rob, 37000) \t (77, Peter, 50000) Employees = 1b 2 3 (23, Albert, 45000) (42, Rob, 37000) (77, Peter, 50000) 1a Figure 2.1: The diﬀerent mapping steps required when mapping two-dimensional relations to storage devices. We have to serialize values (step 1b) in order to convert a higher-level representation, say a sequence of values like integers or strings, into a lower-level representation, say a sequence of bytes, which can then be stored on virtual pages. What is done ﬁrst: linearize or serialize? Whether we ﬁrst perform step 1a (linearize) and then step 1b (serialize) or alternatively perform both steps in a single operation is an implementation decision. Just like step 1a, step 1b may also impact query processing later on depending on the type of serialization we use, see also Section 2.4. 2.2 Page Organizations 95 Quizzes 1. Linearizing values incorporates: (a) assigning an order to tuples without assigning any particular order to the data values (b) sorting the tuples on the key attribute (c) removing duplicates to conﬁrm to the set-semantics of the data structure (d) assigning an order to individual data values 2. The linearization order: (a) has a huge impact on the query performance (b) does not change the result set of a query (c) conﬁrms to the physical order on hard disk (d) conﬁrms to the physical order in RAM 3. Order the following concepts from left (abstract) to right (physical): (a) relations - virtual pages - physical pages - storage blocks (b) relations - tuples - ﬁelds - storage blocks (c) virtual pages - physical pages - tuples - storage blocks 4. The correct order of the mapping steps is the following: (a) linearize - serialize - virtualize - materialize (b) linearize - serialize - devirtualize - materialize (c) virtualize - serialize - linearize - materialize (d) linearize - deserialize - devirtualize - materialize 5. The linearization order allows for: (a) storing all ﬁelds of a tuple contiguously (b) storing all values of a given attribute contiguously (c) storing only some of the attributes of a tuple contiguously (d) assigning an arbitrary order of data values even across tuples 2.2 Page Organizations 2.2.1 Slotted Pages: Basics Material Video: Original Slides: Inverted Slides: 96 Data Layouts Learning Goals and Content Summary Slotted Pages page 56 Header Data (56,3) (56,5) Forward Tuple-IDs page 56 Header Data page 42 Header Data (42,3) Figure 2.2: Slotted pages and Forward Tuple-IDs What is the core idea of a slotted page?slotted page “All problems in computer science can be solved by another level of indirec- tion.” (Butler Lampson). Slotted pages are yet another instance of this quote as the core idea of slotted pages is to introduce an indirection to hide physical addresses from users. Recall, that virtual pages (see Section 1.4) introduce an indirection to hide physical page addresses, slotted pages virtualize addresses inside the page. Therefore, slotted page are just another application 2.2 Page Organizations 97 of address virtualization. address virtualization Slotted pages allow us to address chunks (which may be rows, but don’t have to) inside pages without having to expose physical oﬀsets of chunks outside the page. This is achieved by keeping an array (aka slot array) of physical oﬀsets inside each page. This slot array array plays a similar role as the page table in virtual memory. A (pageID,slot)-pair may be used to identify any chunk kept on a page. Here, the slot simply denotes an index slot into the array. The pageID is the virtual address of the page, it is translated by virtual memory management. The slot is the (virtual) oﬀset within that page, it is translated by by the slot array. A (pageID,slot)-pair can easily be implemented by concatenating pageID and slot to a single value. If the chunks stored in slotted pages are rows, we term that concatenation a rowID. rowID So, in summary, virtual memory virtualizes the preﬁx (pageID) of a memory address. In contrast, the suﬃx (slot) virtualizes the oﬀset inside a page. What is a slot? Aslot is a particularentryin the slot arrayat the beginning of each page pointing toa particular chunk. The chunk pointed to may be a 1. row (if the data is in row layout) or 2. afraction of a row(if a column grouped layout isused or asingle row exceedsthe size of the page) or 3. acolumn (if column layoutisused) or 4. afraction of a column (ifPAX isused or thecolumn exceedsthesize of the page) fraction of a column. What is the relationship of slotted pages to physical data independence? physical data independence The classical notion of physical data independence makes a database schema independent from the storage structures used underneath. This is achieved by introducing an indirec- tion: in terms of the user front-end (SQL) the data and metadata stored in a database system is not coupled to a particular physical organization. Those organizations can (or should) be interchangeable but not tightly coupled to the data. The same indirection technique is used in slotted pages: the indirection of oﬀsets (the WHAT) through slots hides a physical property (the HOW), i.e. the oﬀset of a chunk inside a page. Thus, this is a lightweight form of physical data independence. Is it possible to move data within a slotted page? Like how? Absolutely, just move the chunk to a diﬀerent unoccupied position within that page and update the oﬀset in its slot (ideally in an atomic operation). This does not change the (pageID,slot)-pair of that chunk. What is a forward tuple-ID? forward tuple-ID A forward tuple-ID (a more general and better name would be a forward chunk-ID), is a workaround which allows us to move chunks to another page without changing their 98 Data Layouts (pageID,slot)-pair. For instance, assume the chunks stored on each page are tuples. If we want to move a tuple from, say page 56 to page 42, we place a special forward tuple-ID on page 56 which redirects all requests to that tuple to page 42 (the slot array of page 42 to be precise). What are forward tuple-IDs good for? Again, like using forward tuple-IDs we do not have to change a particular (pageID,slot)- pair.. This may be useful in cases where these pairs are used outside the page in other data structures in the database system, e.g. in indexes. We save the costs for updating all those references. What might be a problem when using forward tuple-IDs? The additional lookup comes at a price: when looking up such a chunk we have to inspect two pages: the old page, page 56 in the example containing the forward tuple-ID, and the page actually containing the data, page 42 in the example. So, the general trade-oﬀ here is: by introducing forward tuple-IDs we save update costs, as we do not have to update all references of that chunk anymore. However, at lookup time, we may need an extra lookup to get to the actual page. When to use forward tuple-IDs or not heavily depends on the workload of the database system. Yet, as a rule of thumb, you should not use more than one indirection through forward tuple-IDs, e.g. a forward tuple-ID pointing to a forward tuple-ID pointing to a chunk. Quizzes 1. Slotted pages: (a) provide an indirection which has a similar eﬀect as logical data independence (b) provide an indirection which has a similar eﬀect as physical data independence (c) allow for marking data as deleted rather than actually deleting it (d) allow for logging changes done to the actual data 2. Slotted pages consist of: (a) header (b) slots (c) data chunks (d) footer 3. In case of slotted pages to access a given tuple we need the following: (a) pageID (b) table name (c) column name (d) slot number 2.2 Page Organizations 99 4. The following components have always a ﬁxed starting position within a slotted page: (a) header (b) the slot-array (c) the individual data chunks (d) footer 5. When moving a tuple inside a slotted page we have to: (a) update the oﬀset to the slot-array in the header (b) update oﬀsets inside the slot-array (c) change the slotID of that page (d) none of these 6. When moving a tuple to another slotted page using a forward reference we have to: (a) update the header in the source page (b) update the slot-array in the source page (c) update the slot-array in the target page (d) update the data chunk in the source page 2.2.2 Slotted Pages: Fixed-size versus Variable-size Components Material Video: Original Slides: Inverted Slides: Additional Material Literature: [RG03], Section 9.6.2 Learning Goals and Content Summary How to organize ﬁxed-size components on a slotted page? ﬁxed-size components slotted pageIn order to store ﬁxed-size components we simply use linear addressing inside that page. Where to store the slot array on a slotted page using ﬁxed-size components? slot array We do not need it anymore. The array got replaced by a function translating the suﬃx of the rowID to an oﬀset inside that page. What is linear addressing? linear addressing Linear addressing means that there is a linear relationship between the suﬃx used as the input to the function and its output. What can we do with variable-sized components? variable-sized components 100 Data Layouts Linear Addressing [Fotos] : {[ ID: int4, camera: int4, label: char(8) ]} 4 + 4 + 8 = 16 Bytes Header Data page 56 012 345 6 Moving Variable-Sized Components Header page 56 0123 456 0 1 2 3 4 5 6 Figure 2.3: Linear Addressing and Variable-Sized Components Basically, for each row, we partition the attributes into two sets: the ﬁxed-size attributes and the variable-sized attributes (Notice that this is an application of both vertical par- titioning, see Section 2.3.3 and PAX, see Section 2.3.4). The ﬁxed-size attributes are stored from bottom to top just like before using linear addressing. In addition, we add an extra artiﬁcial attribute for each row in the ﬁxed-size part with an intra-page oﬀset pointing to the variable part which is stored from top to bottom. Like that the ﬁxed-size part takes over the role of the slot array. However, the slots are only used to address the variable-size fraction of each row. Can we move the ﬁxed-size and variable parts around on a page? The ﬁxed-size parts cannot be moved except if we change the function used for linear 2.2 Page Organizations 101 addressing inside a pages. The variable-size components may be moved just like in the basic slotted pages method. The oﬀset that was previously kept in the slot array is now part of the ﬁxed-size part of a tuple. Quizzes 1. Consider the following table: CREATE TABLE person (id INTEGER4, name CHAR(52), city CHAR(200)), and a slotted page with a size of 4 KB. What is the oﬀset of the second tuple? (a) 3840 (b) 3584 (c) 512 + size of the header (d) 256 + size of the header 2. In case of a 4 KB slotted page how many bits do you need for each oﬀset if you want to address every byte? (a) 12 (b) 32 (c) 16 (d) 64 3. Consider the following table: CREATE TABLE person (id INTEGER4, name CHAR(52), city CHAR(200)), and a slotted page with a size of 4 KB, with a 100 byte header. How many tuples can you store in a single page? (a) 8 (b) 14 (c) 15 (d) 16 4. Which of the following components have a ﬁxed position within a slotted page? (a) header (b) ﬁxed-size components of tuples (c) variable-size components of tuples (d) footer 5. Variable-size components of tuples: (a) are stored together with the ﬁxed-size components (b) are stored where the slot-array would be stored (c) require more storage space than their actual net size (d) require storage space equal to their maximal size 102 Data Layouts Exercise Consider the following table deﬁnition: CREATE TABLE person ( id INTEGER , name VARCHAR (50) , city VARCHAR (100) ); and a slotted page with a page size of 4 KB, with a 128 Byte header. Assume 4 Byte integers, linear addressing, and that variable-sized components are stored in a space- eﬃcient way. Insert the following tuples in the listed order into the same, initially empty page: (1) (41, “Doe\", “Merzig\") (2) (42, “Smith\", “Eppelborn\") (3) (43, “Foobar\", “Saarwellingen\") Show the page after the insertions. What is the total space occupied by the inserted tuples? What is the amount of free space in the page after the insertions? 2.2.3 Finding Free Space Material Video: Original Slides: Inverted Slides: Additional Material Literature: [RG03], Section 9.3.1 Learning Goals and Content Summary What is a segment?segment A segment is a contiguous sequence of physical pages. Therefore, a physical page within a segment can be addressed linearly. It is exactly the same idea as linear addressing of chunks inside a slotted page. The only thing that changes is the size of the granules: We address physical pages inside a segment. Where do we store metadata about free space or any other metadata anyway?free space We need to keep track of how much space is left in our database and where that space is available. So, the general question is: where should we keep that metadata,i.e. datametadata 2.2 Page Organizations 103 Per Segment Table 0 1 2 ... 15 0 1 2 ... 15 0 1 2 ... 15 0 1 2 ... 15 0 1 2 ... 15 0 1 2 ... 15 0 1 2 ... 15 ... 2 3 0 1 152 1000010111... 1011110001... 0 1 segments Figure 2.4: Per segment preﬁx table as a tree What is considered What is considered a “page”? a “chunk”? Slotted Pages page chunk Segments segment physical page Table 2.1: Special-cases of memory indirection and virtualization about the actual data stored in the database? Examples of metadata kept in a database system include: free space information, statistics, indexes, schema information and so forth. Most database systems keep that information in the so-called catalog which is catalog itself a relational database with a vendor-deﬁned schema. However, some metadata is also kept in diﬀerent places for eﬃciency-reasons. This includes data about free space management. That metadata can be stored on a per page or on per segment level. Several database design techniques combine metadata and data on a granule that is smaller than the entire database. In other words, metadata for a page is kept on that page, or: metadata about a segment is stored on that segment. This is again another example for fractal design which we will generalize in Sec- tion 2.3.5. Quizzes 1. Why do we need to manage the free space of the pages available in the database? (a) To make eﬀective use of the available memory. (b) Because we cannot allocate more pages over time. (c) To leave no bit behind. 2. Why not store the exact number of free bytes per page in the free space management 104 Data Layouts table? (a) The required space to store this number might be too high. It is often enough to know an estimation of the available space to decide if a new data item ﬁts into that page. (b) We should store the exact number. Otherwise we don’t know if a new item can ﬁt on the page. 3. Can we immediately write a new data item to a page that has enough available space? (a) Not necessarily. Maybe we need to defragment the stored items ﬁrst, to create enough contiguous room. (b) Deﬁnitely. 2.3 Table Layouts 2.3.1 Data Layouts: Row Layout vs Column Layout Material Video: Original Slides: Inverted Slides: Learning Goals and Content Summary How could we phrase the tuple linearization problem formally?linearization Assume we have a set of tuples T = {t1,. .., tj,. .., ty} where all tuples t 2 T have the same schema, i.e. tj =(a1,j,. .., ai,j,. .., ax,j) 2 A1 ⇥ .. . ⇥ Ai ⇥ .. . ⇥ Ax where Ai is called a domain. We want to ﬁnd a linearization function L : int ⇥ int 7! int which maps two- dimensional attribute values to a one-dimensional space, i.e. for each index (i, j) of an attribute value ai,j we assign a one-dimensional value zk: L(i, j) 7! zk (2.1) such that 8(i1,j1) 7! zk1 , (i2,j2) 7! zk2 ,i1 6= i2 _ j1 6= j2 ) zk1 6= zk2 (2.2) In other words, equation 2.2 states that no two attribute values are mapped to the same zk-value. Any linearization L fulﬁlling that equation is called non-redundant.non-redundant How do we linearize tuples into a row layout?row layout We deﬁne a linearization Lrow with a lexicographical (aka recursive) ordering: Lrow(i, j) 7! Ly(j) M Lx(i) (2.3) 2.3 Table Layouts 105 \t (23, Albert, 45000) \t (42, Rob, 37000) \t (77, Peter, 50000) Row Stores Employees = \t (23, Albert, 45000) \t (42, Rob, 37000) \t (77, Peter, 50000) Column Stores Employees = Figure 2.5: Row store vs column store: the major diﬀerence is the linearization order of attribute values. This means, each index (i, j)is mapped toa zk-value where the ﬁrst part of zk (the preﬁx) is determined by a linearization function Ly(j) deﬁning the order among rows. The second part of zk (the suﬃx) is determined by a linearization function Lx(i) deﬁning the order among columns within one particular row. How do we linearize tuples into a column layout? column layout We swap the roles of columns and rows in equation 2.3. This means, we deﬁne a lin- earization Lcolumn with a lexicographical ordering: Lcolumn(i, j) 7! Lx(i) M Ly(j) (2.4) 106 Data Layouts This means, each index (i, j)is mapped to a zk-value where the ﬁrst part of zk (the preﬁx) is determined by a linearization function Lx(i) deﬁning the order among columns. The second part of zk (the suﬃx) is determined by a linearization function Ly(j) deﬁning the order among rows within one particular column. Why would we linearize tuples in row layout? Row-wise linearization is useful for queries that need to access several attributes of each tuple at a time but at the same time access only few rows. This is typically the case for insert-, update-, and delete-operations. Why would we linearize tuples in column layout? Column-wise linearization is useful for queries that need to access few attributes of each tuple, but at the same time have to access many tuples. This is typically the case for analytical queries, grouping and/or aggregating a large subset of the tuples of a table on few attributes. Which type of layout is better for which type of query? It depends on the attributes required to compute the result to a query. In particular the relationship of number (and size) of accessed attributes over the size of the entire row. What is a row store?row store Arow store is a database store thatuses rowlayouttolinearize data. What is a column store?column store Acolumn store is adatabase store thatuses column layout tolinearize data. Quizzes 1. When linearising tuples of a single relation, given the ordering of the tuples: (a) we can choose either row- or column layout for each tuple in the relation. (b) we can choose either row- or column layout for the whole relation. (c) we get the same linearisation order using any layout for relations containing a single attribute only. (d) we get the same linearisation order using any layout for relations containing a single tuple only. 2.3.2 Options for Column Layouts, Explicit vs Implicit key, Tuple Reconstruction Joins Material Video: Original Slides: Inverted Slides: 2.3 Table Layouts 107 Learning Goals and Content Summary Implicit Keys and Tuple Reconstruction Joins EK ID 0 23 1 42 2 77 EK name 0 Albert 1 Rob 2 Peter EK city_code 0 45000 1 37000 2 50000 Explicit Keys and Tuple Reconstruction Joins EK ID 0 23 2 77 1 42 EK name 1 Rob 2 Peter 0 Albert EK city_code 2 50000 1 37000 0 45000 Figure 2.6: Implicit key vs explicit keys in a column store What are correlated columns? correlated columns Two columns are correlated if the order of their attribute values w.r.t. their rowIDs is the same in both columns. In other words, assume two columns Ci and Cj containing attribute values of rows r0,. .., rN \u00001.Let SO be an arbitrary sort order of the sequence r0,. .., rN \u00001. Then, both Ci and Cj must contain their attribute values in the same sort order SO. What is an implicit key? implicit key An implicit key is a key that is not materialized but rather used as an input parameter to afunction. Forinstance, in acolumn layout, if arraysareused tostore ﬁxed-size attribute 108 Data Layouts values, the implicit key times the size of a single data value yields the oﬀset within the array. This is the function computed implicitly by the programming language compiler. So again, this is another example of linear addressing (implicitly done by the compiler). Could columns also be uncorrelated?uncorrelated Yes, however, then we need to make sure that we know the rowID for each attribute value. What are the consequences of using uncorrelated columns? What is an explicit key?explicit key This implies that we need to store the rowID with each attribute value in each column. So, each attribute of the original table is now represented by a column having two attributes: an explicit key and the actual attribute value. What is the impact of explicit keys on a tuple reconstruction join? tuple reconstruction join Tuple reconstruction joins get more expensive as they cannot rely on the same order of attribute values across columns. In contrast, in a layout using implicit keys for tu- ple reconstruction joins, a simple merge join, see Section 4.1.3, or a direct lookup, see Section 5.3.5, may be used. How do we convert a data layout using explicit keys into a layout using implicit keys? We simply sort all columns on their explicit key column. Then you throw away all explicit key columns and represent all data values in all columns in arrays. Be careful: this does not work when done in a purely relational model. As the relational model does not imply asortorder amongtuplesin arelation, wemustensurethatdatavaluesaremanaged using sequences. Quizzes 1. In case of an uncorrelated column layout (a) the values in each column have to be ordered according to the same sort order. (b) the values in the individual columns do not correlate to the values of the key column. (c) the values at a given position within each column do not necessarily belong to the same tuple. (d) the values at a given position within each column must belong to the same tuple. 2. Assume a column layout with uncorrelated columns. Assume we reconstruct tuples by ﬁrst sorting the columns on their explicit keys and then merging the sorted columns (this is called a sort-merge join). The following statements about this approach are true: (a) we have to sort the columns on the values and merge them into tuples. (b) we can directly merge the columns into tuples. (c) we have to sort the columns on the explicit key and merge them into tuples. (d) we merge the columns into tuples, and sort them on the explicit key. 2.3 Table Layouts 109 3. Explicit keys are required for (a) tuple reconstruction in correlated column layouts. (b) preventing duplicate elimination in uncorrelated column layouts. (c) tuple reconstruction in column layouts where the values at a given position of each column do not necessarily belong to the same tuple. 4. The beneﬁts of column layout with uncorrelated columns over column layout with correlated columns are: (a) faster tuple reconstruction (b) smaller storage requirements (c) faster inserts, in case there is a sort order on the tuples of the column layout with correlated columns (d) allows for sorting a subset of the columns independently (e) faster full scans for SELECT *-queries 5. Whenever a table in column layout is used in a query (a) the table has to be transformed into row layout. (b) the whole tuple has to be reconstructed. (c) apartial tuplereconstruction mightbesuﬃcient. (d) the other tables in the query also have to be in column layout. 2.3.3 Fractured Mirrors, (Redundant) Column Grouping, Verti- cal Partitioning, Bell Numbers Material Video: Original Slides: Inverted Slides: Additional Material Literature: [RDS02] Learning Goals and Content Summary What is the relationship of fractured mirrors to row and column layouts? fractured mirrors When using fractured mirrors the data is linearized in row and column layout redundantly, i.e. data is kept in row layout and column layout redundantly. First, we need to drop the condition of equation 2.2 to allow for redundancy, also recall our discussion in Section 2.3. Any linearization L not fulﬁlling equation 2.2 is called redundant and marked LR.Notice that LR is not a function anymore in the redundant 110 Data Layouts Fractured Mirrors \t (23, Albert, 45000) \t (42, Rob, 37000) \t (77, Peter, 50000) \t (23, Albert, 45000) \t (42, Rob, 37000) \t (77, Peter, 50000) Employees = Employees = \t ( 23, \t\t \t \t \t \t Albert, \t\t \t \t \t \t \t \t 45000 ) \t ( 42, \t\t \t \t \t \t Rob, \t\t \t \t \t \t \t \t \t 37000 ) \t ( 77, \t\t \t \t \t \t Peter,\t\t \t \t \t \t \t \t 50000 ) Column Grouping (aka Vertical Partitioning) Employees = Figure 2.7: Fractured mirrors vs column grouping mathematical sense as it returns a set of values. More formally : We deﬁne a linearization LR frm with a lexicographical ordering: LR frm(i, j) 7! \u0000Lrow(i, j),Lcolumn(i, j) (2.5) This means, each index (i, j)is mapped redundantlytotwo zk-values: one in row layout, one in column layout. Obviously the zk-values mapped to by Lrow(i, j) and Lcolumn(i, j) should be disjoint. Why would fractured mirrors make sense? This makes sense for a workload of queries where some queries are executed against a row 2.3 Table Layouts 111 Column Grouping with Partial Redundancy \t (23, Albert, 45000) \t (42, Rob, 37000) \t (77, Peter, 50000) Employees = \t (23, Albert, 45000) \t (42, Rob, 37000) \t (77, Peter, 50000) Employees = 52 Partitions of a 5-Element Set Figure 2.8: Column grouping with partial redundancy and the 52 possible partitionings of a 5-element set layout and others against a column layout. As the data is available in both layouts, we may route each query independently to the most suitable layout. What are the drawbacks of fractured mirrors? As we keep data redundantly, we need more storage space for this layout. In addition, we have to make sure that updates are reﬂected in both stores, to be more precise: even under updates all queries must return consistent results. For instance, if a query Q1 modiﬁes the row layout only, but not the column layout, it may sometimes still be correct to compute the next query Q2 against the outdated column layout. But, only if Q2 is not aﬀected by the previous change, i.e. whether it is computed against row layout or column layout, it returns the same results. Notice also that there is a strong relationship of fractured 112 Data Layouts mirrors to diﬀerential ﬁles, see Section 1.3.8. For instance, the read-only database may be organized in column layout whereas the diﬀerential ﬁle may be organized in row layout. Other examples include SAP HANA which as of writing these lines keeps its data in both layouts in main memory. What is column grouping and its relationship to vertical partitioning?column grouping vertical partitioning Given a source relation, we can partition it vertically such that each vertical partition contains a subset of that source relation. We can identify two extremes in vertical parti- tioning: the one extreme is a column layout (aka a full vertical partitioning). A column layout is conceptually similar to a vertical partitioning of the source relation where each partition contains one attribute of the original table. How that vertical partitioning is implemented is another story, see Section 2.3.2. Also notice that simply partitioning a table in SQL into vertical partitions does not have the same eﬀect as implementing this vertical partitioning natively in the database store and enriching the query optimizer to be aware of the column layout. The other extreme of vertical partitioning is a row layout, i.e. there is only a single “vertical partition” containing all attributes of the source relation. In-between these two extremes there is room for a layout where the vertical partitions contain more than one attribute from the source relation. These layouts are called column grouping. Again, what are important special cases of vertical partitioning? Assume a table foo with attributes A1,. .., Ax.Assuming that the vertical partitioning is complete and disjoint, we can identify the following special cases of vertical partitioning: Number of disjoint Layout name vertical partitions x column store 1 <i< x column grouping 1 row store However, keep in mind: by creating x diﬀerent vertical partitions of a table in a row store, still performance-wise this is typically far oﬀ from the performance of a native column store. The reason is that column stores typically do not only change the data layout, but also perform considerable changes in terms of how queries are processed, e.g. tuple reconstruction joins. We will get back to this in Section 5.3.5. Which type of queries would beneﬁt from column grouping? Queries touching a subset of the attributes of the source relation where that subset con- tains more than one attribute may beneﬁt from such a layout. How would we introduce data redundancy to column grouping?data redundancy We introduce data redundancy by representing some attributes of the source relation in multiple vertical partitions redundantly. How many vertical partitionings are there?partitionings Given a source relation with n attributes, the number of vertical partitionings is given 2.3 Table Layouts 113 by the Bell number Bn.For n  1 B0=B1=1 and for n> 1 the Bell numbers satisfy the Bell number following recurrence relation: Bn+1 = nX k=0 ✓n k ◆ · Bk. Starting with B0,the ﬁrst few Bell numbers are 1,1, 2,5,15, 52,203,877, 4140. For instance, a source relation with n =5 attributes can be vertically partitioned in B5 =52 diﬀerent ways. Quizzes 1. In case of fractured mirrors the throughput is potentially improved by factors over row- as well as column layout for (a) inserts (b) updates (c) deletes (d) selects 2. In case of fractured mirrors the throughput of read-only queries (a) is doubled, since we can direct the incoming queries to alternating copies. (b) gets worse, since we have to read both copies of the data. (c) can be improved by directing the queries to the copy stored in the layout more suitable for the query. (d) is the same as for row layout. 3. In case of fractured mirrors (a) we keep the ﬁrst half of the table in row layout, and the second half in column layout. (b) we keep two full copies of the tables in any layout. (c) we keep two full copies of the tables, one in row layout and one in column layout. (d) the copy of the table in column layout can only be stored in column layout with correlated columns. 4. Anon-redundantverticalpartitioning is (a) acomplete and disjunctpartitioning of thesetof tuplesof atable. (b) acomplete and disjunctpartitioning of thesetof attributesof atable. (c) acomplete partitioningof thesetof attributesof thenormalized table. (d) acomplete partitioningof theset of distincttuples of atable. 114 Data Layouts Attributes Queries Attributes Queries ORDERKEY CUSTKEY ORDERSTATUS TOTALPRICE ORDERDATE ORDERPRIORITY CLERK SHIPPRIORITY COMMENT 4 bytes 4 bytes 4 bytes 4 bytes 8 bytes 4 bytes 4 bytes 4 bytes 200 bytes Q3 Q4 Q5 Q7 Q8 Q9 Q10 Q12 Q13 Q18 Q21 Q22 X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X Figure 2.9: The usage matrix of the TPC-H Orders table 5. A table with X attributes can be partitioned into subsets along those attributes. Each of those vertical partitions consists of (a) attributes stored in column layout. (b) attributes stored in row layout. (c) attributes stored in any kind of layout. (d) attributes stored in the layout most suitable for the query workload. 6. Vertical partitioning with partial redundancy (a) is basically the same as fractured mirrors. (b) is vertical partitioning plus storing the heavily accessed tuples multiple times. (c) is vertical partitioning that is not disjunct. (d) can improve over non-redundant vertical partitioning in case of read-only work- loads. 7. The complexity of the vertical partitioning problem (a) is polynomial in the number of attributes. (b) is linear in the number of attributes. (c) can be expressed by the function calculating the Bell-numbers. Exercise In order to choose a suitable data layout of a table, we usually need a representative set of queries describing the workload of the given table. Let’s consider a variant of the usage matrix of the TPC-H Orders table from the industry-standard TPC-H benchmark 1 as displayed in Figure 2.9. In this matrix each row lists the attributes referenced by a given query (marked with ’X’). Let’s assume that these queries do nothing else but a full scan on the Orders table stored on disk projecting the marked attributes. In case of any vertically partitioned 1http://www.tpc.org/tpch/ 2.3 Table Layouts 115 layout (including a column layout), each query has to read any partition that contains an attribute referenced by the query. When doing this, it performs a single seek and a full scan for each referenced partition. Thus the execution time of a query is the sum of the time needed to read all partitions it references. The workload execution time is the sum of the execution times of each query in the workload. Caches are completely cold, i.e. the data is not available in main memory and all data required by a query must be fully read from disk for each query. The sizes of each attribute are listed on Figure 2.9 under their names. The TPC-H scale factor of our database is 10, which means the Orders table contains 15,000,000 rows. The server has a hard disk with an average seek time of 3 ms and a read bandwidth of 130 MB/s. The disk-, operating system-, and DB block sizes are all 8 KB. Main memory is limited so that we have a read buﬀer that can store 10 MB (note: kind of small, right?). Notice that data loaded into the buﬀer eventually needs to be reconstructed into atemporaryinternal tuplecontaining allattributesrequired bythisqueryat atime. We have a copy of the table in row-layout, column-layout, and fractured mirrors. For simplicity assume implicit keys for column-layout and other vertical-partitioned layouts. (a) When executing Q18 on the Orders table in each layout, what is the minimum amount of data you have to load into main memory (and very likely into the caches), respec- tively? (b) How long does it take in terms of disk I/O to execute Q18 for each of the layouts? (c) How long does it take in terms of disk I/O to transform the whole table from row layout to any of the other layouts? The disk write bandwidth is 90% of the read bandwidth. (d) Your task is to suggest a “suitable\" vertical partitioning (without replication) by intuition, and using the cost model described above compare it’s estimated workload execution time with that of row- and column-layout. 2.3.4 PAX, How to choose the optimal layout? Material Video: Original Slides: Inverted Slides: Additional Material Literature: [ADHS01] Further Reading: WApache Parquet 116 Data Layouts Learning Goals and Content Summary ( ( ( ( ( ( ( ( ( ( ( ( ) ) ) ) ) ) ) ) ) ) ) ), , , , , , , , , , , , , , , , , , , , , , , , EmployeesEmployeesEmployees ID name city_code 23 Albert 45000 42 Rob 65123 77 Peter 55443 78 Frank 66111 79 Tim 65653 12 Hans 12345 24 Dieter 66121 46 Jens 65432 62 Stefan 45789 64 Sebastian 44211 61 Andreas 37000 75 Rainer 61234 PAX Layout ( ( ( ( ( ( ( ( ( ( ( ( ) ) ) ) ) ) ) ) ) ) ) ), , , , , , , , , , , , , , , , , , , , , , , , EmployeesEmployeesEmployees ID name city_code 23 Albert 45000 42 Rob 65123 77 Peter 55443 78 Frank 66111 79 Tim 65653 12 Hans 12345 24 Dieter 66121 46 Jens 65432 62 Stefan 45789 64 Sebastian 44211 61 Andreas 37000 75 Rainer 61234 Another PAX Layout Figure 2.10: Two diﬀerent variants of PAX layout: three rows per block vs six rows per block What is PAX about?PAX PAX is a hybrid layout that ﬁrst divides the source relation horizontally into horizontal partitions. More formally (compare our discussion in Section 2.3): We deﬁne a linearization LPAX with a lexicographical ordering: LPAX(i, j) 7! Ly1(j) M Lx(i) M Ly2(j) (2.6) This means, each index (i, j)is mapped to a zk-value where the ﬁrst part of zk (the preﬁx) is determined by a linearization function Ly1(j) deﬁning the order among horizontal 2.3 Table Layouts 117 partitions. In other words, this is the horizontal partitioning function. The second part of zk is determined by a linearization function Lx(i) deﬁning the order among columns within one particular horizontal partition. The third part of zk (the suﬃx) is determined by a linearization function Ly2(j) deﬁning the order among rows within columns that are within one particular horizontal partition. How does PAX relate to horizontal partitioning? horizontal partitioning PAX can be considered a hierarchical partitioning: at the top-level (the root node) we apply a horizontal partitioning, underneath (the leaf-level) we apply vertical partitioning. Obviously, we may introduce other partitioning levels in this hierarchy. For instance: we could add another level under the leaf-level partitioning each column of a horizontal partition again horizontally (horizontal ! vertical ! horizontal). This may be useful to allow compression algorithms to read only parts of the column, see also Section 2.4. And, yes, this is yet another example of fractal design, see Section 2.3.5. How does the horizontal partitioning relate to blocks? block The sizes of the horizontal partitions may be chosen according to your system, workloads and needs. For instance, it is natural to choose the horizontal partitions to correspond to a relatively small database page of 64KB, like originally proposed in [ADHS01]. This would allow us to keep the same data on the page, we would just layout that data diﬀer- ently within a page. In this particular case, the overall system would behave unchanged w.r.t. page I/Os, however w.r.t. reads we may observe some performance improvements as queries may beneﬁt from the column layout inside a page. However, page sizes may also be relatively large, like for instance in HDFS, where page sizes are typically bigger than 64 MB. This is the core idea of modern HDFS-page layouts like Apache Parquet which is basically a reﬁnement of PAX. How does PAX relate to column layouts? column layout Again, a column layout is used within each horizontal partition independently. Assume we have a table T with N tuples and at least two attributes. Now, we can diﬀerentiate the following cases: Number of horizontal Layout name tuples per horizontal partitions partition 1 column layout N 1 <i< N PAX dN/ie N row layout 1 In other words, the higher the number of tuples per partition, PAX becomes close to a row layout. Vice versa, the lower the number of tuples per partition, PAX becomes close to a column layout. What are the advantages of PAX? Overall, PAX trades read performance w.r.t. columns with update performance w.r.t. tu- 118 Data Layouts ples. Recall what happens if we want to insert a tuple having ten attributes into a pure column layout: we will have to touch ten diﬀerent, possibly far apart, storage locations: one for each column. In contrast, in a pure row layout, we will only have to touch one storage location. PAX sits in-between those two extremes: we still have to touch ten dif- ferent storage locations, however, the maximum distance among those storage locations is limited by the size of the horizontal partition. How do we get the optimal data layout anyway?optimal data layout Well, ﬁrst make sure you understand what “optimal” means to you, see also Section 3.3. So for you, is “optimality” deﬁned w.r.t. some runtime model, i.e. counting CPU operations? Or are you counting page accesses and/or cache misses? Or is it wall-clock time you have in mind? Second, the right physical database design for your needs depends on your workload, i.e. the types of queries you want to execute. Third, the database schema and the data distributions may also, obviously, impact performance. If all that information is available to you, you may compute the optimal layout using a suitable optimization technique. And then keep your ﬁngers crossed that neither query workload nor data distributions nor schema change as that may require a diﬀerent optimal layout. So, in summary: it is diﬃcult to come up with an optimal layout. You should rather use a layout that is good enough and also robust in case workload, data distributions and/or schema change. For most workloads, PAX is not optimal, but: it is relatively robust and performs very well on many workloads. Quizzes 1. The PAX layout (a) is actually the same as fractured mirrors (b) is a hybrid of row layout and column layout (c) simulates a column layout for each table of a row store (d) is an imitation of row layout inside a column store 2. The properties of the linearisation order of the PAX layout are: (a) we linearise as in column layout inside a chunk of tuples (b) we take a chunk of tuples with as many tuples as attributes in the table (c) we linearise as in column layout for the whole layout (d) we linearise as in row layout inside a chunk of tuples 3. In the PAX layout (a) horizontal partitions typically match the page size (b) horizontal partitioning is applied inside the page (c) inside the page the tuples are laid out exactly as in slotted pages (d) we use a column layout inside the pages 4. Aworkload could be 2.3 Table Layouts 119 (a) aset orsequenceof queries executed againsta database (b) the I/O statistics of a DBMS collected over a given amount of time (c) the utilisation levels of the various storage devices storing the database (d) the layout of the data 5. The optimal layout of the data depends on (a) the workload (b) the storage media used (c) the DBMS (d) the performance (throughput, latency, etc.) preferences of the customer 2.3.5 The Fractal Design Pattern Material Video: Original Slides: Inverted Slides: Learning Goals and Content Summary What is fractal design (or self-similar design) in the context of database systems? fractal design self-similar designFractal or self-similar design occurs when a method X operating on granule Y can be adapted to work on a diﬀerent granule Z as well. A“granule” may be a layer of the storage hierarchy (e.g.L1, DRAM, disk), a computer system of any size (single node, cluster, datacenter), or a storage unit (cache line, page, chunk). For instance, RAID, see Section 1.2.6, was originally proposed to operate on “inexpen- sive disks”. However, disks are not the only granule where RAID may be applied. Other granules where RAID are applied include, RAID-systems (for nested RAID), SSDs, ﬂash storage chips (for the SSD-internal RAID 5), data centers, and cloud storage providers. See Figure 2.12. Why does fractal design help us to devise eﬀective algorithms? If you already know a solution works on granule Y, maybe it can be applied easily to work on a diﬀerent granule Z. This may be more eﬀective than reinventing everything from scratch. So the ﬁrst thing when designing a new algorithm is to understand whether an already existing method X may be adapted to work on granule Y. How does the fractal design pattern relate to The All Levels are Equal Pattern? The All Level are Equal Pattern, see Section 1.1.1, is just a special case of fractal design where granules are restricted to the diﬀerent layers of the storage hierarchy. In fractal design, we drop that restriction. Recall again, that both patterns are guidelines rather than strict rules. 120 Data Layouts Example: Mandelbrot Set Example: Sierpinsky Triangle Figure 2.11: Example fractals: the Mandelbrot Set and the Sierpinsky Triangle Can you name a couple of examples of fractal design in databases that we have already seen? We have seen this already in many diﬀerent places. 1. linear addressing is used to address pages inside a segment (inter-page addressing), but also to address chunks (rows or columns) inside a page (intra-page addressing), see Section 2.2.2. In addition, the organization and virtualization of storage using preﬁx-trees, see Section 1.4.1, is self-similar for diﬀerent granules when changing the length of a preﬁx. 2. RAID, see Section 1.2.6, is used to combine multiple hard disks or SSDs into a virtual disk that is more reliable and depending on the RAID-level also faster. Those virtual 2.3 Table Layouts 121 Example: Nested RAID Example: The Data Redundancy Pattern Figure 2.12: Examples for self-similar design in databases: nested Raid and data redundancy disks may be combined with other virtual disks applying RAID again. Like that we have a two-level (nested) RAID, see Section 1.2.7. This is self-similar design as the idea of RAID is applied on two diﬀerent levels. If the physical SSDs used in that RAID-system uses RAID 5 internally for error correction, see Section 1.2.9, this adds a third layer of self-similarity. And ﬁnally, if we assume that the entire RAID storage system is replicated across data centers this adds a fourth layer of self-similarity. 3. PAX is another example of self-similar layouts, see the discussion in Section 2.3.4. 122 Data Layouts Quizzes 1. What patterns are special cases of the Fractal Design Pattern? (a) All Levels Are Equal Pattern (b) Batch Pattern (c) No Bit Left Behind Pattern 2. Which of the following might be examples of the Fractal Design Pattern? (a) Nested RAID (b) Datacenter replication (c) Tape 3. How many levels of self similarity can you ﬁnd in a RAID-55 system with SSDs? (a) 2.4 Compression 2.4.1 Beneﬁts of Compression in a Database, Lightweight Com- pression, Compression Granularities Material Video: Original Slides: Inverted Slides: Additional Material Literature: [WKHM00] Further Reading: [HRSD07] Learning Goals and Content Summary Compression is mainly about saving storage space, right?storage space Not only. In databases the more important eﬀect is saving bandwidth in all kinds of situations when data is transferred over a wire: from/to disk, over the network, over a bus like the memory bus. Compressing data costs something in addition! You can only lose w.r.t. overall query response times, right? That is simply not true. We have to keep in mind the total costs for 1. compressing the data, 2. transferring it, and then 2.4 Compression 123 Compression Granularities attribute values tuples pages horizontal partitions of a table tables databases Accessibility Compression Ratio Figure 2.13: Compression granularities and their trade-oﬀs w.r.t. accessibility of data and compression ratio 3. decompressing it. Those steps may actually overlap, e.g. the CPU time for step 1, compressing, may overlap with step 2, the actual transfer time. In addition, if step 1 only needs to be executed once, but step 2 is done several times, the costs for compressing may be amortized over several executions of step 2. Moreover, query processing may in several situations work directly on the compressed data. Thus, the costs for step 3 may actually become zero, see Section 2.4.2. What is the major trade-oﬀ we have to keep in mind when compressing and decompressing data? Steps 1, 2, and 3 should be less expensive than just executing step 2 on uncompressed data. What are compression granularities? compression In general, a compression algorithm foo(X) 7! Yis executed on aninput itemX and compresses that item to a byte-sequence Y. The input item X has a speciﬁc size which is coined the granule of X.Examples forX, of increasing granule, are: a single attribute value, a tuple, a data page, a horizontal partition of a table, an entire table or even an entire database. How do the diﬀerent compression granularities aﬀect accessibility and compression ratio accessibility compression ratioof your data (in general)? For most compression algorithms if we want to access a smaller data item Z that is contained in a compressed byte-sequence Y, we cannot simply retrieve it from Y. In order to access Z, we have to decompress Y from the beginning of the compressed byte- sequence Y until we ﬁnd Z (or we even have to decompress Y entirely). This means, 124 Data Layouts accessing Z inside Y triggers some overhead for decompressing other data we are actually not interested in. On the other hand, several compression algorithms work best if they are applied to a large input item X, e.g. if X is an entire table, we will likely gain a lot. In contrast, is X is only a single attribute value, we do not gain much. To sum up: usually larger chunks of data allow for a better compression ratio, however, accessing data within a large compressed chunk may be expensive if only a small data item within that chunk needs to be retrieved. Quizzes 1. Consider the following inequality from the video: time(decompression)+ time(readCompressed) < time(readU ncompressed).Let us assume the in- equality does not hold, i.e. time(decompression)+ time(readCompressed) \u0000 time(readU ncompressed),but it is still worth processing the compressed data. What could be a reason for that? (a) decompression can overlap with reading the compressed data, therefore we should only consider the time it takes to read the compressed data. (b) decompression can overlap with reading the compressed data, and decompres- sion is also faster than the time spent reading the compressed data. (c) this simply cannot happen. 2.4.2 Dictionary Compression, Domain Encoding Material Video: Original Slides: Inverted Slides: Learning Goals and Content Summary What is a dictionary?dictionary Aforeign language dictionarytranslates terms fromlanguage Xtoterms in language Y. In the context of a database system a dictionary does something very similar: a database dictionary translates terms from a domain X to terms of domain Y. Terms in domain X are typically short and memory-eﬃcient keys, e.g. integers. In contrast, terms in Y are typically long, e.g. strings. How do we apply dictionary compression? dictionary compression Assume a table foo with attributes A1,...,An. In order to dictionary compress col- umn Ai,we split foo into two tables which are connected to a foreign key relationship as follows: 1. compute the set of distinct values of Ai, 2. assign an artiﬁcial key ID to each distinct value found, 2.4 Compression 125 Dictionary Compression Colleagues2Colleagues2Colleagues2 name street cityID peter narrowstreet 0 steve macstreet 1 mike longstreet 2 tim unistreet 2 hans msstreet 0 jens meerweinstreet 1 frank narrowstreet 0 olaf macstreet 2 stefan unistreet 2 alekh unistreet 2 felix macstreet 0 jorge narrowstreet 2 Cities_DictionaryCities_Dictionary cityID city 0 new york 1 cuppertino 2 saarbruecken Dictionary Compression Cities_DictionaryCities_Dictionary cityID city 0 new york 1 cuppertino 2 saarbruecken Colleagues3Colleagues3Colleagues3 name streetID cityID peter 0 0 steve 1 1 mike 2 2 tim 3 2 hans 4 0 jens 5 1 frank 0 0 olaf 1 2 stefan 3 2 alekh 3 2 felix 1 0 jorge 0 2 Streets_DictionaryStreets_Dictionary streetID streets 0 narrowstreet 1 macstreet 2 longstreet 3 unistreet 4 msstreet 5 meerweinstreet Figure 2.14: Dictionary compression: just column cityID vs both streetID and cityID 3. create a new table Ai_Dictionary with schema [Ai_Dictionary, term], i.e. Ai_Dictionary is the key, 4. change the schema of attribute Ai in table foo to be a foreign key to column ID in ta- ble Ai_Dictionary,replace values in foo.Ai by corresponding Ai_Dictionary.ID, rename foo to foo_New, 5. create a (dynamic) view foo_view with a natural join on foo_New and Ai_Dictionary showing all attributes except Ai_Dictionary.ID;in other words, this view returns the same result set as table foo, 6. replace all occurrences (in all SQL statements and views) of table foo by the view foo_view. 126 Data Layouts Notice that all of these steps can easily be done (and should be done) in SQL. What do we gain by using dictionary compression? If the sum of the sizes of tables foo_New and Ai_Dictionary.ID is smaller than the size of table foo,we gain space. Whether we gain space depends on the data distribution of values in foo.Ai,in particularthe number of repetitions. In addition to gaining space, we may also speed-up certain types of queries which may operate on dictionary-compressed data without decompressing the data. What do we loose by using a dictionary compression? For each dictionary-compressed column, we introduce an additional join. So, in the worst case, if dictionary compression is applied to all attributes A1,...,An of table foo and we have a query referencing all attributes that query will have to use n additional joins. These join may lead to considerable costs in query processing. Actually, decompressing a dictionary during query processing can be considered a variant of an anti-projection or tuple reconstruction (see Section 5.3.5). In dictionary decompression, tuple reconstruction is done using attribute values rather than rowIDs. How does dictionary compression relate to CREATE DOMAIN in SQL?CREATE DOMAIN If you create a domain in SQL, you often explicitly list the set of allowed values in the domain’s deﬁnition. Once, the domain is deﬁned, you may use that domain to deﬁne attribute types in any table of your database. For data inserted into such a table, the database management system then has two options: 1. insert the actual value into that table, or 2. insert a dictionary key linking to a dictionary representing that domain. However this is implemented by the database system, its implementation does not have to be exposed to the user. For instance, assume you deﬁne a domain that is allowed to contain three possible string values only like “foo”, “bar”, and “whatever”. Now, for each row having an attribute typed with that domain the database system may store the actual string. Obviously this is not very eﬃcient. A better solution is to map the three strings to a dictionary { 0 7! “foo”, 1 7! “bar”, 2 7! “whatever”} The dictionary key only requires 2 Bits. The drawback of this approach is that, depending on the query, we need to lookup the dictionary to “understand” the dictionary keys. In summary, creating a domain in SQL deﬁnes a type where the instances of that type may be represented using a dictionary. In contrast, creating a dictionary does not create a type. Is a dictionary something a user has to be aware of ? Dictionaries should be hidden from the user. In SQL dictionaries should be hidden using (dynamic) views. How does dictionary compression relate to domain encoding?domain encoding Once you applied dictionary compression to a particular column, it may pay oﬀ to also apply domain encoding. For instance, assume the original column foo was of type var- 2.4 Compression 127 char(32). Now, we apply dictionary compression replacing each strings in that column by its corresponding key in the dictionary. Let’s assume we use an int (4 Byte) type for this. So now, our column foo is of type int. This requires 4 Bytes for each entry. However, if our dictionary contains less entries than the 232 diﬀerent keys that can be represented by int, we may use a smaller type: If our dictionary has n entries, we need at most dlog2(n)e bits per entry. For instance, if n = 42,we require dlog2(42)e =6 bits. Can we apply domain encoding at diﬀerent levels? Domain encoding can be done at two diﬀerent levels: (1) by choosing the right type for your attributes when deﬁning/changing the schema; or: (2) by implementing domain encoding internally, i.e. inside the DBMS. Notice that what the DBMS claims to be the type of a column to the outside does not necessarily have to be implemented like that by the DBMS internally. Quizzes 1. Dictionary compression (a) is used to bring tables into the third normal form (b) can be used to eliminate the redundancy of the attribute values (c) can be used to reduce the storage requirements of the attributes values (d) can only be applied for row stores 2. The key idea of dictionary compression is (a) to reduce the storage requirements by transforming the values into a domain whose values need less storage (b) to reduce the storage requirements by denormalizing the table (c) to speed-up processing of the compressed column by counting the distinct values in the column 3. When rewriting a point-query to be executed on a dictionary-compressed column, it is most natural to use (a) asub-query tolookup the compression key of thevalueused in theselection predicate (b) asub-query tolook up the value forthecompressed key used in the selection predicate (c) ajoin between the compressed tableand thedictionaryof the column 4. If we need 3 bits to store each key in a dictionary compressed column in main memory (a) it is always best to use 3 bits to store each key (b) it is best to use exactly as many bits for each key as the size of a processor word, e.g. 32 or 64 bits 128 Data Layouts (c) it can be beneﬁcial to use the smallest directly accessible storage granule that can hold the keys, i.e. a single byte, to avoid the overhead caused by bit-shifting (d) it can be beneﬁcial to use the smallest amount of bits, in this case 3 bits, to store each key, to maximise throughput of full-scan queries 5. The relation of dictionaries and domains is the following: (a) they are basically the same constructs (b) a domain is only a set of values present in a given column, while the dictionary is a mapping between the keys and these values (c) the dictionary is a mapping between the keys and the set of values present in a given column, while the domain deﬁnes the possible values of the column (d) domains only exist in PostgreSQL, while dictionaries are available in the ma- jority of DBMSs 2.4.3 Run Length Encoding (RLE) Material Video: Original Slides: Inverted Slides: Learning Goals and Content Summary Run Length Encode... name peter frank jorge steve olaf felix mike tim stefan alekh hans jens cityID 0 0 2 1 2 0 2 2 2 2 0 1 streetID (3,3) (4,3) 5 (6,3) 7 8 Recurse... name peter frank jorge steve olaf felix mike tim stefan alekh hans jens streetID (3,3) (4,3) 5 (6,3) 7 8 cityID (0,2) 2 1 2 0 2 (2,3) 0 1 Figure 2.15: Run length encoding: on streetID vs streetID and cityID How does run length encoding (RLE) work?run length encoding RLE Run length encoding takes as its input a sequence of values. Whenever that sequence contains a subsequence of at least two equal values, those repetitions are represented as a pair (v, count) where v is the data value and count the number of repetitions. Like that all repetitions in that sequence may be represented by a single pair rather than repeating each value vcount times. 2.4 Compression 129 What is the relationship of RLE to sorting? sorting Obviously, RLE works best if the data was sorted as this increases the likelihood of subsequences having repetitions. What is the relationship of RLE to lexicographical sorting? lexicographical sorting In a database it may pay oﬀ to recursively sort, and hence implicitly group the data, before applying RLE. For instance, for a table Colleagues with attributes streetID and cityID, we may sort these columns lexicographically. This means, we ﬁrst sort on streetID. In that process, for all values in streetID that are equal, we sort their cityID values. Like that the rows are ordered w.r.t. both streetID and cityID. Thus, we may achieve a higher compression ratio as when sorting only on a single attribute. However, whether this strategy pays oﬀ and how many columns should be sorted lexicographically for a particular table depends on the data distribution, in particular the number of diﬀerent values in these columns. What are possible pros and cons of RLE? RLE may not pay oﬀ in cases where the column does not contain duplicates, e.g. if the column is a candidate key. In general, whether RLE pays oﬀ depends heavily on the cardinality of the column, i.e. the number of diﬀerent values in that column and its relationship to the total number of values in that column. Quizzes 1. Run length encoding only makes sense if the column to be compressed (a) is sorted (or same values are clustered) (b) is dictionary encoded (c) stores numbers (d) stores strings 2. Which of the following operations suﬀer in performance from applying run length encoding to a column: (a) aggregation and grouping both on the RLEncoded column (b) a selection after grouping on the RLEncoded column (i.e. a HAVING clause) (c) point lookups (d) projecting the RLEncoded column 130 Data Layouts 2.4.4 7Bit Encoding Material Video: Original Slides: Inverted Slides: Additional Material Further Reading: W UTF-8 Learning Goals and Content Summary Option 2: 7Bit Encoding 0 1 0 1 0 1 1 1 1 bit signal 7 bit data = 1 0 1 0 1 1 1 b = 1 1 0 1 0 1 1 1 =0 1 0 0 1 0 1 0 1 0 1 0 1 1 1 1 0 0 1 0 1 0 b 11210= d 87 d 7Bit Encoding 1 1 0 1 0 1 1 1 =1 1 0 0 1 0 1 0 0 0 0 0 0 0 0 1 1 0 1 0 1 1 11 0 0 1 0 1 0 0 0 0 0 0 0 1 b = 1434881 d = Figure 2.16: 7Bit encoding: up to two vs three bytes What is the core idea of 7Bit Encoding?7Bit Encoding The core idea of 7Bit encoding is to adapt the number of bytes used for storing an individual value. In 7Bit encoding we logically split each byte into two parts: the ﬁrst bit is considered a signal bit, the remaining seven bits represent the actual data. A data value is represented by a sequence of k \u0000 1 bytes where the signal bit of the last byte is unset, all other signal bits are set. Like that, conceptually a data value is represented by a list of bytes. If the input data value has n bits, we require dn/7e bytes. This idea is also used when representing characters in UTF-8. 2.4 Compression 131 How does 7Bit encoding relate to domain encoding? domain encoding In 7Bit encoding the adaptation happens for each individual data value rather than an entire domain, i.e. an entire database column. Usually, 7Bit encoding should be the last compression method to try. If the data values to compress are of varying length, it may be more beneﬁcial to use dictionary compression followed by domain encoding (assuming there are duplicate values). How much storage space is lost or gained when using 7Bit encoding? storage space In terms of net storage, for every eight bits we lose one signal bit (metadata). In other words, one out of eight bits is unused. In general, if before compression you used a ﬁxed- size domain, say of eight bytes for every data value, in 7Bit encoding those 8 bytes can only represent 8 · 7=56 bits rather than 8 · 8 = 64 bits. So every uncompressed value requiring more than 56 bits will need more than 8 bytes for storage in 7Bit encoding. In summary, whether 7Bit encoding pays oﬀ depends heavily on the data distribution. Quizzes 1. When should you use 7-bit encoding? (a) 7-bit encoding is always beneﬁcial. (b) When there is a lot of variance in the sizes of the binary representation of the values. 2. What are drawbacks of 7-bit encoding? (a) Diﬀerent values inside the same column can have diﬀerent sizes. (b) Not all bits are used to store data. (c) Every value needs at least one byte. 3. How can you access an arbitrary position in a column with 7-bit encoded values? (a) Simply calculate the oﬀset of the value. (b) Scan through all values. (c) Jump to the nearest known oﬀset and scan from there. 4. How many bytes are needed to represent a 32 bit integer using 7-bit encoding? (a) Exercise Assume the following database tables as shown in Figure 2.17. (a) Assume the tables are stored in column layout. Which compression techniques pre- sented in the lecture would you choose for the following tables? Find at least one compression technique as well as the sort order for each table that reduces the amount of data considerably. You are allowed to combine compression techniques. Determine the compression ratio, i.e. “compressed data size\"/“uncompressed data size\". 132 Data Layouts (b) Assume the tables are stored in PAX layout. What is the impact on the compression techniques chosen in (a)? Hint: factor in the size of a PAX-block in your argu- mentation. Keep in mind that in reality both tables and PAX block sizes are much larger. (Note: do not think about possible future data for this table, just try to compress the given data as well as possible.) visitors ID Downloads IP_address 13 217 138.92.122.175 81 0 138.92.122.195 42 6 138.92.122.182 25 4 138.92.122.181 21 52 138.92.122.177 56 4 138.92.122.188 78 2 138.92.122.191 30 1 138.92.122.185 23 0 138.92.122.179 80 2 138.92.122.193 27 3 138.92.122.183 82 0 138.92.122.197 issues ID Status Subject 1 In Progress Migrate Moodle site to Turnkey VM 2 In Progress Come up with backup strategy 5 New Test recovery of Moodle site 6 Resolved Drink ﬁfth coﬀee 7 Resolved Buy next coﬀee 8 Resolved Test group selection for students 9 Resolved Set up Moodle site users ID Name Origin 12 Frank Boring (Oregon, USA) 2 Robert Boring (Oregon, USA) 50 Florian Boring (Oregon, USA) 32 Dominik Boring (Oregon, USA) 33 Viktor ii (Finland) 10 Christian ii (Finland) 14 Stefan ii (Finland) 1 Martin ii (Finland) 9 Jan ii (Finland) 21 Josef ii (Finland) 15 Achim ii (Finland) 16 Salim Truth Or Consequences (New Mexico, USA) 34 Dominik Truth Or Consequences (New Mexico, USA) 30 Hassan Truth Or Consequences (New Mexico, USA) 42 Kamran Truth Or Consequences (New Mexico, USA) Figure 2.17: The tables to compress Chapter 3 Indexes 3.1 Motivation for Index Structures, Selectivities, Scan vs. Index Access Material Video: Original Slides: Inverted Slides: Additional Material Literature: [LÖ09], Tree-based Indexing Further Reading: [LÖ09], Index Tuning Learning Goals and Content Summary What are the major analogies of indexing in real life? indexing We can observe several examples of indexing in real life: street signs show you the way and thus allow you to cut down the search space. The same holds for room plans inside buildings which allow you to go straight to the right room inside the building rather than searching all rooms. Similarly, a (printed) phone book is ordered alphabetically by name. This allows you to ﬁnd entries using binary search. Why do we use indexes? The core idea of indexing is to prune the search space. Rather than inspecting all of the entries in a database, you only inspect a subset of the entries. That subset may still be asuperset of the entries you areactually looking for(this type of index isthen called a ﬁlter index). In that case the elements returned by the index still have to be post-ﬁltered. ﬁlter index post-ﬁlterAlternatively, an index may return a precise result which does not have to be post-ﬁltered anymore. Getting back to our street sign analogy: indexes often use multiple street signs. For instance, in a binary search tree, each internal node can be considered a street sign allowing you to make a decision to either continue your search on the left or the right 134 Indexes Figure 3.1: Indexes work just like street signs. subtree. Like that at every street sign (or node) you cut down the search space by a factor two (assuming a perfectly balanced tree). What is selectivity?selectivity Selectivity is a measure quantifying the fraction of elements returned by a function or method. Selectivity is always deﬁned as a ratio of the elements returned over the total number of elements, i.e. selectivity is the ratio 0  |result set| |set|  1 where: result set ✓ set. For instance for a table R and a selection \u0000a=42(R),the selectivity is |\u0000a=42| |R| ,in other words: this is the ratio of the number of tuples of relation R where a=42 over the total number of tuples in R. What is high selectivity?high selectivity 3.1 Motivation for Index Structures, Selectivities, Scan vs. Index Access 135 The term “high selectivity” is counterintuitive: it means that |result set| |set|  1 is very small, i.e. close to 0. In other words, only few elements qualify under the selection, most of the elements are discarded. What is low selectivity? low selectivity The term “low selectivity” is counterintuitive as well: it means that |result set| |set|  1 is very big, i.e. close to 1. In other words, many elements qualify under the selection, only few elements are discarded. What type of data managing systems are indexes important for? Indexing technology is important for almost all types of data managing system. In partic- ular, one can sometimes read statements that main-memory database systems do not use indexes. This is simply not true. Be it that your data resides on disk, in main memory or on any other device. In many situations the right types of indexes allow you to speed up look-ups by several orders of magnitude. However, keep in mind that a simple scan in amain-memorysystem maybeveryfast, e.g.on the orderof millisecondsforscanning 100 million entries in a column single-threaded. Depending on your runtime requirements this may be fast enough already for your application. Hence, technically you may not need indexes to fulﬁll your runtime requirements, yet you may still need indexes to reduce energy consumption and overall throughput of your system. Quizzes 1. What are the basic index structure types mentioned in the video and also used in databases? (a) hash maps (b) trees (c) street signs 2. Which of the following are examples of real-life physical index structures? (a) catalogs in libraries (b) phone books (c) street signs (d) advertisements (e) cooking books 3. What does it mean for a query q to have a high selectivity? (a) The value sel(q) is high. (b) The value sel(q) is close to zero. (c) Only a few tuples are selected by the query. (d) Only a few tuples are excluded from the result. 4. Should you use indexes in main memory? 136 Indexes (a) No. Indexes were invented for disk based systems. In memory it is feasible to perform full scans for all queries. (b) Yes, if it is more beneﬁcial than doing a full scan. 5. What is the deﬁnition of the selectivity of a selection predicate P on a relation R? (a) sel(P )= |\u0000P (R)|/|R| (b) sel(P )=1 \u0000 |\u0000P (R)|/|R| (c) sel(P )= |R|/|\u0000P (R)| Exercise Suppose that the database of a University contains, among others, the following relations (all containing a considerable number of tuples): attendance lectures_id students_id taught_by lectures_id professors_id students id name street city immatriculation_year professors id name street city lectures id name level credits semester Assume that the contents of all tables changes frequently through INSERT, UPDATE, and DELETE operations (more changes happen to students and lectures in the semester break). In addition, the University’s information system triggers SQL-queries retrieving the following data: Q1: get all the lectures (name and semester) a given student (by students.id) attended Q2: get the name of all students attending a given lecture (by lectures.id) Q3: get the name of all students who are in a given semester, e.g. ﬁfth semester Q4: get the city of a given student (by students.id) Q5: get the cities for all students in the database Q6: get all students living in a particular city, e.g. “Saarbrücken” Q7: get all the lectures (name and semester) taught by a given professor (by professors.id) 3.2 B-trees 137 Given the above information, for each query, i.e. treating each query Q1, ... , Q7 independently: (a) Which index(es) would you create (provided in SQL syntax)? (b) What would be a suitable data layout? Given the above information, for the database as a whole: (c) Which index(es) would you create (provided in SQL syntax)? (d) What would be a suitable data layout? 3.2 B-trees 3.2.1 Three Reasons for Using B-tree Indexes, Intuition, Prop- erties, ﬁnd(), ISAM, ﬁnd_range() Material Video: Original Slides: Inverted Slides: Additional Material Literature: [LÖ09], Indexed Sequential Access Method [RG03], Section 8.3.2, [RG03], Section 10.1 – 10.4 Learning Goals and Content Summary The three reasons for using B-trees are ... ? B-trees 1. B-trees are storage-friendly, i.e. they can be adapted to diﬀerent layers of the storage hierarchy. See also the All Levels are Equal Pattern in Section 1.1.1 and Fractal Design, Section 2.3.5. 2. B-trees strike a good balance in terms of sequential versus random layout. For in- stance, consider a binary search tree ( BST): it keeps only one pivot element at each BST node. This means with high probability at every node hop you need to visit a hard to anticipate memory address. The number of expected cache misses is high. And mapping BSTs to disk pages is rather diﬃcult. If you map each node to a separate disk page, a search down that tree will trigger one disk seek in the worst case. A big advantage of BSTs is that their structure may be changed easily in-between 138 Indexes B-Tree Node and Leaf Sizes \r n ∈ [k;2k] keys \t => n+1 children \t [k*;2k*] key/value-pairs \r n ∈ [1;2k] keys \t => n+1 children \t [1;2k*] key/value-pairs 75 k = 1 11 15 34 M T S k* = 2 Nodes Leaves if not root: if root: B-Trees and Interval Partitioning -∞;60[ [60;123[ [123;+∞[ 60 123 63 10322 42 11 15 . . 22 37 . . 44 59 . . 60 62 . . 63 99 . . 103109 . . 137 443 123136 . . 137228 . . 543555 . . k = 1 k* = 1 Figure 3.2: B-tree properties and their recursive interval partitioning 60 84 ﬁnd_key(67) 11 15 34 56 . . . . 67 72 73 83 . . . . 84 87 102 105 . . . . 60 84 ﬁnd_range([34;72]) 11 15 34 56 . . . . 67 72 73 83 . . . . 84 87 102 105 . . . . Figure 3.3: Searching keys and intervals in a B-tree 3.2 B-trees 139 any node (and hence pivot element). Thus, the structure of a BST is extremely ﬂexible. The other extreme in terms of sequential versus random layout is an array. It is simply a compact chunk of sequential memory. All pivot elements represent- ing internal nodes can be precomputed (and or cached automatically, e.g. through changing the layout of the array in clever ways) leading to fewer cache misses. How- ever, it is very hard to insert and or delete elements in/from the array. B-trees are acompromisein-between thosetwoextremes: theyinherittheﬂexibilityof BSTs and the compactness of arrays. 3. B-trees are extremely ﬂexible and may me adjusted to support more advanced in- dexing problems. For instance, in order to index two-dimensional data, B-trees may be extended. The resulting tree is then coined an R-Tree (a rectangle tree). R-Trees R-Tree inherit all core properties already known from B-trees, but introduce important ex- tensions. Still at a high-level, an R-Tree is a special case of a B-tree, and some software libraries implement it accordingly [dBBD+01]. What is the intuition for B-tree indexes? B-trees connect chunks of data in a tree-structure. In contrast to BSTs, the chunks are much larger. Typically, the chunk corresponds to the granule of some layer in the storage hierarchy, e.g. a disk page or a cache line. Other than that the ideas of BSTs and B- trees are very similar: in both structures the data must be maintained range-partitioned. Otherwise, search operations would not improve much. What is a node? node A node keeps a set of keys (aka pivots) and a set of pointers to children. The children may be nodes or leaves. A node is sometimes also called an inner node or index node. inner node index nodeWe will simply call it node. What is a leaf? leaf Aleaf keeps aset of key/value-pairs. What is k? k The parameter k is the minimum number of keys in a node, 2k the maximum number of keys in a node. Notice that these parameters may also be deﬁned such that k/2 is the minimum number of keys and k is the maximum number of keys in a node. We will stick to the former deﬁnition. What is F? F The fan-out F (aka branching factor)of anode is the numberof its children and hence fan-out branching factorthe number of subtrees of this node. For a node having y 2 [k;2k] keys, its fan-out is F =2y +1.Notice that the term fan-out may be used to either denote the minimum fan-out (Fmin = k +1), the current fan-out (Fcurrent =2y +1 where y 2 [k;2k])orthe maximum fan-out (Fmax =2k +1). What is h? h The height h of a B-tree depends heavily on its fan-out F . height 140 Indexes What is k*?k* The parameter 2k⇤ deﬁnes the number of entries in a node. Notice that this parameter may also be deﬁned such that k is the maximum number of elements in a node and k/2 the minimum number of elements. How many keys does a node have? In general, a node has n 2 [k, . . . , 2k] keys. This implies that a node has n +1 pointers to children. If that node is the root node, it may have n 2 [1,. .., 2 · k] keys. How many keys does a leaf have? In general, a leaf has n 2 [k⇤,. .., 2k⇤] key/value-pairs. A leaf does not have children. If that leaf is the root node, it may have n 2 [1,. .., 2 · k⇤] keys. What is the index sequential access method (ISAM)? How is ISAM related to point and index sequential ac- cess method ISAM range queries? The index sequential access method (ISAM) extends a B-tree to additionally implement a doubly linked list among all leaves. This means, each leaf stores two additional pointers: one to its left sibling and one two its right sibling. ISAM allows us to scan multiple leaves in ascending or descending key order without having to visit all visited leaves through their parent nodes. ISAM is useful to implement eﬃcient range queries. What are the major properties of a B-tree? 1. The path from the root (be it a node or a leaf) to any leaf has always the same length. In other words, a B-tree is always balanced w.r.t. the number of nodes and leaves visited during a search. 2. The parameters k and k⇤ are implicitly deﬁned by the size of a node. For instance, assume we set the node size to 4KB, a single key (a pivot) requires 4 Bytes, and a pointer to a child requires 8 Bytes. Then a node with k entries occupies k ·(4+8)+8 Bytes ) k = b(4096 \u0000 8)/(4 + 8)c = 340.Similarly, assuming that a single value (of akey/value-pair)occupies6Bytesand each leaf containstwopointerstosupport ISAM. Then a leaf with k⇤ entries occupies k⇤ · (4 + 6) + 2 · 8 Bytes ) k⇤ = b(4096 \u0000 2 · 8)/(4 + 6)c = 408. 3. The nodes do not contain values (of key/value-pairs). These values are only kept in leaves. The reason for this is that this makes ISAM more eﬃcient. 4. The keys inside nodes and the key/value-pairs inside leaves are sorted by key. This allows for eﬃcient binary search inside anode orleaf. However, this sortingis no strict requirement. 5. Notice that B-trees exist in many diﬀerent variants, e.g. B-tree, B +-tree, and B⇤- tree. The major diﬀerence of B-trees and B+-trees is: B-trees also store values in nodes. In addition, B-trees do not support ISAM. In the following we will only consider B+-trees, yet to be consistent with database literature, we simply refer to B+-trees as B-tree. 3.2 B-trees 141 What is the relationship of B-trees to interval partitioning? interval partitioning Every node and every leaf implies an interval partitioning. Assume a root node with only two keys 60 and 123 with sub-tress sub1, sub2,and sub3 (see Figure 3.2). Assume that we use the convention that equal keys are represented in the right subtree. For instance, if we search for key 60, we have to continue our search at the right child of 60,i.e. subtree sub2. This means this root node partitions the entire tree into three subtrees where each of the three subtrees represents a particular interval of keys. In the example, subtrees sub1, sub2,and sub3 represent intervals ] \u00001; 60[, [60; 123[,and [123; +1[, respectively. This interval partitioning is then applied recursively. For instance, subtree sub1 represents interval ] \u00001; 60[. In Figure 3.2, the node at the second level with keys 22 and 42 implies areﬁned partitioningintointervals ] \u00001; 22[, [22; 42[,and [42; 60[,respectively. How do we search for a particular key in a B-tree, i.e. how do we execute a point query? point query We start our search at the root. If the root is a node (not a leaf), we have to ﬁnd the appropriate subtree, i.e. the appropriate child. To support this a node should implement amethod chooseSubtree(Key K) ! Child.For a given key K this method returns the subtree which may have data for that key. We then continue our search recursively on that subtree. If the root of that subtree is a node, we call chooseSubtree(Key K) on that node again. If it is a leaf, we reached the last level of the tree. Inside a leaf, we check whether key K exists (using binary search). If that is the case, we return the value associated to that key. In other words, a point query in a B-tree is similar to a point query in a binary search tree. The major diﬀerence is: at every node in a B-tree there are possibly more than two children. How do we search for a particular interval in a B-tree, i.e. how do we execute a range query? range query In order to ﬁnd all entries in key interval K1,..., K2, ﬁrst, we execute a point query on K1. Once we found the ﬁrst entry of K1 in a leaf in that point query (or if an entry for K1 does not exist, the ﬁrst entry bigger than K1), we simply scan along the leaf-level until we ﬁnd a value strictly bigger than K2. At that point we stop our search. Notice that this method only works if the B-tree implements ISAM. Quizzes 1. Which of the following trees do not not store all keys in the leaf nodes? (a) B+-trees (b) (original) B-trees 2. Assume a B-tree (yes, we mean B+-tree!) where each node/leaf can contain up to 1000 keys is used to index a set of 1,002,001,000 ordered keys. All nodes and leaves of the B-tree are fully occupied. If the root is always kept in main memory, how many disk accesses are needed at most to ﬁnd any key? (a) 20 (b) 2 142 Indexes (c) 3 (d) 4 3. Which of the following trees keeps its leaf nodes in a double-linked list for easier range queries? (a) B+-trees (b) (original) B-trees 4. When looking for a key in a B-tree (yes, we mean B+-tree!), binary search may be used inside nodes to guide the search correctly. If the height of the B-tree is h and its branching factor (aka fan-out) is F , what is the number of key comparisons made in a search operation assuming all nodes are fully occupied in the worst case, roughly speaking? (a) F · h (b) h (c) logF (F \u0000 1) · h (d) log2(F \u0000 1) · h 3.2.2 B-tree insert, split, delete, merge Material Video: Original Slides: Inverted Slides: Additional Material Literature: [LÖ09], B+-tree [RG03], Section 10.5–10.6 Learning Goals and Content Summary How does a B-tree insert work?insert In order to insert a key/value-pair (k, v) into a B-tree, we ﬁrst execute a point query with key k.Like that we eventually ﬁnd the position in a leaf L where (k, v) belongs. If L still has room to store (k, v) ,we simply insert (k, v) into that leaf (preserving the order of key/value-pairs in L). If L does not have room to store (k, v) ,we need to either (1) move some of L’s entries to sibling leaves, i.e. we redistribute key/value-pairs such that L has room again to store (k, v).Notice that this operation may have to adjust pivots in parent nodes. Alternatively, (2) we split this leaf and then insert (k, v).split Why would we split a leaf or a node? To locally increase the amount of available storage in the B-tree. This is necessary if a leaf or node does not have room to store a pivot/pointer or key/value-pair, respectively. Notice that a split creates a new leaf or node. 3.2 B-trees 143 Summary: insert(74) -> Leaf Split before: after: 11 15 34 56 . . . . 67 72 73 83 . . . . 60 84 84 87 102 105 . . . . 11 15 34 56 . . . . 67 72 . . 60 73 84 84 87 102 105 . . . . 73 74 83 . . . Dienstag, 7. Juli 15 delete(73) -> merge() before: 11 15 34 56 . . . . 67 72 . . 60 73 84 84 87 102 105 . . . . 73 83 . . 11 15 34 56 . . . . 67 72 83 . . . 60 84 84 87 102 105 . . . . after: Dienstag, 7. Juli 15 Figure 3.4: split() is the inverse of merge() And how does this split work in principal? Asplit of aleaf Li simply adds another leaf Li+1 as a right sibling to Li. In addition, a pivot element and a pointer to Li+1 needs to be added to the parent P of Li.he pivot element should be chosen such that the entries are evenly distributed over Li and Li+1. If P does not have room to store that pivot and pointer, we ﬁrst have to split P to make room, i.e. the split operation continues recursively up the tree. If in that process we need to split the root of the B-tree, we create a new root node pointing to the old root node and its newly created sibling. This also implies that a split of the root is the only possibility to increase the height of a B-tree. All other split operations make the tree wider but not higher; only a root node split makes the tree higher. A split of a node Ni works similar to a leaf split. We simply add another node Ni+1 as a right sibling to Ni. In addition, 144 Indexes BTree.insert(key, value) Leaf.insert (key, value):\t \r If this.keys ≥ 2k*:\r //check overﬂow condition \t\t (newLeaf, newLeafPivot) = split();\t //split this Leaf \t\t If key < newLeafPivot:\t //check into which Leaf to insert \t\t \t this.insertEntry(key, value);\t //insert into this (old) Leaf \r\r Else:\r\r //i.e., key ≥ newLeafPivot \t\t \t newLeaf.insertEntry(key, value);\t //insert into newly created Leaf \t\t return (newLeaf, newLeafPivot);\t //return split information to parent \t Else: \t\t \t //no split necessary for this Leaf \t\t this.insert(key, value);\t //insert into newly created Leaf \t\t return (null, null);\t //signal no split to parent AbstractNode Node Leaf Dienstag, 7. Juli 15 BTree.insert(key, value) Node.insert(key, value): \t AbstractNode subtree = this.choose_subtree(key);\t //ﬁnd subtree to follow \t (newChildNode, newChildPivot) = subtree.insert(key, value);\t //route insert-operation to subtree \t If newChildNode != null:\t //did a split occur in the child? \r\r If this.keys ≥ 2k:\r //check overﬂow condition of this Node \t\t \t (newNode, newNodePivot) = this.split();\t //split this Node \t\t \t If newChildPivot < newNodePivot:\t //check into which Node to insert \t\t \t \t this.insertEntry(newChildNode, newChildPivot);\t //insert into this (old) node \r\r \r Else:\r\r //i.e., newChildPivot ≥ newNodePivot\r \t\t \t \t newNode.insertEntry(newChildNode, newChildPivot);\t //insert into newly created node \t\t \t return (newNode, newNodePivot);\t //return split information to parent \t\t Else:\t\t //no split necessary for this Node \t\t \t this.insertEntry(newChildNode, newChildPivot);\t //insert new child into this node \t\t \t return (null, null); \t //signal no split to parent \t Else:\t\t \t //i.e., no split in child \t\t return (null, null); \t //signal no split to parent AbstractNode Node Leaf Dienstag, 7. Juli 15 Figure 3.5: inserting into a leaf vs inserting into a node apivotelementand apointerto Ni+1 needs to be added to the parent P of Ni. If P overﬂows, we have to split it recursively. How do we implement the split operation in an object-oriented programming language? The split-operation may be implemented elegantly by exploiting polymorphism. Intro- duce an AbstractNode either as an interface or as an abstract class. AbstractNode deﬁnes the signatures for the insert and split methods. Introduce two classes, Node and Leaf, both inheriting/implementing from AbstractNode. Like that walking down the tree in an insert operation can be implemented without knowing in the implementation whether the child pointed to is a node or a leaf. In addition, return-values of the split-method may be used to signal whether a split occurred. This allows you to easily detect whether any node visited when walking down has to be split. How do we delete data from a B-tree and its leaves and nodes?delete 3.2 B-trees 145 5 11 13 17 13 3 25 26 2 54 54 584 47 78 81 82 87 7 95 9 99 93 2819 21 Figure 3.6: Initial B-tree. Adelete operation in aB-tree can be considered the inverse of an insertoperation. When deleting a key/value-pair from a leaf, that leaf may underﬂow, i.e. we violate the constraint that a leaf (if it is not the root) should have n 2 [k⇤,. .., 2k⇤] key/value-pairs. In order not to violate the constraint we may therefore merge this leaf with a sibling leaf. In that merge process we also remove a pivot and a pointer from the parent node. In turn the parent may underﬂow as well. Then we need to merge the parent with one of its siblings. This merging continues recursively up the tree until we reach the root. If we try to remove the penultimate pointer from the root, we even remove the root. So again, and inverse to adding a level during insert, we can only remove a level when merging. How are merge() and split() related? merge() is the inverse of split(). Quizzes 1. When a node n has to be split in a B-tree due to an insertion operation, which of the keys of n becomes the pivot? (a) The smallest key found in n (b) The median of the keys found in n (c) The largest of the keys found in n 2. Which of the following paradigms describes best the insertion operation in a B-tree? (a) Top-down (b) Bottom-up Exercise For each (a)–(f) consider the B +-tree of Figure 3.6 as the initial state to answer the questions below. Perform the operations speciﬁed and draw the resulting B+-tree. (a) Insert a data entry with key 49. (b) Insert a data entry with key 23. How many page reads and page writes does the insertion require assuming that at the beginning of the operation no page is available in the DB-buﬀer? 146 Indexes (c) Insert a data entry with key 90. How many page reads and page writes does the insertion require assuming that at the beginning of the operation no page is available in the DB-buﬀer? (d) Delete the data entry with key 54, assuming that the left sibling is checked for a possible node merge. (e) Insert 31 into the initial tree and then remove, from the resulting tree, the data entry with key 42. Assume again that the left sibling is checked ﬁrst for a possible node merge. (f) Successively delete the data entries with key 5 and 11. 3.2.3 Bulk-loading B-trees or other Tree-structured Indexes Material Video: Original Slides: Inverted Slides: Additional Material Literature: [RG03], Section 10.8.2 Further Reading: [dBS01] Learning Goals and Content Summary How do we bulkload aB-tree?bulkload There are many algorithms for bulkloading tree-structured indexes and in particular B- trees. An intuition for a simple and very eﬃcient method is to: (1) sort the data w.r.t. the key attribute(s) to use for the B-tree. (2) Construct a B-tree from left to right and bottom to top. In more detail, insert entries of the sorted data into a newly created leaf. When that leaf is full, create a second leaf and insert pointers to both leaves into a newly created parent node. Keep on adding leaves until the parent node is full. When it is full, add a new parent node, i.e. a new root on the same level as that parent. In addition, create aroot new node pointing to both the old and the new parent node. And so forth. This method gives you the intuition, but it does not always create a valid B-tree. Whether the resulting tree is a valid tree depends on the amount of data inserted. In particular, this method may create a forest of trees or even underfull nodes and leaves. Fixing this is, however, rather simple and can be done in several ways. For the details, see the exercises. What is bulkloading useful for? Basically, in order to create a new B-tree on an already existing table, we have three options: (1) initialize an empty tree, insert tuples one by one (aka tuple-wise insert),tuple-wise insert 3.2 B-trees 147 (2a) bulkload the index as described above or (2b) sort data w.r.t. the key attribute(s) to use for the B-tree and then continue with option (1). Option (1) is relatively expensive as every insert of a tuple into the tree performs an entire walk down the tree, possibly followed by one (or more) split(s). Option (2a) is cheaper as the tree is built from left to right. Still the tree grows by splitting nodes and leaves. In addition, as the input data is sorted, leaves and nodes will only be half full (except the last nodes and leaves in each level). Option (2b) is the cheapest method as no splits are performed. In addition, the entire tree may be written out in a single (streamed) sequential write operation. There is no need to perform random I/O-operations. What should be kept in mind for the free space in nodes and leaves when bulkloading? We can freely adjust how much free space we keep in nodes and leaves. Leaving some room in the nodes and leaves is useful in order to allow for eﬃcient future inserts. If we keep too much free space, we waste space, but decrease the likelihood of splits. If we do not keep any free space at all, we do not waste space, but increase the likelihood of splits. So we need to balance space with insert performance. Quizzes 1. Given a dataset with 20 tuples and a B-tree that can store three keys in the nodes and ﬁve keys per leaf. What do you need to do in order to bulk-load an index with those tuples? (a) always sort the data on the key (b) always sort on the very ﬁrst attribute (c) create four leaves (d) create seven leaves (e) create a single node as the root node (f) create a root node together with two nodes Exercise Assume a B-tree residing entirely in main memory. You are allowed to use a node size of up to 64Bytes, i.e. the size of a cache line. Assume that the type of the key as well as the value is a 4Byte int each and the CPU architecture is 32 Bits. Further assume that the number of entries to store in the tree is N=39304 key/value-mappings. Assume that the tree was bulkloaded. (a) What is the fan-out F of the internal nodes? (b) What is the height h of the tree? (c) How many cache misses CM do you expect in the worst case to fetch a value? (d) What is the size S of the tree in Bytes? 148 Indexes (e) Bonus: so the entire tree ﬁts into main memory. Can you come up with an alternative B-tree that improves ALL four variables F, h, CM, and S? Exercise Assume you have a set of entries with the following key values: 4, 9, 5, 8, 13, 16, 10, 27, 21, 1, 3, 43, 39, 15, 2, 23, 42, 40. Explain how to bulk-load this set of entries into a new B +-tree. Each index-level page can hold up to 2 key values and each leaf page can hold up to 3 records. Draw the resulting B+-tree for each of the steps, i.e. each leaf/inner node added, of the bulk-loading process. Exercise Assume you are getting a sorted stream of data pages (with unique keys). You have to build a sparse B +-Tree with at most 4 keys per inner node and 4 keys per leaf node. None of the inner nodes is allowed to have less than two keys — except the root node. (a) Assume you insert 6 data pages one at a time into an empty tree. What does the tree look like in the end? Draw the structure of the tree. (b) What is the height of the tree if you simply insert n data pages one at a time? (c) Provide an algorithm to create a compact B+-tree, where compact means that you create a tree of minimal height. Your algorithm should denote when a node is ﬂushed to disk. You should minimize the number of inner nodes you keep in memory and ﬂush inner nodes as early as possible to disk. On the other hand you should never overwrite an already ﬂushed inner node. Implement two methods: void insert(DataPage d) and void close() using Pseudo-Code or Java. When close() is called, then you must make sure that a valid B-tree is persisted (ﬂushed) on disk. 3.2.4 Clustered, Unclustered, Dense, Sparse, Coarse-Granular Index Material Video: Original Slides: Inverted Slides: Additional Material Literature: [LÖ09], Dense Index [LÖ09], Sparse Index [RG03], Sections 8.2.1 and 8.5.2 3.2 B-trees 149 60 84 Clustered Index 11 15 34 56 67 72 73 83 84 87 102 105 (11,z,gh,...) (15,a,mi,...) (34,g,di,...) (56,d,jo,...) (67,z,sh,...) (72,a,mi-mo,...) (73,d,hn,...) (83,z,th,...) (84,d,so,...) (87,z,rt,...) (102,z,ik,...) (105,d,hn,...) data pages index leave pages index node pages 60 84 Unclustered Index 11 15 34 56 67 72 73 83 84 87 102 105 (105,d,hn,...) (102,z,ik,...) (67,z,sh,...) (72,a,mi-mo,...) (11,z,gh,...) (83,z,th,...) (87,z,rt,...) (84,d,so,...) (56,d,jo,...) (73,d,hn,...) (15,a,mi,...) (34,g,di,...) data pages index leave pages index node pages Figure 3.7: Clustered vs unclustered index 60 84 Dense Index 11 15 34 56 67 72 73 83 84 87 102 105 (11,z,gh,...) (15,a,mi,...) (34,g,di,...) (56,d,jo,...) (67,z,sh,...) (72,a,mi-mo,...) (73,d,hn,...) (83,z,th,...) (84,d,so,...) (87,z,rt,...) (102,z,ik,...) (105,d,hn,...) data pages index leave pages index node pages Sparse Index 11 67 83 87 (11,z,gh,...) (15,a,mi,...) (34,g,di,...) (56,d,jo,...) (67,z,sh,...) (72,a,mi-mo,...) (73,d,hn,...) (83,z,th,...) (84,d,so,...) (87,z,rt,...) (102,z,ik,...) (105,d,hn,...) data pages index leave pages Figure 3.8: Dense vs sparse index Coarse-Granular Index 11 83 (11,z,gh,...) (15,a,mi,...) (34,g,di,...) (56,d,jo,...) (67,z,sh,...) (72,a,mi-mo,...) (73,d,hn,...) (83,z,th,...) (84,d,so,...) (87,z,rt,...) (102,z,ik,...) (105,d,hn,...) data pages index leave pages No Index ;-) 11 (11,z,gh,...) (15,a,mi,...) (34,g,di,...) (56,d,jo,...) (67,z,sh,...) (72,a,mi-mo,...) (73,d,hn,...) (83,z,th,...) (84,d,so,...) (87,z,rt,...) (102,z,ik,...) (105,d,hn,...) data pages index leave pages Figure 3.9: Coarse-granular vs no index Learning Goals and Content Summary What is a clustered index? clustered index In a clustered index the sort order of keys in the leaves corresponds to the sort order of tuples on the data pages. This implies that a range query can be answered by: (1) execute apointquery, (2)visitthe datapagepointed to, and (3) continuescanningalongthe data pages (rather than the leaves). In other words, the index sequential access is done on the data pages rather than on the leaves (compare Section 3.2.1). Still ISAM on the leaf-level may be useful to execute EXISTS queries, i.e. to compute the keys that exist in aparticularinterval. How many clustered indexes are possible for a table? In the general case, i.e. if the attributes of the table are not correlated, only one clustered 150 Indexes index may be created as sorting the table on one attribute will destroy the sort order on all other attributes. If some columns are correlated, multiple clustered indexes may be created (though few DBMS support this). If we combine replication with clustered indexes, we may create multiple clustered indexes even when columns are not correlated, e.g. keep the table in two sort orders and create diﬀerent clustered indexes on these tables. What is an unclustered index?unclustered index In an unclustered index the sort order of keys in the leaves does not necessarily corresponds to the sort order of tuples on the data pages. This implies that a range query can be answered only at relatively high costs: (1) execute a point query, (2) for each entry in a leaf visit the data page pointed to, and (3) continue with ISAM on the leaf-level and for each entry execute (2) until the end of the range is found. In other words, the index sequential access is done on the leaves of the B-tree and every entry triggers a (possibly random) lookup to a page in the data store. Similar to clustered indexes, ISAM on the leaf-level may still be useful to execute EXISTS queries, i.e. to compute the keys that exist in a particular interval. How many unclustered indexes are possible for a table? You may create as many unclustered indexes as you want. But remember that all indexes have to be maintained whenever the underlying table is changed through inserts, updates, and deletes. Thus, the more indexes you create on a table the higher are the costs for inserts, updates, and deletes. There are two extremes in indexing: on read-only data the number of indexes is just limited by storage space. In contrast, on write-intensive data, the number of indexes is also limited by the index maintenance costs. To get optimal performance, it is important to ﬁnd the right balance. What is a dense index?dense index Adense indexhas one entryon the leaf-levelforeach rowin the table. What is a sparse index?sparse index In contrast to a dense index, a sparse index does not have an entry for every row in the table. Typically, a sparse index only has one entry on the leaf-level for each data page in the table. This implies that leaves cannot (always) answer EXISTS queries anymore, i.e. even though a key is not available in the leaf, it may still exist on a data page. In addition, in a sparse index, leaves play a similar role as nodes, as entries in a leaf mark key ranges in data pages. For instance, if the leaf contains two keys, say 67 and 83 (see Figure 3.8), the value pointed to from key 67 points to a data page containing keys in range [67; 83[. What are the major advantages of a sparse index? A sparse index has fewer entries. Therefore it needs less space. In addition, it is likely to have fewer levels. It is also easier to maintain, i.e. only if the range of the data kept in a data page is changed, the sparse index has to be adjusted accordingly. How is a coarse-granular index related to a sparse index? coarse-granular index “Sparsity” may be deﬁned in several diﬀerent ways. As already stated above, typically, 3.2 B-trees 151 asparse indexonlyhas oneentryon the leaf-level foreach datapage in thetable. But what happens if we use even fewer index entries, say one entry for every two data pages? Then we evolve the index towards a coarse-granular index. Assume we use one index entry for every x pages. An index with x =1 is called sparse, an index with x> 1 is called a coarse granular index. So x is a parameter deﬁning the “sparsity” or “coarseness” of an index. Both sparse and coarse-granular indexes are both variants of what is called aﬁlter index. A ﬁlter index is an index that returns a superset of the actual result. That ﬁlter index superset then needs to be post-ﬁltered by inspecting the database. What are the major advantages of a coarse-granular index? Acoarse-granularindexhas even fewerentries than asparse index. Amajoradvantage of a coarse-granular index is that it is even cheaper to maintain than a sparse index. A drawback of a coarse-granular index is that (typically) a larger portion of data has to be scanned to answer a query. In general, sparse and coarse-granular indexes are beneﬁcial to index data that is frequently changed. For these cases a dense index may be to expensive to maintain, but a sparse or coarse-granular index may be aﬀordable. How is a sparse index related to having no index at all? Assume again we use one index entry for every x pages. Assume that y is the number of data pages that we want to index. Thus, the number of index entries is dy/xe. This implies that for x \u0000 y the number of index entries is exactly one, i.e. the granule is so big that all data pages belong to the same partition. This case has a similar eﬀect as not indexing the data in the ﬁrst place; it actually has some overhead over having no index in the sense that we pay the extra costs for traversing the index and then scanning all data anyway. Quizzes 1. In general, which kind of index is likely to have better performance in practice when doing range queries on the indexed attribute? (a) Dense unclustered index (b) Dense clustered index (c) Sparse unclustered index 2. Which of the following is a beneﬁt of sparse indexes? (a) Index size is small (b) The system can directly tell whether a key exists using the index. (c) The system can directly tell whether a value exists using the index. (d) If the attribute of a tuple which is used as the key in the index is changed, we sometimes do not have to change the index at all. 3. When the selectivity of a range query is very low (say 10 percent of the elements qualify), which option is likely to have better performance in practice for an unclus- tered index? 152 Indexes (a) A coarse-granular index with a granule of two pages (assuming you have thou- sands of pages) (b) Asimple scan of the underlyingtable beingindexed (c) Asparse index (d) Adense index 3.2 B-trees 153 3.2.5 Covering and Composite Index, Duplicates, Overﬂow Pages, Composite Keys Material Video: Original Slides: Inverted Slides: Additional Material Literature: [RG03], Section 10.7 Learning Goals and Content Summary 60 84 Unclustered Index 11 15 34 56 67 72 73 83 84 87 102 105 (105,d,hn,...) (102,z,ik,...) (67,z,sh,...) (72,a,mi-mo,...) (11,z,gh,...) (83,z,th,...) (87,z,rt,...) (84,d,so,...) (56,d,jo,...) (73,d,hn,...) (15,a,mi,...) (34,g,di,...) data pages index leave pages index node pages 60 84 Covering Index 11 15 34 56 z a g d 67 72 73 83 z a d z 84 87 102 105 d z z d (105,d,hn,...) (102,z,ik,...) (67,z,sh,...) (72,a,mi-mo,...) (11,z,gh,...) (83,z,th,...) (87,z,rt,...) (84,d,so,...) (56,d,jo,...) (73,d,hn,...) (15,a,mi,...) (34,g,di,...) data pages index leave pages index node pages Figure 3.10: Non-covering vs covering index 60,a 84,d Composite Index 11,z 15,a 34,g 56,d 67,z 72,a 73,d 83,z 84,d 87,z 102,z 105,z (105,d,hn,...) (102,z,ik,...) (67,z,sh,...) (72,a,mi-mo,...) (11,z,gh,...) (83,z,th,...) (87,z,rt,...) (84,d,so,...) (56,d,jo,...) (73,d,hn,...) (15,a,mi,...) (34,g,di,...) data pages index leave pages index node pages Duplicates: Overﬂow Pages 11 67 83 87 (11,z,gh,...) (11,a,mi,...) (11,g,di,...) (11,d,jo,...) (67,z,sh,...) (72,a,mi-mo,...) (73,d,hn,...) (83,z,th,...) (84,d,so,...) (87,z,rt,...) (102,z,ik,...) (105,d,hn,...) (11,b,zu,...) (11,k,dd,...) (11,c,tr,...) (67,x,cx,...) (67,e,sq,...) (67,y,qq,...) data pages index leave pages data pages Figure 3.11: Composite (key) index vs overﬂow pages What is a covering index? covering index In contrast to an unclustered index, a covering index stores additional attributes at the leaf-level — not only the key/value-pairs kept by an unclustered index. The advantage is that certain queries may be answered by considering the covered attribute(s) rather than looking up data on the data pages. For instance, assume the index maps from attribute A to rowIDs but additionally covers attribute B. Now, whenever we have a query SELECT A, B WHERE A>=72 AND A <= 87, we can compute the result to this query with an index only plan,i.e., we execute the range query on attribute A and for all index only plan entries qualifying we also output B. Compare this to an unclustered index where for every qualifying entry we have to lookup the data page (following the rowID) to lookup B’s 154 Indexes value, see also Figure 3.10. The disadvantage of covering is that attribute B is replicated: its values are stored both in the leaves and the data store. Covering indexes are also related to column layouts and vertical partitioning in general, as a scan along the leaf- level has similar advantages: rather than scanning large portions of a table in row-layout, we have to (potentially) scan less data. This may even be exploited in cases where the index does not help in evaluating the ﬁlter predicate, yet it may be exploited for a column scan on the attribute values that are covered in the index. How are covering indexes related to clustered indexes? Acoveringindexhas similareﬀects on the covered attributes as the clustered index has on the entire data pages pointed to. In other words, in a covering index the covered attributes are clustered w.r.t. the key used for indexing. In a clustered index, data pages are clustered w.r.t. the key used for indexing. So, on a high level, a covering index is a compromise in-between an unclustered and a clustered index. How many covering indexes can we create on a single table? In theory, as many as we want. The price we pay is additional storage space and update costs for the replicated attributes. Notice that the exact same answer can be given to determine how many clustered indexes to create. What is a composite index?composite index In a composite index, we deﬁne the key to be a composite of multiple attributes, see also Figure 3.11. Like that all attributes pertaining to the key are implicitly covered by the index. How is a composite index related to a covering index? We may use a composite index in order to mimic a covering index, compare Figures 3.10 and 3.11. This is useful when a covering index is not supported by the database system. However, notice that a composite index for each key keeps the entire key in nodes as the pivot element1.For a ﬁxed page size, this may potentially decrease the number of entries that can be kept in a node (parameter k). This reduces the fan-out of the nodes which in turn may lead to a tree with more levels and therefore may lead to slower queries. How do we handle duplicates in a B-tree?duplicates There are many ways to handle duplicates: (1) introduce overﬂow pages, i.e. allow leaf- entries to point to a list of data pages rather than a single data page, (2) use a composite key to convert an attribute with duplicates into a duplicate-free column (not necessarily a key), (3) implement an integrated tree with three diﬀerent node-types: nodes, leaves, and data pages (see discussion in the video for details). Quizzes 1. Assume the following query is performed: SELECT C FROM Table WHERE A = a2 AND b1 <= B <= b2. The selectivity of the query is 20 percent, data is stored on HDD, and the size of attribute C is rather small. What kind of index is most 1Which in turn could be optimized by just storing a preﬁx of the key in the nodes as a pivot. This is done in preﬁx B-trees which are a general method to shorten keys in nodes. 3.2 B-trees 155 beneﬁcial for answering this query? (a) Unclustered index on A (b) Unclustered index on B (c) Covering index on A that covers B and C (d) Composite index on A and B (e) Composite index on A and B that covers C as well 2. Assume an index is built using a composite key on a non-key attribute together with the primary key. How should this index handle duplicates? (a) Store all duplicates in overﬂow pages. (b) Not at all. 3. You would like to handle duplicates in a good manner in a B-tree, but unfortunately you have no access to the code of the B-tree. What method do you use then to handle duplicates? (a) Overﬂow pages (b) 3-node type B-tree (c) Composite key containing a key attribute as well (d) this is not possible to ﬁx without having access to the source code 4. Assume the ratio of duplicates to the total number of keys is very bad, i.e., there are a lot of duplicates in the data. Which of the following (B-tree) indexes has the smallest memory footprint? By index size we mean only the size of the B-tree (internal nodes and the leaves) (a) Using overﬂow pages (b) Using composite keys (c) 3-node type 156 Indexes 3.3 Performance Measurements in Computer Science Material Video: Original Slides: Inverted Slides: Additional Material Literature: [Jai91], Sections 2 and 3 [LÖ09], I/O Model of Computation Further Reading: [LÖ09], Performance Monitoring Tools WBig O Notation WSimulation Learning Goals and Content Summary How to determine which algorithm, index, or whatever is better? This is an easy question, however, it is not easy to answer. It depends on what is meant by “better”? Is “faster” the same as “better”? And then is it wall-clock time you are interested in? Or CPU time? Or is less memory consumption better? Less I/O? So before investigating how much “better” a particular method is, make sure you understand what “better” means to you. The same confusion exists with the term “performance”. What are the three ways to measure the performance of a computer program? In general there are three approaches to measure performance (or other properties) of a computer program. 1. Analytical Modeling: we use a mathematical model of the algorithm or system we want to examine. Notice that a model can be created in many diﬀerent ways. Models should always be built having a particular goal in mind: what do you want to do with that model? Which feature do we want to model? A “good” model is at the same time simple (it leaves away things we are not interested in in the analysis) and powerful (it allows us to realistically quantify performance). Once the model has been built, performance is determined by arguing along the model. Be careful not to confuse the model with the underlying reality. This mistake is, unfortunately, done frequently. 2. Simulation: we perform experiments with an analytical model of the algorithm or system. Hence, this method is a blend of analytical modeling and experiment. In contrast to analytical modeling, the focus is on executing the model rather than arguing along the model mathematically. In contrast to an experiment, we execute the model rather than the real system or algorithm. 3. Experiment: we run the actual algorithm and system. 3.3 Performance Measurements in Computer Science 157 (1a) Asymptotic Complexity, O-Notation 1 1 1 1 1 1 1 1 1 1 1 1 (1b.1) Cost Models 1 1 1 100 100 100 100 100 100 100 100 100 Figure 3.12: Asymptotic complexity vs cost models Notice that the boundaries among the diﬀerent methods are fuzzy. For instance, parts of asystem maybesimulated othersmaybe executed. O-Notation is good enough, right? O-Notation It is the root of one of the biggest confusions in computer science to solely reduce perfor- mance analysis to analytical modeling. As stated above, there are three diﬀerent methods to analyze performance. Each method has its pros and cons. Just using analytical model- ing, which typically argues along big O-notation, is often not helpful as analytical models tend to oversimplify. Therefore these models allow us only to draw relatively vague con- clusions, e.g. algorithm X is of complexity class Y. This is unfortunate in situations where many algorithms are in the same complexity class, but vary in wall-clock time by orders 158 Indexes sweat, dehydration strong winds worker has a bad day easy until here painful (2) Simulation run part of the actual algorithm or system simulate other parts problem: \t might still oversimplify reality \t might miss some important eﬀect from reality... Figure 3.13: Simulating a pyramid of magnitude. The latter situation is the 90% case in database systems. What is asymptotic complexity? asymptotic complexity Asymptotic complexity classiﬁes the growth rate of a function w.r.t. a parameter. The input parameter is typically the problem size, e.g. the size of a dataset. The function itself models the runtime or memory consumption of an algorithm. We use big O-notation to characterize a function’s growth rate. What is a cost model?cost model Acostmodelgoes beyond asymptotic complexityin thatwe donotonlydetermine the complexity class of an algorithm or function, but additionally try to compute a cost esti- mate. For instance rather than saying that an algorithm is in complexity class O(n log n), 3.3 Performance Measurements in Computer Science 159 easy till here sweat, dehydration worker has a bad day painful strong winds worker has a bad day strong winds sweat, dehydration painful easy till here Figure 3.14: Additional inﬂuence factors when simulating a pyramid we add constants, units, and terms of lower complexity. So we may come up with a cost formula like C(n)= 1 42 · log2(n)+3.2 · n +2.5 microseconds . Cost formulas are not only useful in performance analysis, but also to predict perfor- mance in a database system. Precise cost estimates are of utmost importance in cost-based optimization, see Section 5.2. What is a simulation? simulation In a simulation we execute a model of the system or algorithm. This is useful if an analytical model of the system is too complex to be argued upon mathematically. It is 160 Indexes (3) Experiment implement it run it measure it problems: \t lacking abstraction \t lacking bounds \t lacking theory \t lacking insight Analytical Modelling Simulation Experiment effort/cost realitymethod generalizability Figure 3.15: Experiments and the diﬀerent trade-oﬀs of modeling, simulation, and experiment also useful if (parts of) the algorithm/system do not exists (yet). In these situations we can also use a blend of a simulation and an experiment where some parts of the system are simulated (by executing a model) and others are actually run (by executing the real system or algorithm). What might be the problem of all of the former methods? Analytical modeling, cost models, and simulations may be considerably oversimplifying reality. They may still imply useful bounds and predictions. However, these predictions may still be too far oﬀ from reality to be useful or informative. This is not saying that modeling is bad in general. This is saying that any form of modeling needs to consider how much that particular model is able to reﬂect reality anyway and whether the results 3.3 Performance Measurements in Computer Science 161 are strong enough to come up with meaningful decisions in practice. What is an experiment? experiment In an experiment we run the actual algorithm and/or system. What could be the problem of an experiment? Experiments typically lack abstraction, i.e. the results observed from an experiment may only be valid for one dataset, on one machine, for one compiler, for one operating system, for one implementation for the system, and so forth. In other words, a possible problem of experiments is that they might be overﬁtting to a particular scenario. Therefore, we overﬁtting have to be careful when trying to generalize results from an experiment. How are the three ways to measure the performance of a computer program correlated to eﬀort/cost, reality, and generalizability? In general, and with notable exceptions in all cases,all methods have diﬀerent trade-oﬀs w.r.t. eﬀort/cost, abstraction from reality, and generalizability, see Figure 3.15. 1. eﬀort/costs: analytical modeling is the cheapest method with the least eﬀort. In contrast, simulations are more complex. The most expensive method is an actual experiment (if we include the costs for creating the algorithm/system). 2. reality: how much do the diﬀerent methods abstract from reality or in other words: how closely can a method predict the behavior of an algorithm/system in reality? Analytical modeling is the most abstract method, a simulation is somewhat less abstract. Experiments are the least abstract method — they are “down-to-earth”. 3. generalizability: this can be considered the inverse of overﬁtting. Obviously, the more abstract a method the higher the generalizability. Therefore, generalizability is high for analytical modeling, it is lower for simulations, and it it is typically low for experiments. Again, this is a high-level view on the diﬀerent trade-oﬀs of the three principal methods in performance analysis. There are notable exceptions for all cases. Quizzes 1. Dr. No has an algorithm that performs n!/2 operations in total, where n is the size of the input. He then runs his algorithm on an input set of size n = 25.If his computer can perform 10 9 operations per second, is Dr. No going to be happy when his algorithm ﬁnishes? (a) Deﬁnitely yes. (b) We have to ask his grandmchildren. 2. Dr. No also has an algorithm for a certain problem that runs in O(2n3/4 log2 n) time. The current best algorithm for that problem runs in O(2 n) time. Assume that the multiplicative constants hidden by the O-notation are all 1. Theoretically speaking, 162 Indexes Dr. No’s algorithm is a breakthrough. Practically speaking, at which input size n> 3 does Dr. No’s algorithm start becoming relevant? (a) 2 (b) 256 (c) 8192 (d) 16384 (e) 65536 3. Assume you have the following query: SELECT SUM(A) WHERE A BETWEEN aANDb, and you knowthatitsselectivityisextremelylow(manytuplesqualify). What kind of gain do you expect to obtain using an index over column A for answering the query in contrast to answering the query by simply doing a linear scan? (a) Negligible gain (b) Considerable gain 4. Assume you have the following query: SELECT SUM(A) WHERE B BETWEEN aANDb, and you knowthatitsselectivityisextremelylow(manytuplesqualify). What kind of gain do you expect to obtain using an index over column A for answering the query in contrast to answering the query by simply doing a linear scan? (a) Negligible gain (b) Considerable gain 5. You manage to parallelize a certain algorithm, yet, when using k threads you do not observe the desired k-fold increase in speed (execution time) w.r.t. the single- threaded version of the algorithm. What could be some of the factors that hinder the scaling of your parallel algorithm? (a) Cache misses (b) Remote memory accesses (c) Memory bandwidth (d) Room temperature 6. You manage to come up with an analytical model for the number of cache misses in acertain datastructure. Moreover,you proved mathematically thatyourmodelis atightlower bound, i.e. forallinputsyou expectatleastthatmanycachemisses. In addition, you also validate your model using an experiment. However, in that experiment, you observe that there are sometimes less cache misses than your tight lower bound predicted. So in summary: the results from the experiment contradict your analytical model! What could be some explanations for the discrepancies between theory and practice? 3.4 Static Hashing, Array vs Hash, Collisions, Overﬂow Chains, Rehash 163 (a) Your model does not consider the eﬀect of memory hierarchy in its computa- tions. (b) Your model does not consider the eﬀects of spatial locality. (c) The processor of the machine has turbo boost enabled. (d) The input is highly skewed and your model assumed perfect uniform distribu- tion for the input. 3.4 Static Hashing, Array vs Hash, Collisions, Overﬂow Chains, Rehash Material Video: Original Slides: Inverted Slides: Additional Material Literature: [LÖ09], Hash Functions [LÖ09], Hash-based Indexing [CSRL09], Section 11 [RG03], Section 8.3.1 and 11.1 [RAD15] Further Reading: Whashing comparison Wlocality-sensitive hashing Learning Goals and Content Summary Why do some people say that the three most important techniques in computer science are hashing, hashing, and hashing? hashing Hashing is a central method in various areas of computer science with several important applications. Applications include checksumming, duplicate detection, similarity search, encryption, and pseudo-randomization (e.g. pseudo-random number generators). In the context of databases, hashing is a building block for query processing in particular for point queries, but it can also be used to a limited degree for range queries. Further appli- cations of hashing in databases include join processing (see Section 4.1.2) and grouping (see Section 4.2). What is the core idea of hashing? The core idea of hashing is to map keys from a relatively large input domain DI = {0,. .., N } to a smaller output domain DO = {0,. .., M } where M<< N .Once the keys have been mapped, they may be stored in very compact data structures, e.g. an array with only M slots. The mapping is done using a hash function h() : DI 7! DO.A design hash function goal of most hash functions is to redistribute, i.e. decluster keys from DI uniformly decluster over DO.However, hash functions may also be used to cluster similar elements like in cluster locality-sensitive hashing (LSH) [GIM99]. locality-sensitive hashing LSH 164 Indexes insert 3917564 bucket M-1bucket 77 DataItem ID=424342 ... slot 424342 h(3917564) = 6 slot 3917564 bucket 6 DataItem ID= 3917564 ... slot 0 slot N-1 bucket 0 Collisions? insert 3456789 slot 424342 slot 3456789 h(3456789) = 77 bucket 77 DataItem ID=424342 ... h(424342) = 77 bucket 6 DataItem ID= 3917564 ... slot 0 slot N-1 bucket 0 bucket M-1 collision Figure 3.16: Inserting into a hash table may lead to collisions How is a hash table organized? There are hundreds of diﬀerent hashing methods which vary in the hashing scheme they use, e.g. one vs multiple arrays, chained lists vs open addressing. They may also vary in the type of hash functions they use, e.g. multiplicative, Murmur. For an overview see the book by Cormen [CSRL09] or in the context of data management our own survey on hashing [RAD15]. In the video and in the following discussion we will focus on chained hashing. It is widely used in practice and also implemented in software libraries like STL and boost. In chained hashing the hash table is simply an array of M slots. Each slot pointschained hashing to a (possibly empty) list of entries pertaining to this slot. Whenever we want to insert 3.4 Static Hashing, Array vs Hash, Collisions, Overﬂow Chains, Rehash 165 Overﬂow Chains slot 424342 slot 3456789 collision bucket 77 DataItem ID=424342 ... DataItem ID= 3456789 ... h(424342) = 77 h(3456789) = 77 bucket 6 DataItem ID= 3917564 ... slot 0 slot N-1 bucket 0 bucket M-1 Figure 3.17: Handling collisions using overﬂow chains a (key, value)-pair into the hash table, we compute the hash value of the key, i.e. hv = h(key).Then we append (key, value) to the list at slot hv.Notice that slot and bucket bucket are used synonymously. In order to lookup entries in a chained hash table having a particular key,again we compute hv = h(key). Then, we perform a linear search in the list found at hash bucket hv. It is important to perform this post-ﬁltering step as, due to collisions, the diﬀerent entries found in the list at slot hv may have diﬀerent keys. Notice that chained hashing can be outperformed by an order of magnitude by other hashing methods [RAD15]. What is the runtime complexity of hashing? For chained hashing, looking up a particular slot is on the order of O(1).Finding a particular item in a chain depends on the number of elements in the chain found at that slot. For a slot of length k it is on the order of O(k). In general, many parameters aﬀect the runtime of hashing. For instance, the execution costs of the hash function and the load factor of the hash table. Hashing is yet another good example of a method where aperformanceanalysissolelybased on runtimecomplexitycan behighlymisleading, see [RAD15]. What is a collision? collision A collision occurs when two diﬀerent keys are mapped to the same value. In other words, given a hash function h() : DI 7! DO and two keys k1,k2 2 DI where k1 6= k2. If h(k1)= h(k2),we call this a collision: “keys k1 and k2 collide in the same slot”. A good hash function should create as little collisions as possible. But obviously, as DI is larger than DO )9k1,k2 2 DI where h(k1)= h(k2). In other words, even if DI is larger than DO by only a single element, a collision is inescapable. See also Figure 3.17 for an example. 166 Indexes How can we handle collisions? In chained hashing each slot stores a pointer to a list of values hashing to that slot. Hence, collisions are handled by appending them to the list available at the bucket pointed to. Why would we rehash?rehash In general, we rehash if we need to increase (or decrease) the hash table. This may be suitable in cases where many slots are non-empty and/or the chains pointed to get to long (which deteriorates search time). Yet, rehashing is typically relatively expensive, i.e. we have to insert all existing entries into a new hash table (of larger size). This has costs on the order of O(n). This investment should be amortized over several future inserts and/or lookups. So, now I know enough about hashing? Most probably not. This video and this chapter are just a very gentle introduction to the core ideas of hashing. The diﬀerent hashing techniques are a topic in itself and typically part of separate textbooks on algorithms and data structures or taught in undergrad courses. Make sure you read the book by Cormen et.al. [CSRL09]. For a performance shootout of diﬀerent hash tables in the context of query processing see [RAD15]. Quizzes 1. You have to make a decision about what data structure to use for a workload heavily based on range queries. You have no other knowledge about this workload. Which data structure would you use in general? (a) Hash table (b) B-tree (c) We should rather use table scan, than a data structure 2. Is it true that using hash functions over primary key attributes, i.e. no duplicates, results in a collision-free hash table? (a) Yes (b) No Exercise Let’s assume a hashing-method that does not keep a list (an overﬂow chain) for each bucket like in the video, but rather keeps all entries in a single array of size m where each array slot corresponds to a hash bucket. If at any time we try to insert an element x into this hash-table and it turns out that the slot hashed to, say slot 0  i<m is already occupied, we inspect slots i +1 through m \u0000 1 and then 0 through i \u0000 1 until we ﬁnd an empty slot to insert x to. (a) Provide pseudocode for remove(int key). (b) Provide pseudocode for get(int key) 3.5 Bitmaps 167 (c) Discuss: what are possible problems of this approach compared to hashing with overﬂow-chains? (d) Assume you change your insert strategy as follows: Given a function D(z):= 8 < :i \u0000 h(z) i \u0000 h(z), i \u0000 h(z)+ m. i < h(z). This function computes the distance of an element z from its ideal hash bucket. Now, whenever during an insert of key x you inspect a bucket i that already contains a key y and D(y) <D(x),then you insert x into that bucket and keep on probing the next bucket using y. Discuss: what are possible advantages of this change? 3.5 Bitmaps 3.5.1 Value Bitmaps Material Video: Original Slides: Inverted Slides: Additional Material Literature: [LÖ09], Bitmap Index Further Reading: [LÖ09], Bitmap-based Index Structures Learning Goals and Content Summary What is the core idea of a bitmap (aka value bitmap)? bitmap value bitmapAssume a table foo and a column foo.c.Further assume that foo contains |foo| = N rows and column foo.c has D distinct entries. The core idea of a bitmap is to keep one bitlist for each of the D distinct values in foo.c (D is called the cardinality of that bitlist column). Each bitlist has N bits. Thus, in total, we need D · N bits for the entire bitmap. Each bitlist Di represents one distinct data value i of column foo.c. If Di[x] is true, this means that the data value of the xth row in column foo.c is i. If Di[x] is false, this means that the data value of the xth row in column foo.c is not equal i.Obviously, if each entry in column foo.c may only represent a single value (i.e. we follow the ﬁrst normal form), it holds 8x if 9y where Di[y]= true, then 8z6=yDi[z]= false, i.e. for a particular row x only one of the bits in the diﬀerent bitlists may be set. What is the size of an uncompressed bitmap? uncompressed bitmap The size of an uncompressed bitmap is D · N bits. Hence, for large D and/or N the bitmap may become too large to be suitable in practice. What are typical bitmap operations and why are they very eﬃcient? Typical bitmap operations evaluate a boolean condition on the diﬀerent bitlists belonging 168 Indexes Bitmap Index on City ColleaguesColleaguesColleagues name street city peter unistreet new york steve macstreet cuppertino mike longstreet berlin tim unistreet berlin hans msstreet new york jens longstreet cuppertino frank unistreet new york olaf macstreet berlin stefan longstreet berlin alekh unistreet berlin felix macstreet new york jorge longstreet berlin citycitycity new york cuppertino berlin 1 0 0 0 1 0 0 0 1 0 0 1 1 0 0 0 1 0 1 0 0 0 0 1 0 0 1 0 0 1 1 0 0 0 0 1 Figure 3.18: A (value) bitmap on attribute city of table Colleagues to the bitmap. For instance, in the video on table colleagues we create a bitmap on column colleagues.city. Now, if we search for all rows where the colleagues.city is equal to “berlin” we may simply inspect bitlist Dberlin which already marks the qualifying rows. Similarly, conjuncts like colleagues.city is equal to “berlin” or colleagues.city is equal to “new york” may be evaluated by ORing bitlists Dberlin and Dnew york. If bitmaps were created for multiple columns on that table, say colleagues.city and colleagues.street,we may also evaluate conditions like colleagues.city is equal to “berlin” AND colleagues.street is equal to “long street” by ANDing bitlist Dberlin of column colleagues.city with bitlist Dlong street of column colleagues.street. How is the cardinality of a column related to the size of the bitmap? The cardinality D of the column directly aﬀects the total size of the bitmap index D · N . Recall, that we do not need to create bitlists for values that are not represented in the column, i.e. we do not need to create a bitlist for a value i where 8xDi[x]= false. What are applications of bitmaps? In general, bitmaps are useful for large datasets where queries contain several complex boolean conditions. Bitmaps are often used in Data Warehousing and OLAP. Bitmaps may also be used across diﬀerent tables as a bitmap join index.bitmap join index Quizzes 1. What is the smallest addressable unit of memory in a computer? (a) One bit (b) One byte (c) One megabyte 3.5 Bitmaps 169 Decomposed Bitmap ColleaguesColleaguesColleagues name street city peter unistreet 653 steve macstreet 42 mike longstreet 7 tim unistreet 7 hans msstreet 653 jens longstreet 42 frank unistreet 653 olaf macstreet 7 stefan longstreet 7 alekh unistreet 7 felix macstreet 653 jorge longstreet 7 michael unistreet 9 albert macstreet 323 volker longstreet 88 city 0 1 2 3 4 5 6 7 8 9 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 2 3 4 5 6 7 8 9 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 2 3 4 5 6 7 8 9 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 ×1×10×100 Figure 3.19: A decomposed (value) bitmap on attribute city of table Colleagues 2. One of the reasons for using bitmaps is to save space. How would you represent (in acomputer) abitmap for N> 0 elements? (a) Array of N of type boolean (in C++ or Java) (b) Array of N of type character (c) Array of dN/8e characters, assuming the size of the character type used is 8 bits 3.5.2 Decomposed Bitmaps Material Video: Original Slides: Inverted Slides: Learning Goals and Content Summary What is the core idea of a decomposed bitmap? decomposed bitmap In a value bitmap, as explained above, we learned that a single value is represented by one particular bitlist, i.e., if we want to know whether for row x the value of an attribute is v, we need to check whether Dv[x] is set to true. In contrast, in a decomposed bitmap, a value v is represented by a linear combination of bitlists, i.e., if we want to know whether for row x the value of an attribute is set to v, we need to check whether a predicate p(x, v) is set to true. Here we assume that p(x, v) is a linear combination of multiple bitlists. The decomposed bitmap variant explained in the video is deﬁned for values in range [0; 1000[, it uses 10 bitlists for each of the three decimals 100, 101,and 102,i.e. 30 bitlists in total. Assume that vj for 0  j  2 returns the jth decimal of v,i.e. vj =(v mod 10j+1) div 10j. 170 Indexes Let’s further assume that a bitlist of decimal j is referred to as Dj,v[x]. Then, we can use the following predicate: p(x, v)= D2,v2[x] AND D1,v1[x] AND D0,v0[x]. For instance, in the example in the video, in order to compute the rows where city=653, we need to inspect at most three bitlists D2,6[x], D1,5[x],and D0,3[x].Only if all three bits are set, we know that in row x the value v is equal to 653. What does this imply for bitmap operations? In order to compute p(x, v) we may need to inspect multiple bitlists. Hence, we may need more I/O at query time. What do we gain in terms of storage space? Depending on the cardinality D of the column to index, we may need less space. Consider again an uncompressed bitmap with D diﬀerent values and N diﬀerent rows. For such an uncompressed bitmap we require D · N bits. In contrast, for a decomposed bitmap we require: dlog10(D)e · 10 · N bits. For an arbitrary base b,we require: dlogb(D)e · b · N bits. Obviously, an uncompressed bitmap can be seen as an extreme case where b = D: dlogD(D)e · D · N bits = D · N bits. Notice that this computation also holds for cases where the domain is not dense. For instance, et’s assume that the domain contains only three values 42, 578, and 653 only. In this case we still have a cardinality of D =3.However, we need an extra remapping step (through a dictionary) which remaps these three values to 0, 1, and 2. The latter, dense, value sequence can be eﬃciently represented by a decomposed bitmap, the former, sparse, value sequence 42, 578, and 653 not. In order to determine whether a decompressed bitmap needs less storage than an uncompressed bitmap, we need to resolve: dlogb(D)e · b · N< D · N ,dlogb(D)e · b<D. In other words, if the number of digits required to represent the domain times the base is smaller than the domain, a decomposed bitmap pays oﬀ storage-wise. OK, but why not use domain encoding anyways? Then we will always win in terms of storage space! Domain encoding is less eﬃcient! This is true in terms of the compression ratio, however, the beneﬁt of bitmap decomposi- tion is not only the compression ratio. It is true that domain encoding requires dlog2(D)e 3.5 Bitmaps 171 bits which is more than the B ⇥dlogB(D)e bits (actually the leading group does not need all B bitlists in the general case), B being the base of the decomposition, required by bitmap decomposition. So, in terms of compression ration, domain encoding clearly wins. However, the goal here is to strike a balance between bitmap-style access at query time and compression ratio. The encoding used in bitmap decomposition is also called a K-of-N encoding [WLO+85]. This means, for each group of bitlists, exactly K out of N bits are set, in this case K =1 and N = B. This implies that for the example used in the video, where D = 1000, B = 10,and hence three groups of 10 bitlists each,for any point query, we only need to scan three bitlists in order to identify the qualifying rows. In contrast, in domain encoding, we would need to scan all bits (10 in this case), as we cannot make assumptions on how many (and which) of the other bits are set. For large tables, and depending on the parameters, this may make quite a diﬀerence. For the example this is already a factor 3.3 in terms of I/O (or memory bandwidth). Quizzes 1. Assume there is an attribute in a table with n tuples that you know can contain 1000 diﬀerent values. However, only 15 diﬀerent values appear in that column. What is the space requirement in bytes of that attribute if you use regular bitmaps to represent it? (a) n · 1000 · sizeof (int)= n · 1000 · 4 bytes (b) n · 15 · sizeof (int)= n · 15 · 4 bytes (c) dn/8e · 15 · sizeof (char)= dn/8e · 15 bytes 3.5.3 Word-Aligned Hybrid Bitmaps (WAH) Material Video: Original Slides: Inverted Slides: Additional Material Literature: Wjavaewah Further Reading: [WOS06] Learning Goals and Content Summary What is the relationship of WAH to RLE and 7-Bit encoding? RLE 7-BitWAH can be considered a combination of two methods: (1) run-length encoding, see Section 2.4.3, and (2) x-Bit encoding, where x =(c · 8) \u0000 1,c \u0000 1,see Section 2.4.4. How does word-aligned hybrid bitmap (WAH) compression work? word-aligned hybrid bitmap WAHConsider an uncompressed input stream of x bits. In order to compress that stream, compresswe segment the input stream into rows (aka chunks) of x =63 bits. Again, as in 7-Bit encoding, x may be changed to x =(c · 8) \u0000 1,c \u0000 1.We simply need one bit less than 172 Indexes the number of bits in the segment to make this method work. Now, in the compressed output stream, we represent adjacent rows containing only 0s or only 1s by a special ﬁll word of 64 bits. For instance, if two adjacent input rows of 63ﬁll word bits contain only 0s, we may compress these two rows to a single ﬁll word of 64 bits. In other words, rather than storing 2 · 63 bits, we only store 64 bits. We set the additional leading bit of the 64 bit output word to mark that this is a ﬁll word, i.e. the leading bit 63 is set to true. In addition, bit 62 contains the ﬁll pattern fp 2 {0, 1} that is represented by this ﬁll word. In the example, as both rows contain only 0s, we set bit 62 to 0. The remaining bits of the 64 bit output word contain the number of repetitions of the pattern contained in bit 62 (the unit for counting is the number of 63 bit input words, not the number of bits in the input). In the example, bits 0 to 61 are set to 102 =210. All other input rows, i.e. rows that do not only contain 0s or 1s, are represented by a literal word of 64 bits. This means, we take an input row of 63 bits and add an additionalliteral word bit in front which is set to 0. The resulting 64 bit word is written to the output. How do we decompress?decompress Decompression is straightforward. Consider a WAH-compressed input stream of n 64 bit words. Recall, that each of the input words must either be a literal or a ﬁll word. We process each input word one by one: if the leading bit of an input word is set to 0, i.e. it is a literal word, we append the remaining 63 bits to the output. Otherwise, if the leading bit of an input word is set to 1, i.e. it is a ﬁll word, we append #repetitions·63 times the ﬁll pattern fp 2 {0, 1} to the output. How could WAH be improved? WAH may be improved in many ways: 1. the ﬁll pattern may be more complex, i.e. rather than using a single bit for the ﬁll pattern (bit 62) we may use multiple bits to represent more complex, e.g. alternating patterns. In turn, fewer repetitions may be represented by a ﬁll word. 2. a ﬁll pattern only makes sense in case of repetitions of adjacent rows. A 63 bit input word containing only 0s or 1s may also be represented by a literal without gaining or losing anything. Hence, in the ﬁll word we should store the number of repetitions as #repetitions - 2. Like that we increase the maximum number of repetitions that may be stored in a ﬁll word from 262 \u0000 1 to 2 62 +1. 3. we may parallelize the method easily. For both compression and decompression, we may simply chunk the (uncompressed or compressed) input stream and treat each chunk by a separate thread. See also [WOS06] for a discussion of possible improvements and variants of WAH. Quizzes 1. Do word-aligned hybrid bitmaps always take up less space than original bitmaps for the same data set? 3.5 Bitmaps 173 (a) yes (b) no 2. Suppose we have a 93-bit long bit list, only the ﬁrst bit is set to 0 (all other bits are set to 1), how many bits can you save if word-aligned hybrid bitmap is applied (the word size is 32 bits)? (a) 3. Assume we created a WAH bitmap of ﬁve 16-bit words where the signal bits of those 5wordsare0, 1, 0, 1, 0. Whatis thelargestpossibleuncompressed bitmap sizein bits we can get if we uncompress these 5 words assuming the word size is 16 bits? (a) 491535 (b) 491505 (c) 491520 (d) 491538 Exercise Assume a table with 992000 rows and a column with 992 diﬀerent values. You want to index this column using a bitmap index. Assume further that your word size is 32 bit. (a) How many bits do you need to store the bitmap index uncompressed? (b) What is the worst-case size of any WAH compressed bitlist of this bitmap? (c) Assume now that the values are distributed round-robin. How many words do you need to store all WAH compressed bitlists of this bitmap? (d) What is the best-case distribution of the values w.r.t. compressed size? How many words do you need to store all WAH compressed bitlists of this bitmap? 3.5.4 Range-Encoded Bitmaps Material Video: Original Slides: Inverted Slides: Learning Goals and Content Summary What is the major idea of a range-encoded bitmap? range-encoded bitmap The major idea of a range-encoded bitmap is to change the semantics of the bitlists. Recall the semantics of a value bitmap as explained in Section 3.5.1. In a value bitmap on column foo,if for row x the bitlist Dv[x] is set, this means that the attribute value of column foo in row x is equal to value v. In a range-encoded bitmap this is changed. Here, we keep bitlists of type Dv[x]. The semantics is changed to: If for row x the 174 Indexes Range-Encoded Bitmap Index on ‘Year of Birth‘ For each value,RID: \r\r if Table[ attribute ][ RID ] ≤ value: \t\t \t \t \t BitMap[ value ][ RID ] := 1 year of birthyear of birthyear of birthyear of birthyear of birthyear of birthyear of birthyear of birthyear of birth 1979 1980 1981 1982 1983 1984 1985 1986 1987 0 0 0 0 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 1 1 1 0 0 0 1 1 1 1 1 1 0 0 0 0 1 1 1 1 1 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 ColleaguesColleaguesColleagues name street year of birth peter unistreet 1983 steve macstreet 1981 mike longstreet 1979 tim unistreet 1985 hans msstreet 1982 jens longstreet 1983 frank unistreet 1984 olaf macstreet 1985 stefan longstreet 1981 alekh unistreet 1980 felix macstreet 1987 jorge longstreet 1986 Figure 3.20: A range-encoded bitmap (value) bitmap on attribute year of birth of table Colleagues bitlist Dv[x] is set, this means that the attribute value of column foo in row x is less or equal to value v. How many bitlists do we need to represent a range-encoded bitmap? A range-encoded bitmap with a cardinality of D may be represented by D \u0000 1 diﬀerent bitlists. This is because, by deﬁnition, for the maximum value of D,let’s call it vmax = max(D),its corresponding bitlist Dvmax [x] is set for all x.Hence, Dvmax [x] is redundant and we do not have to represent it explicitly. What are the space requirements when compared to the standard uncompressed value bitmap? If your table has n tuples and the cardinality of the column to index is D,we need (D \u0000 1) · n bits. So, the diﬀerence to a value bitmap is that we need n bits less. How do we compute a range query? A range query requesting all rows having discrete values in range [v1; v2] may be computed by returning Dv2 [x] AND NOT Dv1\u00001[x]. In other words, we compute all values smaller equal v2 that are not smaller than v1.Notice that we only require two bitlists for any range query rather than a possibly large set of up to D bitlists. For instance, to compute all rows having values in range [1980; 1984] we return: D1984[x] AND NOT D1979[x]. Can we also execute a point query?point query Sure. A point query on value v can be translated to a range query [v; v] which is then executed as described above. For instance, to compute all rows having a value of 1983, 3.5 Bitmaps 175 we translate it to a range [1983; 1983] and hence we return: D1983[x] AND NOT D1982[x]. What is the major trade-oﬀ of range-encoded bitmaps compared to value bitmaps? The advantage of a range-encoded bitmap is that it allows us to execute any range query by inspecting only two bitlists. This is not possible with a value bitmap which needs up to D bitlists to compute the same result. An advantage of value bitmaps over range-encoded bitmaps is that any point query may be computed using a single bitlist. In contrast, a range-encoded bitmap requires two bitlists, even for point queries. In terms of storage requirements, both indexes need more or less the same space: (D \u0000 1) · n versus D · n bits. Quizzes 1. For the Range-Encoded Bitmaps example in the video (assuming that the bit list with the highest value does not have to be represented physically, yet we memorize its key), how many bit lists have to be read to answer a query with the following WHERE clause: WHERE yearOfBirth = 1987? (a) 1 (b) 2 (c) 3 3.5.5 Approximate Bitmaps, Bloom Filters Material Video: Original Slides: Inverted Slides: Additional Material Literature: [LÖ09], Bloom Filters WBloom Filter Learning Goals and Content Summary What are the lookup semantics of a bloom ﬁlter? bloom ﬁlter Recall that in a deterministic data structure (like binary search trees, B-trees, and value bitmaps) it holds that: index.hasKey(key)= true , key exists in database. (3.1) In contrast, a bloom ﬁlter is a probabilistic index structure. Here, condition 3.1 is probabilistic index 176 Indexes 1 0 0 0 0 0 0 1 0 0 1 0 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 setBit(12781), which color is bit 30? bucket 0 bucket M-1 h1(12781) = 0 bit list of size M, insert(12781) h2(12781) = 10 h3(12781) = 30 here M=42 # of inserts so far, N=3 k hash functions, here k=3 lookup(7431) bucket 0 bucket M-1 h1(7431) = 10 bit list of size M, lookup(7431) h2(7431) = 12 h3(7431) = 19 here M=42 k hash functions, here k=3 # of inserts so far, N=3 1 0 0 0 0 0 0 1 0 0 1 0 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 Figure 3.21: setting and looking up bits in a bloom ﬁlter weakened to: index.hasKey(key)= true ( key exists in database. This implies there are cases where index.hasKey(key)= true ^ key does not exist in database. (3.2) The case described by condition 3.2 is called false positive,i.e. the index claims that thefalse positive underlying table contains the key, however, in reality the table does not contain that key. Hence, a bloom ﬁlter, just like sparse and coarse-granular indexes (recall Section 3.2.4), is a ﬁlter index.ﬁlter index What is the core idea of a bloom ﬁlter? The core idea of a bloom ﬁlter is to support the method index.hasKey(key) through a series of hash functions which all operate on the same bitlist. Similarly to chained hashing, we map an input domain of keys DI = {0,. .., N } to a smaller output domain DO = {0,. .., M } where M<< N . In contrast to chained hashing, we do not store key/value-pairs in the hash table, but just set bits at particular slots, i.e. the slots do not point to a list of entries. In a bloom ﬁlter, a slot is simply implemented as a bit. Hence, we need a bitlist of length M . Also in contrast to chained hashing which uses 3.5 Bitmaps 177 one hash function,in a bloom ﬁlter we may use multiple hash functions h1() : DI 7! hash function DO,. .., hk() : DI 7! DO. How do we insert a key into a bloom ﬁlter? For each of the k hash functions we set slot hi(key) to true. This is done independently of whether a particular bit of a slot was already set before. How do we lookup a key in a bloom ﬁlter? For each of the k hash functions we check whether slot hi(key) is set to true.Only if all slots are set to true we return true. This implies that as soon as we detect a slot that is set to false,we can stop inspecting other slots,i.e. we terminate early,and return false. Why would a bloom ﬁlter return a false positive? The reason is the same as in chained hashing: collisions. Assume that we call index.hasKey(key) which returns true.Further assume, that key is not in the table. Hence, key is a false positive. How is that possible? Simply because the k bits inspected by index.hasKey(key) where not set through a previous insert of key into the bloom ﬁlter, but rather by insert-operations of other keys which happened to set those bits. This is a collision. collision What is the optimal number of hash functions? Analytically, the optimal number of hash functions is k = m/n · ln(2). However, in practice, k needs to be an integer. This yields a false positive probability rate of p(“false positive”) ⇡ 0.618503m/n. Quizzes 1. In the case of bloom ﬁlters, as the number of stored items increases, the false positive rate: (a) Decreases (b) Increases (c) False positive rate is independent of the number of items stored 2. While removing an entry from a bloom ﬁlter: (a) All the bits referenced by this item have to be set to 0 (b) Setting only one bit to 0 would suﬃce (since a look-up for an item returns false even if a single referenced bit is 0) (c) In general, it is not possible to remove an entry from a bloom ﬁlter. 3. The False positive rate in bloom ﬁlters is decreased by: (a) Increasing the length of the bit list 178 Indexes (b) Decreasing the number of hash functions 4. In the context of databases, bloom ﬁlters are ideal for checking: (a) Whether an entry exists in a table (b) Whether an entry does not exist in a table (c) The number of times an entry appears in a table Exercise Your task is to design a software that is able to answer the following question: given a matriculation number consisting of 7 digits, decide whether the student is enrolled in the university. To make this process faster, you can create a Bloom ﬁlter on the matriculation numbers, and store it in main memory. Let’s assume the university has n = 20, 000 students. Let’s say you have m = 200, 000 bits to store your Bloom ﬁlter, and 14 hash functions (k). (a) What is the false-positive rate of your Bloom ﬁlter? (b) Let’s assume the average computation time for a hash function is 500 ns and the average random memory access time is 100 ns. Assume the worst case that the bloom ﬁlter is not in any of the caches. What is the cost of a lookup in the Bloom ﬁlter for an element that is contained? (c) In case of a positive answer from the bloom ﬁlter, you still have to go to disk, and check whether the student is really enrolled. This triggers an additional lookup with an average lookup time for a single table entry of 5 ms. Compare the expected costs of a random lookup: a disk-lookup (without using the bloom ﬁlter) vs a lookup using the Bloom ﬁlter as well (and going to disk on demand only)! (d) How could you improve the parameters of your Bloom ﬁlter, i.e. k? Exercise The bloom ﬁlters introduced above have a big disadvantage: they do not support delete operations. You are given two hash functions: 1. h1(x)= x mod 8 2. h2(x)=(x/4) \u0000 3mod 8 Where \u0000 denotes the binary XOR operation. These two hash functions are used to build a numbering bloom ﬁlter. In a numbering bloom ﬁlter you use more than a single bit for every bucket. Whenever a key is inserted into the numbering bloom ﬁlter, the counts of 3.5 Bitmaps 179 all buckets that are hit are incremented by one (except if it would overﬂow). Assume you have 8 bytes available to store the entire numbering bloom ﬁlter and that it is initially empty (i.e. all counts are zero, bucket size is 1 Byte). (a) Insert 123, 345, 3, 77, and 5. Draw the resulting ﬁlter after each insert. (b) Can you perform deletes in such a data structure? If it is possible, explain under what circumstances. Delete 345 if possible. (c) Assume the numbering bloom ﬁlter contains only the elements 345, 5, 6, and 7. What is the empirical false positive rate of the Numbering bloom ﬁlter w.r.t. the interval [0;8]? Hint: Use the concrete hash functions as they are speciﬁed in the beginning. Exercise Similar to bloom ﬁlters, in a numbering bloom ﬁlter we need to make the right choices for k, m as well as the number of bits s to use for each bucket. In the exercise you have to plot several graphs. A free tool to create plots is gnuplot; an alternative to this is R. Assume you want to insert n = 100, 000 elements into your bloom ﬁlter. (a) Plot the false-positive rate for k 2 [1, 4] hash function against the number of bits m. For every k, what is the smallest bitmap size m to get a false-positive rate smaller than 1%? (b) Assume you use k =2 hash functions and you have M = 362, 880 bits available to store your Bloom ﬁlter. (i) What is the highest number of collisions of any bucket? Assume a varying number of bits per bucket where the number of bits per bucket is varied in s =1,. .., 10.Hence the actual number of buckets of the numbering bloom ﬁlter is M/s.Either ﬁnd an analytical solution for the expected value or run an experiment with suﬃcient (e.g. 1000) repetitions to empirically determine the expected value. (ii) Now also plot the false positive rate against the diﬀerent bucket sizes. (iii) What is the bucket size in s =1,. .., 10 that allows you to represent two times the number of expected collisions and still that has the lowest false positive rate? 180 Indexes Chapter 4 Query Processing Algorithms 4.1 Join Algorithms 4.1.1 Applications of Join Algorithms, Nested-Loop Join, Index Nested-Loop Join Material Video: Original Slides: Inverted Slides: Additional Material Literature: [LÖ09], Nested Loop Join [LÖ09], Index Join [RG03], Section 14.4.1 Learning Goals and Content Summary Why are join algorithms important? join algorithm Join algorithms are a fundamental building block of query processing. These algorithms are used internally by the database system not only to translate SQL-joins into exe- cutable programs, but also to implement grouping and aggregation (including duplicate elimination), as well as subqueries. What is the possible impact of joins on query performance? The impact of join algorithms on query performance may be tremendous. In particular, in those cases where the database system chooses the wrong join algorithm, e.g. nested loops for large input datasets, the impact on runtime may be substantial and lead to several orders of magnitude performance diﬀerences. What are the four principal classes of join algorithms? 182 Query Processing Algorithms Nested-Loop Join JP(r,s) := r.x == s.x\t //deﬁnition of the join predicate NestedLoopJoin( R, S, JP(r,s) ): \t ForEach r in R\t //for every tuple in R\t \t\t ForEach s in S:\t //for every tuple in S \t\t \t If JP(r,s):\t //check join predicate \t\t \t \t output( (r,s) );\t //output join result R S Index Nested-Loop Join JP(r,s) := r.x == s.x\t //deﬁnition of the join predicate indexOnRX := catalog.get( indexes, R.x );\t //use existing index on R.x \t\t \t \t \t //precondition for this join algorithm IndexNestedLoopJoin( indexOnRX, S, JP(r,s) ): \t ForEach s in S:\t //for every tuple in S \t\t queryResultSet = indexOnRX.query(s.x);\t //query index for this s (aka probe the index) \t\t If queryResultSet NOT empty:\t //did the query return results? \r\r \r output( {s} × queryResultSet );\r //output join results R S Figure 4.1: Nested-Loop vs Index Nested-Loop Join 1. nested-loop joins 2. index nested-loop joins 3. hash joins 4. sort-merge joins This is the traditional classiﬁcation of joins that you ﬁnd in textbooks. However, note that this classiﬁcation is somewhat misleading. As hash joins can be considered a special case of index nested-loop joins. What is a nested-loop join (NL)?nested-loop join NL Let’s assume two relations R and S and a join predicate JP(r,s):= r.x==s.x. A nested-loop join simply implements the cross product of R and S followed by a selection, i.e. \u0000JP(R ⇥ S). This is implemented by nesting two loops, one is iterating over all elements of R, the other over all elements of S. Every possible combination of elements from R and S is created and probed with the join predicate. Obviously, this naive implementation has a worst-case complexity of O(|R| ⇥ |S|).Hence, NL should only be used for: 4.1 Join Algorithms 183 1. small input sets (i.e., datasets that are small after having applied individual ﬁlters on the input datasets), 2. cases where no other join algorithm is available (e.g., complex join predicates, in particular join predicates that are a blackbox function), 3. for testing purposes to obtain baseline result sets. For which type of join predicates does NL work? This “algorithm” works for all possible join predicates. What is index nested-loop join (INL)? index nested-loop join INLIn an index-nested loop join (INL) we replace one of the loops of a nested-loop join (NL) by an index. This works only for those cases where the join predicate can be evaluated eﬃciently by that index. In order to determine whether this is possible, we need to look inside the join predicate, i.e. we inspect the deﬁnition (and/or implementation) of the join predicate which may actually be a blackbox. In case of an equi join predicate like in the video, this is easy. Hence, we simply need an index structure supporting point queries, e.g. a B-tree index. In the video, we create an index on R and then probe all elements of S against R. Likewise, we could create an index on S and then probe all elements of R against S. INL is typically a good choice if one of the inputs R or S already has a suitable index available. Even if the index has to build ﬁrst, INL may pay oﬀ. What is required to run INL? We need either an already existing index that was built on a suitable key such that we may exploit that existing index in INL. Or we may bulkload an empty index. For which type of join predicate does INL work? join predicate This algorithm works for join predicates that may be translated into a sequence of index lookups. This is not only the case for equi joins. For instance, assume a join predicate JP(r,s):= abs(r.x-s.x) <= 42. This could be translated into a sequence of queries against an index on S.x as follows: 8r 2 R : indexOnSx.query([r \u0000 42; r + 42]). In other words, for each element of R, we query the index on S.x using an inter- val query [r \u0000 42; r + 42]. What is the runtime complexity of INL? The runtime complexity of INL depends on the complexity of an individual index lookup. In the case of a B-tree built on S where an individual insert and point query takes O(log |S|) time, the overall complexity is O(|R| log |S|) if we do not factor in the costs for building the index on S which costs O(|S| log |S|). 184 Query Processing Algorithms Quizzes 1. Which of the following relational operations can be performed with or as a side-eﬀect of a join algorithm? (a) GROUP BY (b) INTERSECT (c) ORDER BY 2. Assume you have two tables R and S with |R| = n and |S| = m.What is the worst-case complexity (number of comparisons) of a Nested-Loop Equi-Join on R and S?. You may assume that the predicate has O(1) complexity (a) O(n log m) (b) O(n · m) (c) O(n2) (d) O(m 2) 3. Would any kind of join predicate, regardless of how complicated it is, be suitable to implement the entire join with a Nested-Loop Join? (a) No (b) Yes 4. Would any kind of join predicate, regardless of how complicated it is, be suitable to implement the entire join with an Index Nested-Loop Join? (a) No (b) Yes 4.1.2 Simple Hash Join Material Video: Original Slides: Inverted Slides: Additional Material Literature: [LÖ09], Hash Join [RG03], Section 14.4.3 4.1 Join Algorithms 185 Simple Hash Join JP(r,s) := r.x == s.x\t //deﬁnition of the join predicate indexOnRX := catalog.get( indexes, R.x );\t //use existing index on R.x \t\t \t \t \t //precondition for this join algorithm SimpleHashJoin(indexOnRX, S, JP(r,s)): \t indexOnRX := build_ht(R.x);\t //build hash table on R \t ForEach s in S:\t //for every tuple in S \t\t queryResultSet = indexOnRX.query(s.x);\t //query index for this s (aka probe the index) \t\t If queryResultSet NOT empty:\t //did the query return results? \r\r \r output( {s} × queryResultSet );\r //output join results R S Index Nested-Loop Join JP(r,s) := r.x == s.x\t //deﬁnition of the join predicate indexOnRX := catalog.get( indexes, R.x );\t //use existing index on R.x \t\t \t \t \t //precondition for this join algorithm IndexNestedLoopJoin( indexOnRX, S, JP(r,s) ): \t ForEach s in S:\t //for every tuple in S \t\t queryResultSet = indexOnRX.query(s.x);\t //query existing index on R.x for this s.x \t\t If queryResultSet NOT empty:\t //did the query return results? \r\r \r output( {s} × queryResultSet );\r //output join results R S Figure 4.2: Simple Hash Join vs Index Nested-Loop Join Learning Goals and Content Summary How does simple hash join (SHJ) work? simple hash join SHJAsimple-hash-joinbuilds (oruses an existing) hashtable onone of the inputs, sayR. All elements of S are then probed against that hash table. SHJ is merely a special case of an index-nested loop join. In INL, just replace any occurrence of “index” by “hash table”. For historic reasons, however, these classes of joins are often still treated as separate algorithms. For which type of join predicates does SHJ work? SHJ is more restricted in this regard than the more general INL as hash tables typically do not support range queries (see Discussion in Section 4.1.1). Quizzes 1. Can the Simple Hash Joins be considered a special case of Index Nested-Loop Join? (a) Yes (b) No 2. If a Simple Hash Join uses hashing with overﬂow chains on attribute R.x of table R, what elements are returned as the result of SELECT * FROM R JOIN S ON 186 Query Processing Algorithms R.x = S.x? (a) Pairs of R and S that do not hash to the same bucket. (b) All pairs of R and S that hash to the same bucket. (c) All pairs of R and S that hash to the same bucket and for which R.x == S.x. 4.1.3 Sort-Merge Join, CoGrouping Material Video: Original Slides: Inverted Slides: Additional Material Literature: [LÖ09], Sort-Merge Join [RG03], Section 14.4.2 Learning Goals and Content Summary How does sort-merge join (SMJ) work?sort-merge join SMJ Assume an equi-join predicate JP(r,s):= r.x==s.x. In order to perform an equi join using SMJ, we ﬁrst sort both tables on their join columns, i.e. we sort table R on attribute x and table S on attribute x. For each input table we may skip sorting if the table is already sorted according to the join attribute. Now, the main idea is to step through both tables synchronously. This is done by keeping two pointers, PR and PS. PR points to the ﬁrst row of R, PS to the ﬁrst row of S. Whenever the two pointers point to two rows having the same join key, we found a join result and output it. Then, in any case, we move the pointer currently pointing to the join key (either r.x or s.x) that is smaller. This process is repeated until the end of one of the inputs is reached. See Figure 4.3. What has to be considered when deciding which pointer to move forward? In a tie, i.e. if both PR and PS point to rows having the same join key, we may either move pointer PR or PS forward. This, however, may have consequences on the correctness of the sort-merge join algorithm. To make the algorithm correct, we have to consider which of the input tables contains duplicates. Let’s assume that R is the left and S the right input table to the join. If R and S are related through a join predicate JP in a 1:N- relationship, in case of a tie, we should always move PS ﬁrst. If it is a N:1-relationship, we should always move PR ﬁrst. N:M-relationships should not occur, as they should have been resolved in database modeling by using a 1:N and a N:1-relationship. However, if we are not sure whether one of the join columns is duplicate free or the join is used on a non-foreign key relationship, we have to extend the algorithm to handle possible duplicates in both rows. How would I treat a situation where duplicates exist in both join columns?duplicates In that situation (see also Figure 4.4) we have multiple options: 4.1 Join Algorithms 187 Found a Join Result CustomersCustomersCustomers name street cityID frank minstreet 0 peter minstreet 0 stefan unistreet 0 jens shortstreet 1 steve macstreet 1 felix macstreet 5 hans msstreet 5 alekh unistreet 7 jorge minstreet 9 mike longstreet 9 olaf macstreet 9 tim unistreet 9 Cities_DictionaryCities_Dictionary cityID city 0 new york 1 cuppertino 3 paris 5 berlin 7 london 9 saarbruecken Customers NATURAL JOIN Cities_DictionaryCustomers NATURAL JOIN Cities_DictionaryCustomers NATURAL JOIN Cities_DictionaryCustomers NATURAL JOIN Cities_Dictionary name street cityID city frank minstreet 0 new york peter minstreet 0 new york stefan unistreet 0 new york jens shortstreet 1 cuppertino steve macstreet 1 cuppertino felix macstreet 5 berlin Sort-Merge Join JP(r,s) := r.x == s.x\t //deﬁnition of the join predicate SortMergeJoin( R, S, JP(r,s) ): \t sort(R on R.x);\t //sort R on join attribute x \t sort(S on S.x);\t //sort S on join attribute x \t Pointer PR = R[0];\t //initialize pointer to R \t Pointer PS = S[0];\t //initialize pointer to S \t do:\t\t \t \t //start loop \t\t if PR.x == PS.x:\t //if values match \t\t \t output( (PR, PS) );\t //output join result \t\t if PR.x <= PS.x:\t //ﬁnd smaller join key (move left ﬁrst!) \t\t \t PR++;\t //move pointer to R forward \t\t else: \t\t \t PS++;\t //move pointer to S forward \t while PR != R.end && PS != S.end\t //both tables unprocessed \t ...\t\t \t \t //output join results... \t\t ...\t\t \t //...for non-empty table R S Figure 4.3: Synchronously scanning through two sorted tables in a merge join: example vs pseudo-code 1. modify SMJ allowing it to move one of the pointers, PR and PS, backwards in order to perform a cross product within two duplicate sets having the same join key. 2. keep a window of elements having the same join key for one of the input tables in main-memory. For instance, for table S and at all times, make sure that the set of elements having the currently treated join key is kept in memory. This is also the fundamental idea behind sweep-line algorithms which are wildly used in computational geometry, see [APR+98]. 3. use any other algorithm allowing you to (a) cogroup both inputs and then 188 Query Processing Algorithms Natural Join CustomersCustomersCustomers name street cityID frank minstreet 0 peter minstreet 0 stefan unistreet 0 jens shortstreet 1 steve macstreet 1 felix macstreet 5 hans msstreet 5 alekh unistreet 7 jorge minstreet 9 mike longstreet 9 olaf macstreet 9 tim unistreet 9 Cities_DictionaryCities_Dictionary cityID city 0 new york 1 cuppertino 3 paris 5 berlin 7 london 9 saarbruecken Joins? CoGroups! CustomersCustomersCustomers name street cityID frank minstreet 0peter minstreet 0 stefan unistreet 0 jens shortstreet 1 steve macstreet 1 felix macstreet 5 hans msstreet 5 alekh unistreet 7 jorge minstreet 9 mike longstreet 9 olaf macstreet 9 tim unistreet 9 Cities_DictionaryCities_Dictionary cityNo city 1 new york 1 cuppertino1 paris 5 berlin 5 london 9 saarbruecken Figure 4.4: Foreign-key constraint typically imply duplicates in one table only; relationship of joins to cogrouping (b) perform a join within each cogroup. If the cogrouping is done such that all rows within each cogroup have the same key, then it is enough to use a cross product within each cogroup. What is the relationship between joins and CoGroups? What is CoGrouping?CoGroup CoGrouping Joins and cogrouping are highly related. Unfortunately, cogrouping is neither part of relational algebra nor SQL. In both, we may only group on a single input relation/table. Cogrouping is simply an extension of the normal single-relation grouping to allow for multiple inputs. In other words, we group multiple inputs at the same time w.r.t. the same grouping key. Quizzes 1. Assume you want to perform a Sort-merge Join on two tables R and S.However, both tables are already indexed in the system on the join attribute using a clustered B-tree index. Does the Sort-merge Join still have to perform the sorting step? 4.1 Join Algorithms 189 (a) Yes (b) No 2. What is the worst-case complexity (number of comparisons) of the merge step in the Sort-merge Join algorithm? Assume that the number of tuples in tables R and S is n and m,respectively. (a) O(n + m) (b) O((n + m) log(n + m)) (c) O((n + m)2) 3. Does the Sort-merge Join algorithm (as shown in the video with two pointers PR and PS, checking which of the inputs is smaller and then moving pointers forward) als work if both join attributes are non-unique? (a) Yes (b) No 4. Would any kind of join predicate, regardless of how complicated it is, be suitable to implement the entire join with a Sort-Merge Join? (a) No (b) Yes 4.1.4 Generalized CoGrouped Join (on Disk, NUMA, and Dis- tributed Systems) Material Video: Original Slides: Inverted Slides: Learning Goals and Content Summary How do we deﬁne cogrouping? There are multiple ways to deﬁne cogrouping: 1. disjoint partitioning variant:First, we need a grouping function group(Tuple) 7! [0,. .., k \u00001]. This function must be deﬁned on all input sets. For every incoming tuple it assigns a group ID to the tuple. In addition, we require a single function partition() : (Set, group()) 7! Setof P air < Set, groupID >. This function parti- tions an input set into a set of disjoint groups. All tuples within a group have the same group ID. See Figure 4.6. 2. labeling variant:Another way to deﬁne cogrouping is by using separate labeling functions for each input relation. This may emulate a disjoint partitioning as above. 190 Query Processing Algorithms Joins? CoGroups! CustomersCustomersCustomers name street cityID frank minstreet 0peter minstreet 0 stefan unistreet 0 jens shortstreet 1 steve macstreet 1 felix macstreet 5 hans msstreet 5 alekh unistreet 7 jorge minstreet 9 mike longstreet 9 olaf macstreet 9 tim unistreet 9 Cities_DictionaryCities_Dictionary cityNo city 1 new york 1 cuppertino1 paris 5 berlin 5 london 9 saarbruecken Co-Grouping without Sorting CustomersCustomersCustomers name street cityID peter minstreet 0 steve macstreet 1 mike longstreet 9 tim unistreet 9 hans msstreet 5 jens shortstreet 1 frank minstreet 0 olaf macstreet 9 stefan unistreet 0 alekh unistreet 7 felix macstreet 5 jorge minstreet 9 Cities_DictionaryCities_Dictionary cityNo city 5 berlin 1 cuppertino 9 saarbruecken 1 paris 1 new york 5 london Figure 4.5: Foreign-key constraint typically imply duplicates in one table only; relationship of joins to cogrouping However, we may also assign multiple labels to each input tuple and thus replicate tuples into multiple partitions. CoGroup(I1,...In,p1(),..., pn()) 7! n\u0000O1,. .., On l1 ,. .., \u0000O1,. .., On lm o. Here, I1,. .., In are input relations. p1(),. .., pn() are labeling functions where pi : Ii 7! {L}.Here L is a discrete domain of labels. Every input to a pi is mapped to a subset of L. In other words, for each input tuple in any of the input sets Ii we assign a set of labels. For each distinct label lj 2 L there will be one group of outputs {O1,. .., Om}lj . Why are cogroups a general property of equi-joins rather than a property of a speciﬁc joincogroup join algorithm? This follows directly from the deﬁnition of cogrouping. An equi-join identiﬁes pairs of rows having the same join key. This is a property of the join as deﬁned in relational 4.1 Join Algorithms 191 Generalized Co-Grouped Join JP(r,s) := r.x == s.x\t //deﬁnition of the join predicate group( Tuple ): Tuple \u0000 [0, ..., k-1]\r //generalized grouping function partition( Set, group() ): (Set, group()) \u0000 Set of Pair<Set, groupID>\r //generalized partitioning function CoGroupedJoin( R, S, JP(r,s), group(), partition() ): \r Set of Pair<Set, groupID> build := partition( R, group() ):\r //partition R into subsets (aka groups) \r Set of Pair<Set, groupID> probe := partition( S, group() );\r //partition S into subsets (aka groups) \t ForEach groupID in [0 to k-1]:\t //foreach existing unique groupID \t\t leftInput = build.getSet( groupID );\t //retrieve corresponding subset from R \t\t rightInput = probe.getSet( groupID );\t //retrieve corresponding subset from S \t\t If NOT leftInput.isEmpty() AND NOT rightInput.isEmpty():\t //only if both inputs have some data \t\t \t WhateverJoin( leftInput, rightInput, JP(r,s) );\t //call whatever join algorithm R S Figure 4.6: Pseudo-code of generalized cogrouped join algebra. It is not speciﬁc to a speciﬁc implementation of a join. How can we express an equi-join using cogrouping? We could express this join through a cogrouping operation in multiple ways: 1. Grouping by equality: all labeling functions p1(),..., pn() simply return the join key of their corresponding table. Like that, all tuples within a cogroup have the same key. In order to compute the join result, it is enough to compute a cross product within each cogroup. 2. Grouping by partitioning: all labeling functions p1(),..., pn() simply return a func- tion of the join key. The same function is used for all input values. Like that, all tuples within a cogroup have the same return value for that function. However, tuples within the same cogroup may have diﬀerent values in their join columns. Hence, we still need to perform a join within each cogroup. Let’s go back to our join predicate JP(r,s):= r.x==s.x. Assuming, I1 = R and I2 = S, we could express this join as either: 1. Grouping by equality: p1() = r.x and p2() = s.x. 2. Grouping by partitioning: p1() = r.x/k and p2() = s.x/k.Here k is the number of cogroups. Could we use a cross product inside a cogroup rather than a join algorithm? cross product Again, yes, if the grouping is done by equality. Which three special cases can be implemented with this algorithm? 192 Query Processing Algorithms 1. Grace Hash Join: a join algorithm that partitions data into buckets on disk. Then each cogroup is joined using a main-memory join. 2. Distributed Join: a join algorithm that partitions data into buckets. Each bucket gets assigned to a separate computing node. Then each computing node works on its cogroup independently (be it on disk or in main-memory). 3. NUMA Join: a join algorithm that partitions data into, yes, surprise: buckets! Each bucket gets assigned to a separate memory region in a NUMA architecture. Then each CPU works on its cogroup independently (on NUMA-local main-memory). So, it is always the same pattern: partition both input sets into buckets using the same partitioning method. After that, handle the cogroups independently. The only thing that changes is the layer of the storage hierarchy that is used to partition the data into buckets and then join the cogroups. Many other special cases and variants can be derived using generalized cogrouped join as a framework. Which phases can be identiﬁed in the algorithm? 1. Co-Partitioning Phase: partition all input relations 2. Join Phase: perform a join within each cogroup. How can Generalized Co-Grouped Join be optimized to avoid calling that algorithm in certain cases anyhow? Let’s assume again that we only have to input tables, R and S, that should be joined through an equi join. 1. While partitioning R, we could collect the set of keys in an index, e.g. a Bloom Filter. Then, when partitioning S, we could discard all elements of S whose keys are not contained in that index. This means, elements of S that will not ﬁnd a join partner in R will never be written to a cogroup in the partitioning phase. 2. After partitioning, for every cogroup where at least one of the sets of the cogroup is empty, we may discard the join on this cogroup as this cogroup simply cannot produce any join result. 3. After partitioning, we may repartition cogroups that are too big (e.g. in Grace Hash Join, a cogroup may be too big to serve as the input to a main memory join, hence we need to repartition). Quizzes 1. Is it possible to join two tables R and S that are way larger than the main memory of the machine the database is running on? (a) Yes 4.1 Join Algorithms 193 (b) No 2. When co-grouping the tuples contained in two tables R and S,would it suﬃce to co-group only one of the tables? (a) Yes (b) No 3. Which of the following join algorithms is not a co-group join algorithm? (a) Grace hash join (b) Index nested-loop join (c) NUMA join 4. Assume we are executing the Co-group Join algorithm explained in the video, and assume that all co-groups on the join key are already computed. If we are doing an Equi-Join, could the ﬁnal answer (join) be computed from this point on using only across-productalgorithm? (a) Yes (b) No Exercise Assume you have two relations R and S stored on disk. R has a size of 100 pages and Sof120 pages, respectively. Furthermoreyouhave14 pagesofmainmemory available including space for input and output buﬀers. You are given a hash function h() that allows you to partition S into k equally-sized partitions, where k can be chosen by you (note: assume that the sum of the partition sizes in terms of pages is equal to the size of their input dataset. Depending on the k you choose this may lead to slightly unbalanced partition sizes). Due to statistics kept in the database, you already know before executing the join that the sizes of the partitions created by h() on R are highly skewed, i.e. the ﬁrst partition R0 contains 90 of the 100 pages of R and the remaining 10 pages are equally distributed among the remaining partitions R1,. .., Rk\u00001.Assume for simplicity that an in-memory hash table on a data page does not consume any extra space. (a) How many partitions k do you need to create to perform a generalized co-group join, where at least one side of each co-group ﬁts into main memory? (b) How many blocks do you need to read and write to perform the co-group join of (a) not counting the I/O-costs for writing out the join result and not counting the costs for the initial read of R and S (as we have to read them anyway)? (c) How can you improve the algorithm to minimize the total number of blocks read and written? What is the number of blocks read and written then? (Hint: What could you do with the available main memory?) 194 Query Processing Algorithms (d) How to specialize this join of (a) to run in a multicore NUMA architecture (single machine)? Provide (high-level) pseudo-code. (e) How to specialize this join of (a) to run in a distributed system (k machines)? Provide (high-level) pseudo-code. Exercise All of them work well for equi-joins. You are given two tables A and B and the following query: SELECT * FROM A, B WHERE sqrt ( abs ( A . x - B . x ) ) < 3 (a) Which of the join algorithms we already learned about can be used to implement this query? And if so, how? (b) Provide pseudo-code of an eﬃcient join algorithm for the above query. 4.1.5 Double-Pipelined Hash Join, Relationship to Index Nested- Loop Join Material Video: Original Slides: Inverted Slides: Learning Goals and Content Summary When does it make sense to consider using double-pipelined hash join (DPHJ)? double- pipelined hash join DPHJ Double-pipelined hash join is suitable in cases where: 1. we need to produce at least some join results early, i.e. we do not want to wait until ahash tablewaspopulated with an entire inputset(likein SHJ)orwedonothave the time to sort sort the inputs (like in SMJ) 2. the inputs are delivered by a stream (or any sort of pipelining concept) which: (a) may temporarily block, i.e. for a certain time interval it does not deliver any results anymore for whatever reason (e.g. network problem) (b) is unbounded, hence there is no way to put it entirely into a hash table or sort it How does DPHJ work? In what sense is this algorithm symmetric? The algorithm works as follows: we initialize one empty hash table for each input set, let’s call them HTR and HTS. 4.1 Join Algorithms 195 HTR HTS hash-table for R hash-table for S 1. probe HTS using r1 3. insert of r1 into HTR r1 2. no join result no output R S Example: Double-pipelined Hash Join HTR HTS hash-table for R hash-table for S 1. probe HTR using s1 3. insert of s1 into HTS s1 2. join results output R S r1 Example: Double-pipelined Hash Join Figure 4.7: Processing element of R is mirror-inverted the same as processing an element of S 1. decide which input set to draw from. Let’s assume we draw tuple r1 from R (see Figure 4.7). Now, we 2. probe HTS using r1 as the lookup key 3. if that probe produces results, return the cross product of r1 and the result set as ajoin result 4. insert r1 into HTR Now, we need to decide again which input to draw from: 1. decide which input set to draw from. Let’s assume we draw tuple s1 from S (see 196 Query Processing Algorithms Double-pipelined Hash Join JP(r,s) := r.x == s.x\t\t \t //deﬁnition of the join predicate probeAndInsert( tuple, indexToInsert, indexToProbe ):\t \t queryResultSet = indexToProbe.query( tuple.x );\t\t //query index for this tuple.x (probe the index) \t If NOT queryResultSet.isEmpty():\t\t //did the query return results? \r\r joinResultSet = { tuple } × queryResultSet;\r\r //cross product tuple with the query result \t indexToInsert.insert( tuple.x );\t\t //insert the tuple into the corresponding index \t return joinResultSet;\t\t //return the actual join result for this tuple DoublePipelinedHashJoin( R, S, JP(r,s) ): \t indexOnRX = new HashTable(); indexOnSX = new HashTable();\t\t //initialize empty hash tables \t bool readFromR = TRUE;\t\t //ﬂag to determine relation to read from \t While R.hasNext() AND S.hasNext():\t\t //if both relations have items to read \t\t If readFromR:\t\t //if we have to read from R in this round \t\t \t output( probeAndInsert( R.next(), indexOnRX, indexOnSX ) );\t \t //join the R tuple with the probed tuples of S \t\t Else:\t\t \t \t //if we have to read from S in this round \t\t \t output( probeAndInsert (S.next(), indexOnSX, indexOnRX ) );\t\t //join the S tuple with the probed tuples of R \t\t readFromR = NOT readFromR;\t\t //switch to other relation for next round \t While R.hasNext():\t\t //if only R has entries left \t\t output( probeAndInsert( R.next(), indexOnRX, indexOnSX ) );\t\t //probe them against S and output \t While S.hasNext():\t\t //if only S has entries left \t\t output( probeAndInsert( S.next(), indexOnSX, indexOnRX ) );\t\t //probe them against R and output R S Figure 4.8: Pseudo-code of DPHJ and its diﬀerence over INLJ. Figure 4.7). Now, we 2. probe HTR using s1 as the lookup key 3. if that probe produces results, return the cross product of s1 and the result set as ajoin result 4. insert s1 into HTS And so forth, ... So, in this algorithm we simply exchange the roles of HTR and HTS as well as the roles of r1 and s1. That is why this algorithm is symmetric. So a high-level way of phrasing this principle would be: 1. decide which input set to draw from. Draw element x from that input. 2. probe HT of other input using x as the lookup key 3. if that probe produces results, return the cross product of x and the result set as a join result 4. insert x into this input’s HT What is double-pipelined index join (DPIJ)? double- pipelined index join DPIJ SHJ simply instantiates INL using a hash table. As discussed above, SHJ can be consid- ered a special case of INL. It is the same historic confusion as INL versus SHJ. For DPHJ as well there is no need to use a hash table as an index, any other index supporting the join predicate eﬃciently works well. For instance, you could run this algorithm using two B-trees. 4.1 Join Algorithms 197 What is the relationship between DPIJ and Index Nested-Loop Join? INL can be considered a special case of DPIJ in the sense that INL makes a decision on a speciﬁc order of drawing elements. For DPIJ we did not deﬁne an order and just said that the order may depend on the availability of the input tuples (in particular in a streaming environment). In contrast, INL ﬁrst draws all elements from the left input, then it draws all elements from the right input. Obviously, if you stick to this drawing order from the beginning, there is no need to build (and keep) a hash table for the right input. There is also no need to probe any element of the left input against elements stored in such hash table. And then DPIJ becomes exactly the same as INL. To be more precise, DPIJ mimics the behavior of INL. Does double-pipelined index join have a build and a probe phase? Not really. Only for each element treated you may split the algorithm into a build phase (inserting the element into the hash table) and a probe phase (querying the other hash table with that element). Again, this separation exists only for each element treated. There is no split into two separate phases over the execution of the entire join as in INL or SHJ. What is the diﬀerence between DPIJ and INLJ w.r.t. the join results produced over time? INLJ DPIJ may produce join results very early. In contrast, in INLJ we ﬁrst have to build the index — which may take a lot of time. In INLJ, only after having built the index, we may produce join results. Obviously, even though DPIJ may produce join results early, the number of join results produced will be relatively small in the beginning as initially the hash tables are scarcely populated. Quizzes 1. Assume you have a collection of 10 bee hives on which you have diﬀerent kinds of sensors, say outer temperature, inner temperature, temperature right below the hive, etc. The sensors constantly output data that you are interested in analyzing, so ajoin operation isanaturaloperation toperformon the data. Whatjoin algorithm would you choose for this scenario? (a) Nested-Loop Join (b) Sort-Merge Join (c) Double-pipelined Hash Join 2. Assume you want to join two tables R and S, each of which have a ﬁxed number n and m of tuples, respectively. What hash-based join algorithm would you rather choose for the join if you are interested in the least number of calls to a hash function? (a) Double-pipelined Hash Join (b) Simple Hash Join 198 Query Processing Algorithms 4.2 Implementing Grouping and Aggregation Material Video: Original Slides: Inverted Slides: Additional Material Literature: [RG03], Section 14.6 Learning Goals and Content Summary Hash-based Grouping and Aggregation grouping attribute: R.x aggregation function: aggregate( MultiSet<R‘> ): MultiSet<R‘> \u0000 <ScalarType>,\r[R‘] ⊆ [R] HashBasedGrouping( R, aggregate() ):\t\t \t \t HashMap hm = new HashMap();\t //initialize hash map \t List group = NULL;\t //handle to group of R.x \t ForEach r in R:\t //for every tuple in R \t\t If NOT hm.contains( r.x ):\t //check if already seen this key \t\t \t group = new List();\t //create new group \t\t Else:\t\t \t //i.e. key already in hash map (key seen before) \t\t \t group = hm.get( r.x );\t //get existing group from hash map \t\t group.append( r );\t //append element to group \r\r hm.put( r.x, group );\r //insert/replace group in hash map \t ForEach key in hm:\t //loop over all existing keys in hash map \t\t group = hm.get( key );\t //retrieve result group for that key \t\t aggregationResult = aggregate( group );\t //call aggregation function on that group \t\t output( key, aggregationResult );\t //output result pairs R Sort-based Grouping and Aggregation grouping attribute: R.x aggregation function: aggregate( MultiSet<R‘> ): MultiSet<R‘> \u0000 <ScalarType>,\r[R‘] ⊆ [R] SortBasedGrouping( R, aggregate() ):\t\t \t \t sort( R on R.x );\t //sort R on grouping attribute R.x \t Pointer PR = R[0];\t //initialize pointer to ﬁrst tuple of R \t Value currentGroupValue = R[0].x;\t //value of R.x for this group \t List group = new List();\t //create new group \t Do:\t\t \t \t //start loop \t\t If PR.x != currentGroupValue:\t //if current tuple belongs to same group \t\t \t aggregationResult = aggregate( group );\t //aggregate existing result group \t\t \t output( currentGroupValue, aggregationResult );\t //output result pairs \t\t \t group = new List();\t //create new group \t\t \t currentGroupValue = PR.x;\t //re-init value of R.x for this group \t\t group.append( PR );\t //append this tuple to result group anyway \t\t PR++;\t\t //move pointer forward to next tuple \t While PR != R.end;\t //end of table? \t aggregationResult = aggregate( group );\t //aggregate existing result group R Figure 4.9: Pseudo-code of hash-based vs sort-based grouping and aggregation 4.2 Implementing Grouping and Aggregation 199 Would it be fair to say that grouping and aggregation are just like cogrouping but using grouping aggregation cogrouping a single subset for each cogroup? Yes. Which phases can be identiﬁed in hash-based grouping? hash-based grouping 1. grouping phase: build hash table on the left input (if there isn’t already a hash table available on that key) 2. aggregation phase: for each distinct key existing in the hash table: call aggregation function on elements of that group What exactly is kept in the hashmap? hashmap In general, you need to keep the entire tuple of the input. However, depending on your grouping and aggregation attributes (which implicitly project the tuples) you may keep less data in the hashmap. For instance, assume a query SELECT A, sum(B) FROM foo. Independently of the schema of table foo, for the hashmap we simply use attribute A as the key and the running sum on attribute B as the value. There is no need to store any other information. Notice that this optimization may only be applied to so-called distributive aggregation functions. OK, but what is a distributive aggregation function? distributive aggregation function Assume an aggregation function f : X 7! Y .Let {X0,. .., Xn} be a disjoint and complete partitioning of X,i.e. Xi \\ Xj = ;8i 6= j and S i Xi = X.Assume there exists a function h : X 7! Y such that h⇣\u0000f (X0),. .., f (Xn) ⌘ = f \u0000{X0,. .., Xn}\u0000. Then we call f () a distributive aggregation function. In other words, we may compute this aggregate by ﬁrst aggregating partitions of the data using f ().These initial aggregates are then further aggregated using h(). The aggregation is distributed over diﬀerent function calls. Notice that h() maps from X to Y just like f (). So why is sum() distributive? Because we may deﬁne h() = g() = sum(). Is avg() distributive? No, however, avg() is algebraic. Sounds great, hmm, but what exactly is an algebraic aggregation function? algebraic aggregation function Similar to a distributive aggregation function, we want to perform the aggregation in steps. However, we simply cannot ﬁnd two functions h() and f () that operate on the same domains. We need more freedom in choosing these functions. So, again our user- deﬁned aggregation function is f : X 7! Y .We need two functions h() and g() where g() : X 7! W ^ h() : {W } 7! Y. 200 Query Processing Algorithms Here, W is any suitable “intermediate” domain. Now we can search for two functions h() and g() such that: h⇣\u0000g(X0),. .., g(Xn) ⌘ = f \u0000{X0,. .., Xn} \u0000. If these functions exist, we call f () an algebraic aggregation function. So why is avg() distributive? Because we may deﬁne g() : X 7! (Y, int) ^ h() : {(Y, int)} 7! Y where g() returns a pair with the sum and a count of the inputs and h() simply adds up these pairs and returns the quotient as a result. Which phases can be identiﬁed in sort-based grouping?sort-based grouping 1. grouping phase: sort table on the grouping key (if it is not already sorted w.r.t. that key) 2. aggregation phase: for each distinct key in the sorted sequence: call aggregation function on elements of that group Quizzes 1. Could the Sort-based Grouping algorithm highly beneﬁt from an existing B-tree on the grouping attribute? (a) Yes (b) No 2. Could the Sort-based Grouping algorithm beneﬁt in general from an existing hash table on the grouping attribute? (a) Yes (b) No 3. In the Hash-based Grouping and Aggregation algorithm, once the hash table has been created and we proceed to perform the actual aggregation, the pseudo-code loops over the set of existing keys to retrieve the content of all non-empty hash buckets and then perform the actual aggregation. Could we implement this loop without calling the hash function for each existing key again? (a) No, as this would aggregate diﬀerent keys within the same bucket (hash colli- sions) (b) No, as we cannot identify the buckets of the existing keys. (c) Yes, as we could simply iterate over the hash buckets to identify non-empty buckets. Within each bucket the entries have to be post-partitioned anyway. 4.3 External Sorting 201 4.3 External Sorting 4.3.1 External Merge Sort Material Video: Original Slides: Inverted Slides: Additional Material Literature: [RG03], Section 13.1 – 13.2 WMain Memory Merge Sort Recap [LÖ09], Index Join W Merge-sort with Transylvanian-saxon (German) folk dance WWhy it is important to know about Sorting in any Job Learning Goals and Content Summary Run Generation: First Run 2,3 4,6 PASS 0 Input file 2-page runs 3,4 6,2 9,4 8,7 5,6 3,1 7,4 6,1 Run Generation: Fourth Run 2,3 4,6 4,7 8,9 1,3 5,6 1,4 6,7 PASS 0 Input file 2-page runs 3,4 6,2 9,4 8,7 5,6 3,1 7,4 6,1 Figure 4.10: Pass 0: Run generation from ﬁrst to (in this case) fourth run When does it make sense to run external merge sort? external merge sort It makes sense to use external merge sort when a dataset is too large to be sorted in main-memory. That is the vanilla answer. But let’s think a bit more about this. Why not simply use quicksort on a memory- mapped or ram-buﬀered ﬁle? So, let’s assume we have n pages to sort, but only m< n pages of main memory. The critical part for quicksort is the initial phase when it is recursively partitioning the data and those partitions do not yet ﬁt into main memory. 202 Query Processing Algorithms PASS 1: First Merge 3,4 6,2 9,4 8,7 5,6 3,1 7,4 6,1 2,3 4,6 4,7 8,9 1,3 5,6 1,4 6,7 2,3 4,4 6,7 8,9 PASS 1 Input file 2-page runs 4-page runs PASS 0 PASS 1 Done. 3,4 6,2 9,4 8,7 5,6 3,1 7,4 6,1 2,3 4,6 4,7 8,9 1,3 5,6 1,4 6,7 2,3 4,4 6,7 8,9 1,1 3,4 5,6 6,7 PASS 0 PASS 1 Input file 2-page runs 4-page runs Figure 4.11: Pass 1: Merge runs containing two pages each into larger runs containing four pages each In order to perform one pivot step on an input partition p, quicksort will read the entire partition from the beginning and from the end until both pointers meet. In addition, whenever there is a swap of a pair of elements, the elements change a page and hence those pages have to be written back to disk (alternatively, we could write out the two new partitions to a separate place). So, in summary, for a single partition step, we can assume that you have to read and write the entire input partition once. Every pivot step partitions the data into two sub-partitions. You need to recursively partition until the partitions ﬁt into main memory. Then you simply run quicksort on them. The main problem with this algorithm is that it performs a binary partitioning, i.e. in one step the input partition is split into two sub-partitions only. This may implicitly lead to a lot of random I/O if both the input and the output partitions reside on the same hard disk and/or the underlying I/O-system does not make good use of the available main- memory, i.e. the page sequences read or written at a time are too small. This triggers another problem: you have to continue the binary partitioning until the partitions ﬁt. Recall, that the number of recursion levels of this strategy is log2dn/me.For instance, if the dataset is 8 times bigger than the available main memory, we will need three recursion levels. This means, the entire dataset is read three times and written three times. Notice that another, minor problem, with this algorithm is that it reads the input 4.3 External Sorting 203 PASS 2: First (and only) Merge Done. 3,4 6,2 9,4 8,7 5,6 3,1 7,4 6,1 2,3 4,6 4,7 8,9 1,3 5,6 1,4 6,7 2,3 4,4 6,7 8,9 1,1 3,4 5,6 6,7 1,1 2,3 3,4 4,4 5,6 6,6 7,7 8,9 PASS 0 PASS 1 PASS 2 Input file 2-page runs 4-page runs 8-page runs External Merge Sort F\t := fan-in of the merge-phase m\t := # pages available for a run (typically = available main memory) ExternalSorting(R): \t Heap<Run> runs;\t //heap of runs to consider \t While R not empty:\t //Pass 0: \t\t Run run = runGenerate( R, m );\t\t //read m pages of input from R and sort \t\t runs.add( run );\t\t //add reference to this run to heap \t While runs.size() > 1:\t //Passes 1 and following: \t\t List<Run> inputs;\t\t //list of inputs to merge (in a single merge) \t\t inputs = runs.popK( F );\t\t //remove next F inputs from the heap \t\t Run run = mergeRuns( inputs );\t\t //merge runs into one output run \t\t runs.add( run );\t\t //add reference to merged run to runs R Figure 4.12: Pass 2: Merge runs containing four pages each into larger runs containing eight pages each; pseudo code of external merge sort using two input streams. This means, it uses four streams in total (two for reading and two for writing) rather than three (one for reading and two for writing). So how could we ﬁx this? We need an algorithm that performs less passes over the data. However, at the same time that algorithm should not introduce too many random I/O-operations. Balancing the amount of passes over the data and random I/O is diﬃcult and depends heavily on the concrete combination of hardware, operating systems, and your algorithm. For the binary partitioning discussed above, it is easy to see that a better strategy is to partition by reading one input and writing F output partitions at a time. In partitioning, F is called the fan-out. The number of partitions is limited by F< m \u0000 1 as we need fan-out 204 Query Processing Algorithms to keep a buﬀer of the size of one page for reading the input and a buﬀer of one page for each output. If we increase F , we will need more random-I/O-operations, but less passes over the data and vice versa. Exactly the same balancing happens in external merge-sort. However, in contrast to a recursive partitioning, we start from the other direction: we ﬁrst start by creating sorted “partitions” (called runs in the following). Then we recursively merge these sorted “sub-partitions” into larger sorted partitions, hopefully with an F> 2. What is run generation?run generation In run generation we ﬁll the entire available main-memory with data, quicksort the data, and write the sorted data out to disk. We call this sorted sequence on disk a run.run Obviously, if we use quicksort, a run has the same size as the available main-memory. What happens during the merge phase of external merge sort, i.e. pass > 0?merge phase pass We repeatedly merge multiple input runs into one larger input run. We continue with this process until we end up with a single sorted run on disk — which is the ﬁnal result of the sort operation. It is a good idea to keep track of the existing runs using a heap which prioritizes by the size of the run, i.e. the shortest run has the highest priority. This leads to balanced merge-trees. What is the fan-in F?fan-in F The fan-in F is the number of input runs we merge in a single merge operation. What happens during a single merge of external merge sort? The merge may be implemented in many ways. A good strategy is the following. If m is the number of main-memory pages available, distribute those pages evenly to serve as input buﬀers for the F input runs and one output buﬀer for the output run. Use a heap with F elements. The heap contains a metadata-entry for each input buﬀer, i.e. a pointer to the next untreated element of that input buﬀer and the sort key of that element. The heap prioritizes smaller elements. Now, we repeatedly remove the top-element from the heap and write it to the output buﬀer. We re-insert an element into the heap for the input just drawn from. If the output buﬀer overﬂows, we write it to disk. Likewise, if an input buﬀer becomes empty, we reﬁll it with more data from the corresponding input run. We continue until all input runs have been entirely processed. Does the output buﬀer have the same size as the input buﬀers? Assume we have F input runs of size n/F each and m pages of main memory. For simplicity, assume that we do not count the extra space required for bookkeeping (the tiny heap for merging as explained above). Assume we reserve kI · F pages for the input buﬀers and kO pages for the output buﬀer and of course: kI · F + kO  m.Hence, kO = m \u0000 kI · F. Or: kI =(m \u0000 kO)/F. 4.3 External Sorting 205 0 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000 11000 5 10 15 20 25 30 35 40 45 50random I/Os k_I Figure 4.13: A single merge-operation: number of random I/O-operations when varying the size of the input buﬀers over the size of the output buﬀer for n =10, 000, m =1, 000,and F =20.If we increase F the “valley” gets more narrow. We can assume that each reﬁll of an input buﬀer as well as every write of the output buﬀer triggers a random I/O-operation. Hence, the number of random I/O-operations of asingle merge-step is: frand(kI ,F )= F · d(n/F )/kI e + dn/(m \u0000 kI · F )e Here, n and m can be considered constants. An example for n = 10, 000, m =1, 000, and F = 20 is shown in Figure 4.13. We can observe that for these settings there is a wide range where we obtain a close to minimal number of random I/Os. We learn that it is important to avoid two situations: 1. the input buﬀers are relatively small, in contrast, the output buﬀer has many pages (kI < 10 in the ﬁgure), 2. the input buﬀers have are relatively big, in contrast, the output has few pages (kI > 47 in the ﬁgure). Why would it be a bad idea to implement merge sort with a fan-in of F =2? As explained above for the case of partitioning, choosing F =2 is a bad trade-oﬀ as you will need many merge-levels. The number of merge levels in external sorting is dlogF (n/m)e.Hence, the bigger F the better — again: if we ignore random I/O which increases for a larger F . 206 Query Processing Algorithms Why would it also be a bad idea to implement merge sort with a fan-in F = m \u0000 1 (where m is the number of pages available in main memory)? Again, and as discussed for a single merge (see above), this is only optimal w.r.t. the number of passes over the data, i.e. the amount of data read and written. However, it leads to a tremendous amount of random I/O-operations which may lead to slow runtimes. You did not forget that this speciﬁc variant is a very simple version of the algorithm, right? External-merge sort may be optimized in many ways. In this section we just focussed on its core ideas. For a good discussion of various optimizations see [Gra06]. Quizzes 1. Assume you want to sort 1,000,000,000 4-byte (positive) integers with the property that each one of them is strictly less than 1,000,000,000. These integers are stored on disk on a server with only 2 GB of RAM. What algorithm would you call to sort these numbers? (a) Quicksort (b) Radix Sort (c) External Merge Sort Exercise Suppose you have a hard disk with the following characteristics given by the vendor: Seek time =4 ms, Read/write bandwidth =150 MB/s, Hard disk block size =4KB. Assume that your disk performs a seek only if you read into or ﬂush from a diﬀerent buﬀer, than the previous I/O operation did. Assume that inputs to a merge are uniformly read, i.e. all inputs to a merge are exhausted at the same time. Your task is to sort 2 20 4–byte integers, having 64 KB of memory at your disposal. Compare the seek- and scan costs in the worst case of sorting using external merge-sort for the following three variants: (a) 2–way external merge-sort, using two memory blocks for the output (b) 2–way external merge-sort, using half of the memory as an input buﬀer, and half of it as an output buﬀer (c) M–way external merge-sort, using a single memory block for the output (where M +1 is the number of available memory blocks) 4.3 External Sorting 207 4.3.2 Replacement Selection Material Video: Original Slides: Inverted Slides: Additional Material Literature: [Knu73] Further Reading: Wheapsort Learning Goals and Content Summary Two Main Memory Pages for Run Generation with Quicksort 2,3 4,6 4,7 8,9 1,3 5,6 1,4 6,7 2,3 4,4 6,7 8,9 1,1 3,4 5,6 6,7 1,1 2,3 3,4 4,4 5,6 6,6 7,7 8,9 PASS 0 PASS 1 PASS 2 Input file 2-page runs 4-page runs 8-page runs 3,4 6,2 9,4 8,7 5,6 3,1 7,4 6,1 Two Main Memory Pages for Run Generation with Replacement Selection 2,3 4,4 6,7 8,9 1,1 3,4 5,6 6,7 1,1 2,3 3,4 4,4 5,6 6,6 7,7 8,9 PASS 0 PASS 1 4-page runs 8-page runs 3,4 6,2 9,4 8,7 5,6 3,1 7,4 6,1 Figure 4.14: Run generation: Quicksort vs replacement selection 208 Query Processing Algorithms Replacement Selection Example M = 4 30 20 10 40 25 73 16 26 33 50 31 |heap| + |list| ≤ M Input ROutput run R’ list heap Replacement Selection Example M = 4 73 33 50 3110 20 25 30 40 Input ROutput run R’ 16 26 |heap| + |list| ≤ M list heap Figure 4.15: replacement selection: using a heap for everything that is smaller than the last pivot and a list for everything that is larger Which overall impact does replacement selection typically have when used with externalreplacement selection merge sort? Why would we use this algorithm? In general, by using replacement selection we can expect the input runs to become larger than the available main-memory. Hence, we have less input runs compared to using quicksort. Therefore, we may have less merge operations, a smaller fan-out for individual merge operations or even less merge levels in the merge-tree. See Figure 4.14 for an example with n =8 input pages and m =2 pages of main memory for run generation. (Yes, we ignore for a moment that a merge operation needs three main memory pages as a minimum anyway. The example is kept intentionally minimalistic.) What is the core idea of replacement selection? 4.3 External Sorting 209 Replacement Selection Example M = 4 26 33 50 3110 20 25 30 40 73 16 Input ROutput run R’ Run 0 complete Start of Run 1 |heap| + |list| ≤ M list heap Replacement Selection M\t := # elements in main memory R\t := input queue R‘\r := output queue ReplacementSelection( R ): \t Buﬀer B = read( R, M );\t //read M records from queue R into buﬀer B \t Heap heap = heapify( B );\t //bulkload the data of buﬀer B into a heap \t List list = new List();\t\t //create empty list \t do {\t\t \t \t \t //do while heap not empty \t\t While NOT heap.isEmpty() {\t //while heap contains elements \r\r \r Tuple r‘ = heap.pop();\r //remove top element (smallest on the heap) \r\r \r R‘.append( r‘ );\r //append that element to output queue R‘ \t\t \t If NOT R.isEmpty():\t //i.e. only if more elements in input R available \t\t \t \t Tuple next = read( R, 1 );\t //read M records from queue R into variable next \r\r \r \r If next ≥ r‘:\r //next ≥ ‘last output‘ r‘? (Note: what if next == r‘?) \t\t \t \t \t heap.push( next );\t //put it on the heap \r\r \r \r Else:\r\r //i.e. next < ‘last output‘ r‘! \t\t \t \t \t list.append( next );\t //append to list, i.e. need to treat this later \t\t }\t\t \t \t //end of while-loop \t\t heap = heapify( list );\t //bulkload the data of the list into a heap \t\t list = new List();\t\t //create a new empty list \t } While NOT heap.isEmpty()\t //continue until heap is empty R Figure 4.16: replacement selection may produce runs that are larger than the available main memory; pseudo-code of replacement selection In replacement selection we use all available main memory for sorting, just like in quick- sort, however, rather than simply outputting the sorted sequence available in main mem- ory, we output elements one by one. Whenever we remove the currently smallest element r0 from main memory and write it to an output run, we ﬁll the empty space with the next element from the input, say next.For next we check whether we can place it correctly into the sorted sequence available in main-memory. This is the case if next is larger than or equal to the last element r0 written to disk. Otherwise, i.e. if next is smaller than r0,we also keep next in main-memory, but do not consider it to be part of the sorted sequence. What is the main trick used in replacement selection? 210 Query Processing Algorithms The main trick here is to make use of the room that becomes available while writing out arun todisk. Fordata read in that processwesimplyhaveto becareful tounderstand whether we can still integrate it into the sorted output currently written or not. In order to maintain the two types of elements in main memory (elements that can or cannot be integrated into the sorted sequence), it is easy to see that we do not need to maintain a sorted sequence in main-memory. The only functionality we need is being able to identify the smallest element from the set of elements that can be integrated into the sorted sequence. This is supported by priority-queues like a min-heap (or a max-heap if we want to sort in descending order). As a heap can operate on an array, and the input elements have a ﬁxed-size, it is a good idea to allocate an array consuming all available main-memory. The head of that array represents the heap, the tail is used to buﬀer elements that cannot be integrated. During run generation the size of the heap is decreased for every element that cannot be placed in the sorted sequence. Once the heap size is zero, a new output run is started. Also see the examples and pseudo-code in Figures 4.15 and 4.16. What is the relationship between heapsort and replacement selection? Replacement selection may be considered a variant of heap sort in the sense that in heapsort the heap is iteratively decreased to make room for the sorted sequence. In contrast, in replacement selection the sorted sequence is written to an output stream. The room that becomes available is exploited to either integrate more elements into the heap or buﬀer elements that need to be considered for the next run. What is the typical size of a run generated by replacement selection compared to quicksort?quicksort Typically the size of an output run generated by replacement selection is twice the size of the available main-memory. See Donald Knuth’s book for a discussion [Knu73]. When is an element inserted into the heap? If the elements is larger or equal the last element written to the output run. When is an element inserted into the list? If the element is strictly smaller than the last element written to the output run. When does a run end? Arun ends if the heap size becomes zero, i.e. there are no more elements available in main memory that may be integrated into the sorted sequence currently being written to the output run. What can be said about the cache-eﬃciency of this algorithm?cache-efﬁciency In terms of cache-eﬃciency, replacement selection inherits the properties (and problems) of heapsort. Heapsort is optimal in a theoretical sense (also recall our discussion in Section 3.3), i.e. every insert and remove-operation to the heap takes in the worst case O(log n) comparisons. In total this leads to a worst case of O(log n).However, heapsort is not very-cache-eﬃcient: in particular for lower levels of the heap the likelihood increases that an element is not available in a cache and thus we receive an LLC. In contrast, 4.3 External Sorting 211 for quicksort and depending on the pivot strategy, the worst case complexity is O(n2). Quicksort is simply less robust than heapsort. However, in general, quicksort is much more cache-friendly. In most real world cases quicksort is faster than heapsort. This is (probably) the reason why it is used in many standard libraries like STL or boost. However, quicksort may degenerate heavily for some inputs. So what is the best sorting algorithm in a database? Interestingly, this is not easy to answer because it depends heavily on the storage system and data layouts used. Even for the very simple case of sorting ﬁxed-size data in main- memory the answer is non-trivial, see [ASDR14] for an experimental comparison of various multi-threaded main-memory methods. Quizzes 1. Is it theoretically possible to create sorted runs of size m even though the available RAM of the server cannot hold m elements at the same time? (a) Yes (b) No 2. Is it true that Replacement Selection always produces the sorted sequence of its input set of elements? (a) Yes (b) No 3. Is it true that Replacement Selection always produces a set of sorted sequences of its input set of elements? (a) Yes (b) No 4. In Replacement Selection, how many passes are performed over any given (whole) input disk block during the execution of the algorithm? (a) Only one (b) The number of elements it contains 5. Assume you run Replacement Selection on a sequence of n> 0 numbers that is sorted in decreasing order on disk. If the server has enough RAM to keep m<< n of those numbers at the same time, what is the number of increasing runs produced by the Replacement Selection Algorithm? (a) dn/2e (b) dm/2e (c) dn/me 212 Query Processing Algorithms Exercise Assume you have a main memory buﬀer of four tuples at any given time and also that you have enough space reserved in memory for input and output operations. You wish to create initial runs from the following sequence of sort keys using replacement selection: 25, 11, 71, 12, 8, 66, 69, 32, 71, 81, 96, 76, 37, 42, 51, 62, 2, 14, 27, 87 (a) Perform run generation and show each step of it. (b) How many runs did you create? How large is each run? 4.3 External Sorting 213 4.3.3 Late, Online, and Early Grouping with Aggregation Material Video: Original Slides: Inverted Slides: Learning Goals and Content Summary Online Grouping with Aggregation (aka Online Merge) 3,4 6,2 9,4 8,7 5,6 3,1 7,4 6,1 2,3 4,4 6,7 8,9 1,1 3,4 5,6 6,7 PASS 0 PASS 1 Input file 4-page runs 8-page runs (1,2) 2 (3,2) (4,3) 5 (6,3) (7,2) 8,9 group and aggregate here 2,3 (4,2) 6,7 8,9 (1,2) 3,4 5,(6,2) 7 Early Grouping with Aggregation: group and count() 3,4 6,2 9,4 8,7 5,6 3,1 7,4 6,1 PASS 0 PASS 1 Input file 4-page runs 8-page runs (1,2) 2 (3,2) (4,3) 5 (6,3) (7,2) 8,9 group and aggregate here Figure 4.17: Grouping and aggregation: online vs early Why should we be careful with terminology here? Typically in database literature the term “aggregation” is assumed to include a grouping operation as well. So, any particular aggregation method may also additionally include a grouping step. Hence “late aggregation” may include “late grouping” as a component. We 214 Query Processing Algorithms will diﬀerentiate among the two things in the following and always talk about “grouping and aggregation”. What is late grouping and aggregation? late grouping and aggregation In late grouping and aggregation we ﬁrst fully sort the data using external sorting. This includes writing the sorted data to disk. After that the sorted data is read. In that process we assign adjacent rows having the same grouping key to a group and for each group we compute the aggregates. In summary, in this method we do not overlap the execution of external sorting with grouping and aggregation. What is online grouping and aggregation? online grouping and aggregation In online grouping and aggregation we perform external sorting partially, i.e., we perform run generation and all merges except the ﬁnal merge. This means the fully sorted dataset is not written to disk. After that we perform the ﬁnal merge while grouping and aggre- gation is done at the same time. In other words, while we read the remaining F runs from disk, we merge them in main-memory and feed them directly into the grouping and aggregation operation. Just like in standard grouping and aggregation, in that process we assign adjacent rows having the same grouping key to a group and for each group we compute the aggregates. In summary, in this method we overlap the execution of the ﬁnal merge of external sorting with grouping and aggregation. What is early grouping and aggregation? early grouping and aggregation In early grouping and aggregation whenever we generate a run (be it using quicksort or replacement selection) or whenever we execute a merge, we feed the sorted output stream directly into the grouping and aggregation operation. This means, not only the ﬁnal merge is directly fed into the grouping and aggregation operation (like in online grouping and aggregation). In contrast, every time a sorted stream is generated, we exploit this as an opportunity to already perform grouping and aggregation. Just like in late and online grouping and aggregation, every time we group we assign adjacent rows having the same grouping key to a group and for each group we compute the aggregates. Notice that the aggregate functions need to be distributive or algebraic for this method (see Section 4.2). In summary, we fully overlap the execution of external sorting with grouping and aggregation. What has a huge impact on the actual savings for early grouping and aggregation? Early grouping and aggregation works best if only few groups are created. So the general pattern where this method shines is a query like SELECT foo, count(*) FROM table. If the cardinality of attribute foo is small, only few distinct groups will be created, and the savings in I/O will be relatively high. 4.3 External Sorting 215 Can early grouping and aggregation only be used with grouping or also with aggregation- only queries? Assume we do not specify a grouping condition like in SELECT count(*) FROM table. Then there is no need to group the data, hence there is no need to sort, we simply aggregate the data into a single value. In other words, recall that we sort the data in order to group, however the data within within a group does not need to be sorted. Then there is no need for grouping and hence sorting, we simply aggregate. And that aggregation is (kind of) early anyway... What is the amount of data transferred by the diﬀerent methods? Let’s assume we want to perform a sort-based grouping (see Section 4.2) using external merge-sort for sorting (see Section 4.3.1). Let’s deﬁne 1 R (1 W ) to be the I/O-costs (just the amount of data transferred) for one read (write) of the entire dataset.Let’s assume that T corresponds to the I/O required to write the grouped and aggregated dataset. Notice that we will assume the typical case that T  W ,but this does not have to be the case: it depends on the grouping attribute and the type of aggregation. Assuming k = dlogF (n/m)e merge-levels (as discussed in Section 4.3.1), for external sorting, we will require 1 R and 1 W for run generation plus 1 R and 1 W for each merge-level, i.e. in total (k + 1) R and (k + 1) W .However, the total data transferred to perform both sorting and grouping/aggregation varies considerably across the diﬀerent methods. See Table 4.1. Grouping and Aggregation Method Late Online Early where? after external sorting wrote the sorted out- put to disk during ﬁnal merge of external sorting during run generation and every merge of ex- ternal sorting I/O (k +2)R+(k +1)W +T (k + 1)R + kW + T R +(x)+ T Table 4.1: Methods for grouping and aggregation and their I/O-costs What are drawbacks of both online and early grouping and aggregation? In early grouping and aggregation we create a sorted version of the input table as a side- eﬀect. That sorted table may be exploited for future queries requiring the same sort order. In online and late grouping and aggregation we never materialize that sorted output on disk. Quizzes 1. Assume you have the following query: SELECT number, sum(*) FROM input GROUP BY number. If you perform this query on a set of numbers so large that it does not ﬁt into main memory, and you compute the queries by External Merge Sort + Grouping + Aggregation, what is the main bottleneck in the performance obtained? 216 Query Processing Algorithms (a) Number of comparisons (b) I/O operations 2. What is (deﬁnitely) an easy ﬁx to the problem of multiple read/write operations in the last step of External Merge Sort + Grouping + Aggregation? (a) Use Replacement Selection to create bigger runs each time (b) Group and aggregate the output directly right in the last pass 3. When grouping elements, these elements can be stored as tuples of type (i, m), meaning that input element i appears m times in the input run. If we group and aggregate directly right from the very beginning, in the Early Grouping and Aggregation algorithm, could the size of the output stream of the early aggregation be larger than its input? (a) Yes (b) No Chapter 5 Query Planning and Optimization 5.1 Overview and Challenges 5.1.1 Query Optimizer Overview Material Video: Original Slides: Inverted Slides: Additional Material Literature: [LÖ09], Query Plan W PostgreSQL System Catalog Learning Goals and Content Summary What is the task of the query parser? query parser The task of the query parser is to translate a declarative query (the WHAT part), i.e. an SQL statement, to an algebraic expression. That algebraic expression resembles relational algebra, however, it may contain additional annotations, e.g. speciﬁc hints available in the SQL statement. What is the high-level task of the query optimizer? query optimizer The high-level task of the query optimizer is to translate the algebraic expression into an executable program (the HOW part). That executable program computes the result to the SQL query. Typically, the goal of the query optimizer is to generate an optimal program w.r.t. runtime. However, this does not need to be the optimization goal. For instance, alternative optimization goals may be to generate a program that is never worse than a full scan or a program that is not more than k-times slower than the optimal program. The latter goals emphasize robustness of query optimization rather than overall robustness performance. 218 Query Planning and Optimization Indexer Store Query Optimizer DBMS Overview SELECT A.title FROM A,B WHERE A.name =‘Hugo‘ AND A.id = B.dz Query Parser Query OptimizerCatalog Query Plan Interpreter Precompiled Modules Code Generation Code Execution ∏A.title(σA.name=’Hugo’ and A.id=B.dz (A×B)) declarative query algebraic expression physical query execution plan (QEP) executable program Figure 5.1: The positioning of the query optimizer in a DBMS; input and output of the query optimizer What is the relationship between the WHAT and the HOW aspects query planning? The SQL statement does not specify HOW the result to that query should be computed. Using an SQL query the user just declares WHAT he wants to retrieve from the database. The query optimizer then has to decide HOW to retrieve and compute that data from the database. That decision is an optimization problem that must be solved either when the SQL query is submitted or in advance, e.g. using prepared statements or any other form of pre-optimized queries like query warehousing. Why would the query optimizer need statistics about the data?statistics The query optimizer requires statistics about the data stored in the database as the decision for an optimal executable program may depend on those statistics. For instance, 5.1 Overview and Challenges 219 in order to decide for an SQL query whether to access rows in a table through a full scan or an unclustered index depends on the selectivity of the query. If we keep meaningful statistics for the database, e.g. histograms of data distributions, we may estimate or even precisely compute that selectivity and hence make a better decision. Where does the database obtain those statistics from? Statistics are derived from the database contained in the database management system. An important consideration is whether statistics are computed and updated automatically or whether the user (or a database administrator) has to trigger statistics collection manually. Make sure to check out the manual of your database product to be sure that statistics are collected and updated regularly. If not, the query optimizer will not be able to come up with optimal executable programs. What is query plan interpretation? query plan interpretation In query plan interpretation a program is generated using pre-compiled modules (called operators). Multiple operators are combined to form a query plan.A query plan is query plan adirected acyclicgraph (DAG) where thenodes areoperators and theedges imple- ment any variant of pipelining (typically an iterator-interface). That query plan is than interpreted by an executable program called the query plan interpreter (also called ONC- interface). Query plan interpretation is typically used by disk-based systems. In these systems, the CPU-overhead of interpretation is negligible compared to the I/O-costs. In amain-memorysystem, queryplan interpretation isabad choiceastheoverheadsfor interpretation may overshadow the total query execution costs. Make sure to not confuse query plan interpretation with interpretation used in scripting languages (like Python or PHP). In query plan interpretation, interpretation happens on a much more coarse- granular level (modules or operators) whereas scripting languages need to interpret every line of code. What is code generation? code generation Code generation means that the query optimizer generates an executable program without relying on pre-compiled modules like operators. Also, in contrast to query plan interpre- tation where we generate an operator DAG that is then executed by the query plan interpreter, in code generation, we generate an executable. Any programming language may be used to generate code, e.g. C++, assembly or LLVM. What is the relationship between query optimization and programming language compilation? programming language compilation The high-level task of a programming language compiler is to compile a program written in a speciﬁc programming language (e.g. Java, C++) into an executable binary. A query optimizer has a very similar task: it translates a program (a declarative SQL query) into an executable binary. Hence, query optimization can be considered a domain-speciﬁc compilation problem. A major diﬀerence to general purpose compilation is that in query optimization the variety of input programs is much more restricted. This allows us to apply domain-speciﬁc optimizations, e.g. join enumeration or tuple reconstruction. In addition, more precise statistics, e.g. statistics on data distributions, are available and can be considered for the optimization and compilation process. Notice that even though 220 Query Planning and Optimization the problem is domain-speciﬁc, this does not imply that the problem gets much simpler. Quizzes 1. What is the task of the query optimizer in a database system? (a) parsing queries (b) answering the query (c) deﬁning a plan to eﬃciently answer a query (d) gathering statistics of the database 2. What is the output of the query optimizer? (a) The result of the given query (b) The parse-tree of the query (c) Aplan (algorithm)thattells howaqueryshould be executed eﬃciently (d) Statistics of the tables touched by the query 3. What kind of information do you think is useful to the query optimizer? (a) Distribution of the attribute values (e.g. histograms) (b) Record count for each table (c) The number of user accounts in the database (d) Buﬀer sizes 5.1.2 Challenges in Query Optimization: Rule-Based Optimiza- tion Material Video: Original Slides: Inverted Slides: Learning Goals and Content Summary What is the canonical form of a query?canonical form If we translate a SQL query into relational algebra using only the base operators of relational algebra (cross product, selection, projection, union, minus, and rename), we get the canonical form of an SQL query. This canonical form serves as the input to further optimizations. It is the most basic (and trivial) form of a query plan.query plan When would the canonical form be a DAG (directed acyclic graph)?DAG If any input relation (or subtree) is input to more than one other operator, we receive a DAG. For instance, assume a self-join, i.e. we join a relation R with itself. Then R is at the same time the right and the left input to a binary join. Therefore the canonical form of the query is a DAG. 5.1 Overview and Challenges 221 Query Parser SELECT A1,..,An FROM R1,..,Rk WHERE P; ∏A1,...,An(σP (R1×...×Rk)) project ﬁlter input Input: query parser Canonical Form of a Query Input: ∏A1,...,An(σP (R1×...×Rk)) × ∏A1,...,An σP × × R1 R2 R3 Rk Figure 5.2: The query parser and the canonical form of a query What does rule-based optimization do in principle? rule-based optimization In rule-based optimization we apply transformation rules to plans. A rule should be applied only if it matches a certain condition. For instance, a rule might match a condition where a selection is applied on a single table after applying a cross product. The rule should then rewrite the plan to push down the selection through the cross product to perform the selection before the cross product. What does a single rule do? rule A single rule takes as its input a plan and emits a transformed plan. The transformed plan, when executed, produces the same result as the input plan. What is the advantage of breaking up conjunctions of selections? 222 Query Planning and Optimization Example for Both Transformation Steps SELECT title FROM A,B WHERE A.name =’Hugo’ AND A.id = B.dz; ∏title(σA.name=’Hugo’ ∧ A.id=B.dz (A×B)) × ∏title A B σA.name=’Hugo’ ∧ A.id=B.dz Push Down Selections × ∏title A B σA.id=B.dz σA.name=‘Hugo’× ∏title A B σA.name=’Hugo’ σA.id=B.dz Figure 5.3: Example transformation from SQL to the canonical form; a query rewrite pushing down a selection predicate If we break conjunctions into individual selections, this may enable us to apply additional rules, e.g. we may then be able to apply selection push-down. Why does it make sense to push down selections?push down This makes sense as it allows us to remove data from query processing as early as possible. Hence, that data does not have to be transmitted or processed by other operations. Selection push-down is a very generic technique that appears in many places of query processing, e.g. distributed query processing. Why should we prefer joins over cross products whenever possible? In most cases, except pathological ones where joins are not selective, a join has a much 5.1 Overview and Challenges 223 better runtime complexity, typically O(n) or O(n log n),than a cross product, O(n2). Cross products are only useful for very small inputs, if runtime is not critical or for testing purposes. When is it possible to introduce a join? If a cross product is followed by a selection using a predicate that is deﬁned on both inputs of the cross product, we may rewrite the cross product and the selection to a join. When is it possible to push down projections? Projections may be pushed-down through an operator if that operator does not require attributes which are removed by that projection. For instance, if a join is deﬁned on an attribute A but the projection does not preserve A, we may not push it down. Still, we may push it down by increasing the attribute list of the join to also include all additional attributes required by the operator. What does the data layout of the store have to do with pushing down projections? data layout Processing data in a row store is (and should) be very diﬀerent from processing data in a column store. In general, in a row layout it is somewhat easier to push-down selections and hence ﬁlter out tuples. In contrast, in a column store, it is easier to push-down projections and hence ﬁlter out entire columns. See also Section 5.3.5. What are the most important rules? The three most important rewrite rules are: 1. push down selections and projections 2. combine selections and cross products into joins 3. insert additional projections Quizzes 1. What is the main task of the rule-based optimizer (RBO)? (a) To apply certain pre-deﬁned rules that could optimize the query plan (b) To analyze the current workload and optimize the query plans based on that information (c) To eﬃciently parse the queries 2. Given two queries Q1 and Q2. Assume a rule-based optimizer is called on Q1 and then on Q2. The result of the calls may lead to diﬀerent plans even though (a) the strings of Q1 and Q2 are equal. (b) the strings of Q1 and Q2 are equal, except: some constants in the WHERE- clauses diﬀer. (c) Q1 is semantically equivalent to Q2, i.e. it returns the same result set on any instance of the database having the same schema. 224 Query Planning and Optimization (d) Q1 returns the same result set as Q2. 3. Could the rule-based optimizer actually have a negative impact on query perfor- mance rather than a positive one? (a) Yes (b) No 5.1.3 Challenges in Query Optimization: Join Order, Costs, and Index Access Material Video: Original Slides: Inverted Slides: Additional Material Literature: [LÖ09], Access Path [LÖ09], Join Order Learning Goals and Content Summary What is join order?join order For a query plan that contains multiple joins, we often have a choice in which order we execute the diﬀerent join operations. For instance, assume three input relations A, B,and C and a query A ./ C ./ B.We may execute this plan as either (A ./ C) ./ B,i.e. the join of A and C is executed ﬁrst, or alternatively A ./ (C ./ B) or (A ./ B) ./ C.Notice that a particular order assumes a suitable join predicate on all pairwise joins. If no such predicate exists, that pairwise join is equal to a cross product and hence it is likely that the particular join order does not have to be considered for query optimization. What may be the eﬀect of picking the wrong join order? If we pick an unsuitable join order, we may end up with a slow query plan. See also Figure 5.4. In this example, we consider the sizes of intermediate results produced by the joins as our optimization criterion. The join C ./ B produces a large intermediate result set which then serves as the input to the join with A. In contrast, the join A ./ C produces a relatively small intermediate result. Hence, the input to the join with B is smaller. For this example, the join order (A ./ C) ./ B is optimal w.r.t. to the number of intermediate results produced. Notice that the third plan (A ./ B) ./ C is assumed to be non-selective and hence we did not consider it. Is the join order problem only relevant for joins? No, the “join order” problem exists for all binary operations that are commutative and associative, e.g. union, intersection, and cross product. In particular for intersections, e.g. in the context of a search engine intersecting IDs from diﬀerent posting lists, the 5.1 Overview and Challenges 225 Eﬀects of Join Order A B Plan 1: ⋈ ⋈ C 100 100,000 1,0001,000 1,000 |A|=|B|=|C|=1,000 Plan 1: Top-level join has to process 1,000 + 100,000 tuples. Plan 2: Top-level join has to process 100 + 1,000 tuples. Plan 2: A B ⋈ ⋈ C 100 100 1,000 1,0001,000 |C⋈B| |C| × |B| = 100,000 1,000 × 1,000 = 0.1selC ⋈ B := |A⋈C| |A| × |C| = 100 1,000 × 1,000 = 0.0001selA ⋈ B := Eﬀects of a Scan Access Path ∏title A B ⋈A.id=B.dz σA.name=‘Hugo’ ∏title,id ∏title B ⋈A.id=B.dz σA.name=‘Hugo’ ∏title,id A scan on A Figure 5.4: The possible eﬀects of join orders and access paths order of intersections may make a diﬀerence. How do we pick the right access path? access path Unfortunately, this is not as easy as we might imagine. For instance, if we have the choice of scanning a table or using an index, it sounds like a trivial decision: let’s use the index as it is computationally cheaper than any scan. However, like that we may end up with a query that is orders of magnitude slower than a simple scan. How is that possible? The issue is again the I/O-costs of a particular access method rather than the computational costs. The I/O-costs may overshadow the overall costs of a query by orders of magnitude. Hence, in particular for an unclustered index which may trigger many random accesses to the database store, we should be careful with our decision for a particular access 226 Query Planning and Optimization Eﬀects of an Index Access Path σA.name=‘Hugo’ A ∏title B ⋈A.id=B.dz ∏title,id A σA.name=‘Hugo’ ∏title B ⋈A.id=B.dz ∏title,id index on A.name Estimating Scan vs Index Access Costs σA.name=‘Hugo’ ∏title,id unclustered index on A.name selectivity costs few results many results ∏title,id σA.name=‘Hugo’ A scan on A expected costs: one random I/O to go to start of A, then sequential read unclustered index on A.name scan on A A expected costs: as clustered index, plus random I/O to store depending on selectivity Figure 5.5: Eﬀects of an index access in a plan; estimating costs for scans vs estimating costs for index accesses plan. Unclustered indexes are only competitive for highly selective queries (few tuples qualify). Hence, in order to pick the right access plan, we have to understand for each table mentioned in a query: what type of indexes exists for which attribute? What are the costs of using that index rather than scanning the table? In order to answer this question, the query optimizer requires statistics about the data distributions and good estimates on predicate selectivities. How does the query optimizer decide which access plan to take? The query optimizer has to estimate the costs of the diﬀerent alternatives. Based on these estimates, the query optimizer picks the plan with the lowest estimated costs. Keep in mind that those estimates may be wrong, i.e. the optimizer may pick a plan that is 5.1 Overview and Challenges 227 not the most eﬃcient plan in reality. What are the possibly diﬀerent costs of picking a scan, clustered index, unclustered index or covering index in a disk-based system? See Table 5.1 for a textual overview of the estimated costs. These costs can then be Scan Clustered Unclustered Covering Index index index one random I/O to go to start of A, then sequential read one random I/O to fetch leaf, one ran- dom I/O in store, then ISAM as clustered index, plus random I/O to store, depending on selectivity one random I/O to fetch leaf, then ISAM, no need to go to store Table 5.1: estimated I/O-costs of diﬀerent access methods expressed by a cost model. That cost model must be calibrated to reﬂect the reality of the system, i.e. we need to benchmark the performance characteristics of the underlying hardware. Those measurements should then be used to ﬁne-tune the cost model. Recall that: Essentially, all models are wrong, but some are useful. [George E. P. Box] Why should we be very careful with unclustered indexes? Unclustered indexes are very fragile w.r.t selectivity estimates. This means, if we estimate an index to return 100 result tuples, but in reality the index returns 200 result tuples, this diﬀerence may be enough to slow down the ﬁnal query substantially. Hence, if in doubt: do not use an unclustered index. What may happen if the selectivity estimates of a query are wrong? selectivity estimate Again, we may end up with a query plan that is by orders of magnitude slower than the optimal plan. Quizzes 1. When performing a join operation does the order of the operands have any impact on the performance of the join in general? (a) Yes (b) No 2. When the query optimizer decides to use an index for retrieving data for operands in a certain operation, say a join, would it be possible that the use of this index slows things down? (a) Yes (b) No 228 Query Planning and Optimization 3. Assume you have to perform a query that has a very low selectivity on attribute A, i.e. the size of the output is not much smaller in comparison to the input size of the query. The database ships with a physical design advisor. It suggests to build an unclustered index on A (this index is not currently present in the system). Does the suggestion of the physical design advisor make sense to speed up future queries that are similar to the current one? (a) Yes, deﬁnitely (b) Rather not 4. When the query optimizer is trying to decide how to perform a join operation, what kind of optimization ﬁts better? (a) Cost-based optimization (b) Rule-based optimization 5.1.4 An Overview of Query Optimization in Relational Systems Material Literature: [Cha98] Sections 1–4.1.1 Additional Material Literature: [LÖ09], Query Optimization [LÖ09], System R (R*) Optimizer [LÖ09], Query Processing Learning Goals and Content Summary What should we be aware of when reading this article? We should be aware that this article describes the state-of-the-art in query optimization as of 1998. In addition, this article focusses on disk-based systems. Query optimization for main-memory and other systems may be very diﬀerent. However, in order to understand how query optimization works for the latter, it is is important to understand these basic techniques ﬁrst. What is a physical operator?physical operator Aphysicaloperatoris asoftware entity operatingon one ormultiple inputstreams and producing one or multiple output streams. In the context of a relational database sys- tem a physical operator is considered a speciﬁc algorithm and/or implementation of a logical operator.A logical operator is any of the relational algebra operators, e.g. join,logical operator projection, intersection, etc. A physical operator is one possible implementation of a log- ical operator. For instance, the logical operator join may be implemented as a physical operator taking two input streams and producing one output stream. Algorithmically, 5.1 Overview and Challenges 229 we may use either a hash-join (see Section 4.1.2), a sort-merge join (see Section 4.1.3) or aco-grouped join (see Section 4.1.4). Hence, we havea leastthree diﬀerent physicaljoin operators. Most databases allow you to inspect the physical plan generated for a given SQL query and also display the physical operators used in that plan. In most database systems this can be done by writing EXPLAIN in front of the SQL-command. What is the algebraic representation of a query? algebraic representation An algebraic representation of a query (aka logical plan)is anysuitable represen- logical plan tation of relational algebra, e.g. rather than writing (R ./ S) ./ T we may write join(join(R,S),T).However, thatrepresentation should notcontain information on physical properties, i.e. the speciﬁc algorithms used to implement the algebraic opera- tions. The latter is done in a physical plan. What is a cost estimation in the context of the query optimizer? cost estimation query optimizerThe cost-based query optimizer estimates the costs of (possibly) a large number of plans. Based on these estimates the query optimizer chooses one plan for execution. Recall our discussion in Section 5.1.3. What type of search space did System-R use? search space System-RThe search space of System-R focussed on linear trees (aka left-deep trees). linear treesWhat is a left-deep tree? left-deep treeGiven a sequence of input relations R1,. .., Rn,a left-deep tree starts by a binary join on R1 and R2. Then it iteratively adds binary joins R3,. .., Rn. What is a bushy tree? bushy tree Abushytree is atree that is not leftdeep, i.e.at leastforone of the joins in thatplan its right input is not an input relation but another join operation. Notice that the union of the set of bushy trees and the set of left-deep trees forms the set of all possible trees. set of bushy trees set of left-deep treesWhat must be considered when applying transformations? We can apply transformations... 1. heuristically, i.e. we assume that the transformation always improves the plan. In that case, plan enumeration is applied only after applying certain transformations. Ahiddenassumptionhere is that even afterapplying those transformations, we can still ﬁnd the globally optimal plan. However this may not be true in all cases. A transformation may rewrite a plan such that only a locally optimal plan is reachable in cost-based enumeration. 2. cost-based, i.e. transformations are used as part of cost-based plan enumeration in the ﬁrst place. Like that we can ﬁnd a globally optimal plan, however, we need to estimate the costs of a larger number of plans. What is an interesting order? interesting order Originally, an interesting order described a situation where one operator, e.g. a sort operator, produces its output ordered by a particular attribute. That order could then 230 Query Planning and Optimization be exploited by one of the next operators. For instance, assume we do a sort-based grouping followed by sort-merge join. Further assume that both grouping and join are on the same attribute. In that situation, we do not need to sort the output of the sort- based grouping again in order to perform the merge-join as they are already sorted. We exploit the ‘interesting order’ produced by the grouping for the following join. In the general case, any interesting physical property produced by a subplan may be useful interesting physical property and possibly exploited by subsequent operators. Other examples of exploitable physical properties that either exist before executing a query or are created as a side-eﬀect of query processing include: partitioning, partially sorting, and indexing. Do bushy join sequences require materialization of intermediate relations?bushy join intermediate relation In Section 4.1.1. of the article, you ﬁnd the sentence “Bushy join sequences require ma- terialization of intermediate relations.” Well, this is not entirely true. A simple counter example is the case where all joins are merge joins on the same attribute. This leads us to a more general question: when to materialize what exactly in a query plan? We can materialize in-between physical operators, inside physical operators (sometimes we mate- rialize entire relations inside an operator!) or we stop thinking along physical operators altogether. We will look at this in Section 5.3 in more detail. Quizzes 1. What is the main task of the rule-based optimizer (RBO)? (a) To apply certain pre-deﬁned rules that could optimize the query plan (b) To analyze the current workload and optimize the query plans based on that information (c) To eﬃciently parse the queries 2. Given two queries Q1 and Q2. Assume a rule-based optimizer is called on Q1 and then on Q2. The result of the calls may lead to diﬀerent plans even though (a) the strings of Q1 and Q2 are equal. (b) the strings of Q1 and Q2 are equal, except: some constants in the WHERE- clauses diﬀer. (c) Q1 is semantically equivalent to Q2, i.e. it returns the same result set on any instance of the database having the same schema. (d) Q1 returns the same result set as Q2. 3. Could the rule-based optimizer actually have a negative impact on query perfor- mance rather than a positive one? (a) Yes (b) No Exercise You are given the following query: 5.2 Cost-based Optimization 231 SELECT A.b , B.c , C.d , D.e FROM A, B , C , D WHERE A.x=B.x AND A.a <24 AND A.c=C.d AND B.d=D.b AND D.a >24 AND C.d=D. c AND C.b >45 AND B.x =23 (a) Create a relational algebra tree using only cross products, selections, and projections for this query. (b) Use rule-based optimization to improve the logical plan. Explain what you did and draw the new plan as a relational algebra tree. 5.2 Cost-based Optimization 5.2.1 Cost-Based Optimization, Plan Enumeration, Search Space, Catalan Numbers, Identical Plans Material Video: Original Slides: Inverted Slides: Additional Material Literature: see 5.1.4 Further Reading: WDeﬁnition from Wolfram W Deﬁnition from WIkipedia [OL90] Learning Goals and Content Summary What is the overall idea in cost-based optimization? The overall idea of cost-based optimization is to: 1. enumerate the set of all plan alternatives, 2. then for each plan estimate the costs of executing it, 3. pick the plan with the lowest estimated costs and execute it. What is the number of possible left-deep plans for n input relations? The number of possible left-deep plans for n input relations is n!. What is the number of possible bushy trees for n input relations? This is given by the Catalan number Cn\u00001 which is deﬁned as: 232 Query Planning and Optimization Figure 5.6: Search space for left-deep and bushy trees Cn = 1 n +1 ✓2n n ◆ = (2n)! (n + 1)n!n! = (2n)! (n + 1)!n! or as a recurrence relation: C0 =1 and Cn+1 = nX i=0 CiCn\u0000i for n \u0000 0.s For n =0, 1, 2, 3,. .. this yields the sequence 1, 1, 2, 5, 14, 42, 132, 429, 1430, 4862, 16796, 58786, 208012, 742900, 2674440, 9694845, .. .. What is the hidden assumption in this calculation when counting bushy plans? 5.2 Cost-based Optimization 233 The hidden assumption in this calculation is that all binary joins are asymmetric and hence cannot be commuted, i.e. R ./ S is assumed to be always diﬀerent from S ./ R.Notice that this is not necessarily always the case, e.g. SortMergeJoin(R,S) = SortMergeJoin(S,R).Similar symmetries mayexist forpartitioningjoins. Quizzes 1. What is the rough idea of a cost-based query optimizer? (a) Explore the search space to obtain the optimal way of performing the given operation under a given cost model. (b) Apply certain pre-deﬁned rules that are known to potentially give good per- formance 2. What is the (exact) total number of options when the query optimizer considers left- deep trees? Assume n is the number of operands involved and that the operation is commutative and associative. (a) 1 (b) 4n (c) n (d) n! 3. Can a bushy plan degenerate into a left-deep plan? (a) Yes (b) No 4. What is the (exact) total number of options when the query optimizer considers bushy plans? Assume n is the number of operands involved, and that the operation is commutative and associative. Cn denotes the n-th Catalan number. (a) Cn\u00001 (b) n! · Cn\u00001 (c) Cn\u00001! (d) nCn\u00001 5. Which of the following parenthesization describes left-deep plans? The symbol () represents a binary operation and A, B, C, D, E are the operands (a) (((A,B), C),(D,E)) (b) ((((A, B), C), D), E) (c) (((A, B),(C, D)), E) (d) (A, (B, (C, (D, E)))) 234 Query Planning and Optimization 6. Assume you have two diﬀerent plans that produce the same number of intermediate results. Should you be careful when choosing which algorithm to use to perform the corresponding join operations? (a) Yes (b) No 5.2.2 Dynamic Programming: Core Idea, Requirements, Join Graph Material Video: Original Slides: Inverted Slides: (slides for 5.2.2, 5.2.3, and 5.2.4) Additional Material Literature: V Recursion with memoization [LÖ09], Query Optimization (in Relational Databases) Further Reading: WDynamic Programming Learning Goals and Content Summary What is the join graph?join graph The core idea of a join graph is to represent the join predicates for a given query. The join graph allows us to restrict join enumeration to plans without cross products. A join graph is deﬁned as G =(V, E) where the set of vertices V represents all relations referenced in the query and the set of edges E represents all join predicates (also inferable join predicates). As any pair of relations may trivially be “joined” through a cross product anyway, a join graph may be considered to be dense, i.e. all edges are at least set to the join predicate JP (x, y)= true or to a (non-trivial) selective join predicate. However, typically only the non-trivial join predicates are considered and displayed in a join graph. Thus, whenever there is a (non-trivial) join predicate on two relations, we represent that join predicate through an edge. In the video, we use thin white edges to visualize the trivial cross products and thick yellow lines to visualize nontrivial join predicates. What is the core idea of dynamic programming? dynamic programming The core idea of dynamic programming is to construct larger optimal solutions by assem- bling smaller optimal solutions. In other words, dynamic programming works best if an optimal solution to a problem may be constructed by ﬁrst solving smaller subproblems which may then be combined to solve a bigger problem. What are the two requirements for dynamic programming ro make sense? There are two requirements: 5.2 Cost-based Optimization 235 Join Example and its Join Graph D C B A B C ⋈ ⋈ ⋈ × × × A B C D SELECT \t A.a1, B.a2, B.a3, D.a4 FROM \t A\tJOIN\t B ON A.id=B.a_id \t JOIN\t D ON D.b_id=B.id \t JOIN\t C ON C.id=D.c_id WHERE\t A.a1 = 42 \t AND B.a3=12 \t AND D.a2=25 ⋈A ⋈ ⋈B D C σa1=42(A) C σa3=12(B) σa2=25(D) A.id=B.a_id C.id=D.c_id D.b_id=B.id Figure 5.7: The join graph and its relationship to dynamic programming 1. Optimality principle. This means that an optimal solution can be found by as- sembling optimal solutions to smaller subproblems. In the context of plan enumer- ation, this means that in order to ﬁnd an optimal join order to a query referencing relations R1,. .., Rn,we may ﬁnd that optimal join order by ﬁrst computing op- timal solutions on subsets of the relations R1,. .., Rn and then composing those subsets into bigger sets until eventually we ﬁnd the optimal solution for the entire set of relations. Notice that this principle does not imply that any combination of subsets leads to the optimal plan. For instance, ﬁnding optimal smaller solutions on relations R1,. .., Rk and Rk+1,. .., Rn,for 1 <k < n,only and then combin- ing them may not ﬁnd the optimal solution. This may be the case if the optimal solution contains R1,Rn as an optimal sub-solution. 236 Query Planning and Optimization 2. Overlapping Subproblems. This means that we face a situation where the sub- problems overlap, i.e. the subproblems are not disjoint. If subproblems do not overlap, i.e. they are disjoint, we could directly use divide-and-conquer. Consider the case of partitioning (like in Quicksort): once we partitioned the data into parti- tions, each partition may be treated independently (e.g. by a separate thread). Once all partitions have been treated, we simply combine the results. This principle is applied recursively in Quicksort. In contrast, when dealing with overlapping subproblems, we cannot partition the problem into smaller disjoint subproblems. In other words: there (potentially) exist multiple partitionings of the same problem. This is exactly the case in join enumeration: assume a query referencing relations R1,R2,R3.Let’s assume we simply want to compute the optimal join order (we do not even consider physical join algorithms and we assume that all joins are symmetric). Then, we must not simply combine R1 ./ R2 (an optimal subplan) with R3 as this would be one random subplan of size two combined with the remaining table. Using this optimal subplan may not necessarily result in an optimal plan for the entire join operation. Hence, in order to ﬁnd the optimal join order among R1,R2,R3 we also have to consider the optimal subplans for R2 ./ R3 and R1 ./ R3. What is an optimal subplan?optimal subplan Assume a query referencing relations R1,. .., Rn.A subplan SP references a real subset S of those relations, i.e. S ⇢ {R1,. .., Rn}.We call SP optimal if no other subplan with lower costs exists for S. Optimal subplans are the building blocks of dynamic programming. We may use them as we assume that join orders may be computed relying on the optimality principle explained above (in Section 5.2.3, we will see a situation where the optimality principle does not hold). Quizzes 1. What are the requirements for applying dynamic programming? (a) the optimal solution can be constructed from the optimal solutions of its sub- problems (b) smaller subproblems should appear multiple times in larger subproblems (c) the join graph is connected 2. When solving a problem using dynamic programming, are subproblems that are encountered more than once recomputed each time? (a) No (b) Yes 3. Dynamic programming solves a problem by breaking it into smaller pieces. What is then the diﬀerence between dynamic programming and a divide-and-conquer al- gorithms? 5.2 Cost-based Optimization 237 (a) no diﬀerence at all (b) with dynamic programming we can optimally solve problems with dependent subproblems (c) with divide and conquer we usually solve problems with independent subprob- lems 5.2.3 Dynamic Programming Example without Interesting Or- ders, Pseudo-Code Material Video: Slides: see 5.2.2 Additional Material Literature: see 5.2.2 Further Reading: see 5.2.2 Learning Goals and Content Summary How does dynamic programming work when applied to the join enumeration problem? dynamic programming join enumerationAssume a query on input relations R1,. .., Rn. In the join enumeration problem, we apply dynamic programming as follows: 1. for each input table R1,. .., Rn compute the set of subplans that access a single input table 2. for each subset: only keep the subplan with the lowest estimated costs 3. set plansize =2 4. compute all subplans that access a subset of size plansize from the input relations {R1,. .., Rn};use smaller plans (already stored in previous steps) as building blocks to compute those plans 5. for each subset: only keep the subplan with the lowest estimated costs 6. if plansize is equal to n,we are done,else: increase plansize by one and goto step 4. What does the pruning step do? pruning The pruning step removes all subplans that are expected to have higher costs than other subplans on the same subset of input relations. In other words, for each subset, we keep asingle plan thatisestimated tohave thelowestcosts. How many diﬀerent entries does the table have at each iteration step? iteration Asingle iteration for plansize adds \u0000 n plansize\u0000 entries to the table, i.e. the number of subsets of size plansize. This calculation assumes that all subplans are enumerated even 238 Query Planning and Optimization Example: Size 1 Plans (generating...) Optimal SubplansOptimal SubplansOptimal SubplansOptimal SubplansOptimal Subplans subgraph considered subgraph considered subgraph considered subgraph considered best plans A B C D best plans X scan(A), iseek(a1, A), isam(id, A) X scan(B), iseek(a3, B), isam(id, B) X scan(C), isam(id, C) X scan(D), iseek(a2, D), isam(id, D) σa1=42(A) C σa3=12(B) σa2=25(D) A.id=B.a_id C.id=D.c_id D.b_id=B.id assuming no interesting orders Example: Size 1 Plans (pruning...) Optimal SubplansOptimal SubplansOptimal SubplansOptimal SubplansOptimal Subplans subgraph considered subgraph considered subgraph considered subgraph considered best plans A B C D best plans X scan(A), iseek(a1, A), isam(id, A) X scan(B), iseek(a3, B), isam(id, B) X scan(C), isam(id, C) X scan(D), iseek(a2, D), isam(id, D) σa1=42(A) C σa3=12(B) σa2=25(D) A.id=B.a_id C.id=D.c_id D.b_id=B.id X X X X X XX assuming no interesting orders Figure 5.8: Dynamic programming: size 1 plan generation and pruning though they contain cross products. If plans with cross products are ignored, fewer plans will be inserted. In the worst case, the total number of entries required is: nX plansize=1 ✓ n plansize ◆ =2 n \u0000 1. What does the mergePlan()-method do? How would it merge two subplans?mergePlan() Merging two disjoint subplans implies connecting those subplans through either a join (if ajoin predicate exists)orthrough across product. Recallthatdiﬀerent join algorithms vary in costs. Hence, they must be considered in the costing of the merged plan. 5.2 Cost-based Optimization 239 Example: Size 3 Plans (pruned) Optimal SubplansOptimal SubplansOptimal SubplansOptimal SubplansOptimal Subplans subgraph considered subgraph considered subgraph considered subgraph considered best plans A B C D best plans X iseek(a1, A) X iseek(a3, B) X scan(C) X iseek(a2, D) X X iseek(a1, A) SHJ iseek(a3, B) X X iseek(a1, A) CP scan(C) X X iseek(a1, A) CP iseek(a2, D) X X iseek(a3, B) CP scan(C) X X iseek(a3, B) SHJ P iseek(a2, D) X X iseek(a2, D) SHJ scan(C) X X X (iseek(a1, A) SHJ iseek(a3, B)) CP scan(C) X X X (iseek(a1, A) SHJ iseek(a3, B)) SHJ iseek(a2, D) iseek(a2, D) X X X (iseek(a2, D) SHJ scan(C)) CP iseek(a1, A) X X X (iseek(a3, B) SHJ iseek(a2, D)) SHJ scan(C) σa1=42(A) C σa3=12(B) σa2=25(D) A.id=B.a_id C.id=D.c_id D.b_id=B.id assuming no interesting orders Dynamic Programming costs( Plan ):\r \u0000 Integer\r //cost function estimating costs for a plan DynamicProgramming( R1, ..., Rn, costs() ):\t //relations Ri; cost function for pruning \r For i = 1 to n:\r\r //i.e. all S ⊂ {R1, ..., Rn} with |S| == 1: \t\t optPlan[{Ri}] := AccessPlans( Ri );\t //get all possible plans for Ri \r\r prune( optPlan[{Ri}], costs() );\r //prune set of plans using cost function \t For plansize = 2 to n:\t //for all subplans having at least two inputs \r\r ForEach S ⊂ {R1, ..., Rn} with |S| == plansize:\r //inspect each proper subset S of that size \r\r \r optPlan[S] := ∅;\r //initialize optPlan[S] with empty set \r\r \r ForEach O ⊂ S:\r //inspect each proper subset O of S \r\r \r \r optPlan[S] ∪=\r //extend optPlan[S]-entry to contain... \t\t \t \t mergePlans( optPlan[O], optPlan[S\\O] );\t //..the merged plan of two optimal subplans \r\r \r \r prune( optPlan[S], costs() ); \r //prune set of plans using cost function \r prune( optPlan[{R1, ..., Rn}], costs() );\r //ﬁnal pruning, i.e. pick the ﬁnal plan \t return optPlan[{R1, ..., Rn}];\t //return the ﬁnal plan Figure 5.9: Dynamic programming: size 3 pruned plans and pseudo code Quizzes 1. What paradigm does dynamic programming use? (a) Bottom-up (b) Top-down 2. Are cost models important for designing dynamic programming algorithms for databases? (a) Yes (b) No 240 Query Planning and Optimization 3. Does dynamic programming work by building up a table, that has to be kept ex- plicitly, containing the optimal solutions for the sub-problems met so far? (a) No (b) Yes 5.2.4 Dynamic Programming Optimizations: Interesting Orders, Graph Structure Material Video: Slides: see 5.2.2 Additional Material Literature: see 5.2.2 Further Reading: see 5.2.2 Learning Goals and Content Summary Why do we have to change the simple dynamic programming algorithm to consider inter- dynamic programming esting orders? Dynamic programming relies on the optimality principle. This principle is violated if two subplans joining the same subset of relations produce diﬀerent interesting orders. Hence, if we do not change the dynamic programming algorithm, we may not ﬁnd the optimal plan. When exactly would we change the dynamic programming algorithm? When pruning subplans, we need to keep for each subset of relations AND diﬀerent interesting order the best plan. Hence, we change dynamic programming as follows (compared to the algorithm shown in Section 5.2.3). Assume a query on input relations R1,. .., Rn. In the join enumeration problem, we apply dynamic programming as follows: 1. for each input table R1,. .., Rn compute the set of subplans that access a single input table 2. for each subset: for each interesting order: keep the subplan with the lowest estimated costs 3. set plansize =2 4. compute all subplans that access a subset of size plansize from the input relations {R1,. .., Rn};use smaller plans (already stored in previous steps) as building blocks to compute those plans 5. for each subset: for each interesting order: keep the subplan with the lowest estimated costs 5.2 Cost-based Optimization 241 Optimization 1: Interesting Orders Optimal SubplansOptimal SubplansOptimal SubplansOptimal SubplansOptimal Subplans subgraph considered subgraph considered subgraph considered subgraph considered best plans A B C D best plans X scan(A), iseek(a1, A), isam(id, A) X scan(B), iseek(a3, B), isam(id, B) X scan(C), isam(id, C) X scan(D), iseek(a2, D), isam(id, D) X X isam(id, C) MJ sort( iseek(a2, D) ), ... σa1=42(A) C σa3=12(B) σa2=25(D) A.id=B.a_id C.id=D.c_id D.b_id=B.id X X X X Optimization 2: Exploit Graph Structure Optimal SubplansOptimal SubplansOptimal SubplansOptimal SubplansOptimal Subplans subgraph considered subgraph considered subgraph considered subgraph considered best plans A B C D best plans X iseek(a1, A) X iseek(a3, B) X scan(C) X iseek(a2, D) X X iseek(a1, A) SHJ iseek(a3, B), ... X X ... X X ... X X ... X X iseek(a3, B) SHJ P iseek(a2, D), ... X X scan(C) SHJ iseek(a2,D), iseek(a2,D) SHJ scan(C), ... σa1=42(A) C σa3=12(B) σa2=25(D) A.id=B.a_id C.id=D.c_id D.b_id=B.id Figure 5.10: Dynamic programming: plan generation considering interesting orders and join graph 6. if plansize is equal to n,we are done,else: increase plansize by one and goto step 4. Why would we exploit the join graph in dynamic programming? The join graph allows us to detect whether a pair of relations can be combined through a selective join or through a cross product. Hence, we may exploit the join graph to never enumerate join orders containing cross products. 242 Query Planning and Optimization Quizzes 1. If the query optimizer does not take into consideration Interesting Orders when making decisions, would it be possible that its decision is actually not the best one? (a) Yes (b) No 2. Which of the following properties can be the beneﬁt of choosing the right physical subplan? (a) Sort order (b) Query selectivity 3. Which of the following optimizations, when designing dynamic programming algo- rithms, are explained in the video? (a) Interesting/physical property (b) Greedily choosing plans (c) Take graph structure into account Exercise Assume you want to compute the following query: SELECT A.b , B.c , C.d , D.e FROM A, B , C , D WHERE A. x = B . x AND A.c=C.d AND B.d=D.b AND C.d=D. c You have no indexes available and the relations have the following sizes in tuples: |A| = 20.000, |B| = 20.000, |C| = 20.000, |D| = 30.000 You are also given the following join selectivities: selA./B =0.01,selA./C =0.02,selB./D =0.02,selC./D =0.03 You can assume the selectivities to be independent. (a) How many diﬀerent join orders are possible for this query? (b) How many rows would you produce in the dynamic programming table to solve the join ordering problem if you include cross products and relation access plans? (c) Perform dynamic programming to ﬁnd the optimal join order. Take the join graph into account. Assume you have only simple hash join available and the following cost model: CSHJ (R ./ S)= |R| + |S|. 5.3 Query Execution Models 243 The cost of executing a join is just the number of input tuples it needs to process. Use a table with the following columns: Subproblem Cost Plan Outputsize (d) Perform dynamic programming to ﬁnd the optimal join order. Assume that only Grace hash joins (GHJ) and sort-merge join (SMJ) are available. The cost models are deﬁned as follows: If a join has two inputs R and S, then • The cost model for GHJ is: CGHJ (R ./ S)=3 ⇤ (|R| + |S|). • The cost model for SMJ is: CSM J (R ./ S)= |R| ⇤ (dlog1024(|R|)e + 1) + |S| ⇤ (dlog1024(|S|)e + 1) where |R| ⇤dlog1024(|R|)e and |S| ⇤dlog1024(|S|)e are the costs of sorting the inputs. Use a table with the following columns: Subproblem Cost Plan Outputsize Interesting Property 5.3 Query Execution Models 5.3.1 Query Execution Models, Function Calls vs Pipelining, Pipeline Breakers Material Video: Original Slides: Inverted Slides: Additional Material Literature: [LÖ09], Pipelining Learning Goals and Content Summary What are the options for executing a query plan? Aquery plan is not directlyexecutable. We have tosomehowtranslate it into executable code. There are several options for that including 1. function libraries: we wrap each physical operation, e.g. a hash join, into function. Then, every query plan can simply be translated into nested function calls. For 244 Query Planning and Optimization Approach 1: Function Library iseek(a1=42, A) SHJ MJC.b_id=B.id sortb_id ﬁltera3=12 isam(id, B)scan(C) iseek(a1=42, A) SHJ(.,.) MJC.b_id=B.id(.,.) sortb_id(.) ﬁltera3=12(.) isam(id, B)scan(C) SHJ( iseek(a1=42, A), MJC.b_id=B.id ( sortb_id( scan(C) ), ﬁltera3=12( isam(id, B) ) ) )= how to execute? Approach 2: Pipelining iseek(a1=42, A) SHJ MJC.b_id=B.id sortb_id ﬁltera3=12 isam(id, B)scan(C) iseek(a1=42, A) SHJ sortb_id ﬁltera3=12 isam(id, B)scan(C) MJC.b_id=B.id how to execute? F Figure 5.11: Translating a query plan to function calls or some sort of pipeline instance, we implement a ﬁlter as a function, a join as a function, as well as a scan. Like that we could treat a join on two relations R1 and R2 as a call: join( ﬁlter( scan(R1), \"a=42\" ), ﬁlter( scan(R2), \"b=2\" ), \"R1.ID=R2.x\" ); In this approach, function calls whose output is used by more than one other func- tion, i.e. the query plan is a DAG, may be materialized explicitly and then passed to all those functions separately. 2. precompiled modules: we wrap each physical database algorithm into an operator. Operators are connected through some sort of pipelining mechanism. 3. code compilation: we compile the physical query plan into executable code. 5.3 Query Execution Models 245 Pipeline Breakers? iseek(a1=42, A) ﬁltera3=12 isam(id, B)scan(C) MJC.b_id=B.id water tank water tank F SHJ sortb_id \t Blocking\t\t \t \t \t\t \t \t vs \t\t \t Non-Blocking Algorithms ﬁlterF projectP sort INLJ/SHJ (with bulkload) bulkload INLJ/SHJ (without bulkload) Merge MJ DPHJ Figure 5.12: Blocking and non-blocking algorithms used in query pipelines What is the problem with translating a query plan using a function library? function library If we translate a query to a series of function calls, each function will run to completion, materialize its entire result and only then returns it as a parameter to the (possible) next function call. This may be a bad idea, in particular for large intermediate results. For instance, consider a join producing a large intermediate result which is the input to another join. The entire result of that join needs to be materialized and temporarily stored. This consumes both storage space and storage bandwidth. What is the core idea of pipelining? Notice that by pipelining,we do not mean one speciﬁc implementation like Unix pipes. We rather consider pipelining a generic concept which may be implemented in many diﬀerent 246 Query Planning and Optimization ways. The core idea of pipelining is to allow functions to return some of their results before running to completion. Whether a function can in principle return some of its result before completion depends on the type of computation performed by the function. For instance, a min()-function, which may be implemented by scanning its entire input to return the minimal element, cannot return its result before consuming its entire input to completion (as the non-consumed input may always have another element that is smaller than the ones already consumed). What is a pipeline breaker?pipeline breaker Apipeline breakeris afunction thatmayonly return a(partial) resultafter consumingits entire input to completion. Examples of pipeline breakers include: grouping, aggregation, sorting, and partitioning. In contrast, some functions can be streamed perfectly, i.e. they inspect one tuple from the input and immediately produce 0, 1 or many results. Examples include: ﬁlter (selection), projection, and map. Why is it important to consider pipeline breakers in query processing? It is important to be aware of pipeline breakers as they implicitly materialize their entire inputs. Thus, in terms of pipelining, they do not have an advantage over simple function calls. What are the three stages of simple hash join and index nested-loop join? Both SHJ and INLJ have three stages each: 1. reading: they consume the left input (the build relation) entirely. In that process, implicitly, the entire left input is materialized. 2. once the left input is exhausted:using the buﬀered data, the algorithms bulkload ahash table oranyothersuitableindex structure. Noticethatthisphase may be interleaved with step 1. For instance, in SHJ once an item has been read, it is typically directly, i.e. item-wise, inserted into the hash table. 3. looping and probing: the right input (the probe relation) is read item-wise, each item is considered a query against the previously built hash table/index. A possible result of each query may be returned immediately. Hence, steps 1 and 2 are blocking. Step 3 is non-blocking. What are the three stages of quicksort? Quicksort has three stages: 1. reading: if quicksort does not operate directly on the data structure used in the previous step, e.g. an array, quicksort consumes its input entirely. In that process, implicitly, the entire input is materialized (and copied) in the algorithm. 2. sorting: once the input is exhausted, quicksort fully sorts the data. 3. outputting: the output is returned item-wise 5.3 Query Execution Models 247 Hence, steps 1 and 2 are blocking. Step 3 is non-blocking. Notice that steps 2 and 3 may overlap if the recursion in quicksort uses depth-ﬁrst search, i.e. every time a leaf-partition has been fully sorted, data in that partition may be returned directly. However, this does not change the blocking behavior of quicksort: the algorithm may only return the ﬁrst tuple of the sorted output, i.e. the min or max element of the input set, once the entire input has been consumed to completion. What are the three stages of external merge sort and external merge sort? External Merge Sort has three stages: 1. reading (with run generation): external merge sort consumes its input entirely. In that process, implicitly, the entire left input is materialized into runs, i.e. phase 0 of external merge sort. 2. merging (without ﬁnal merge): once the input is exhausted, external merge sort performs all merges except the ﬁnal merge on. 3. outputting (with ﬁnal merge): the ﬁnal merge is run and the output is returned item-wise Hence, steps 1 and 2 are blocking. Step 3 is non-blocking. Notice that steps 1 and 2 may overlap, i.e. some runs may already be merged before all runs have been created. What are the blocking and non-blocking building blocks of query pipelines? blocking non-blockingFigure 5.12 summarizes the most important blocking and non-blocking building blocks used in query pipelines. Quizzes 1. Once the physical execution plan for a given query has been determined, can the query be executed right away? (a) Yes (b) No 2. Is it in general a good idea to materialize intermediate results as they are created? (a) Yes (b) No 248 Query Planning and Optimization 5.3.2 Implementing Pipelines, Operators, Iterators, ResultSet- style Iteration, Iteration Granularities Material Video: Original Slides: Inverted Slides: Additional Material Literature: [LÖ09], Iterator W Java Iterator interface WJavaResultSet interface Learning Goals and Content Summary How do we implement a pipeline, i.e. how do we get from the high-level idea of pipeliningpipeline to a concrete implementation of pipelining? There are multiple ways to implement pipelining. The most prominent one is unix-pipes, i.e. multiple shell commands connected can be connected via the ‘|’-symbol, e.g. ‘foo | bar’. Here foo’s output is piped to bar and used as its input. However, many more imple- mentations of pipelining exist, e.g. any form of producer-consumer, operators, iterators, cursors, and java ResultSets. What are the core ideas and purposes of pipelining? The core idea of pipelining is to overcome the limitations of a simple function call which would deliver its result in one step only, i.e. after having fully completed. In contrast, pipelines allow us to forward a subset of the result to the next operation. This implies, in a pipeline like ‘foo | bar’, we may already forward a subset of foo’s result to bar which in turn may start computing without waiting for foo to complete. This may serve multiple purposes: (1) better resource utilization, i.e. higher degrees of parallelization, (2) less memory consumption, i.e. intermediate results do not (always) have to be materialized, and (3) lazy evaluation and early termination, i.e. operations may not need their entire inputs to compute a results, e.g. assume you compute the minimum over an already sorted sequence. What is the core idea of the operator interface?operator interface An operator interface allows us to return the result in chunks where each chunk represents asubsetof theentireresult. Each call to next() returns the next chunk. The next()- function should also be implemented in a way that this call triggers exactly as much work as is required to be able to return the next chunk, but not more ( lazy evaluation).lazy evaluation What happens in a hasNext()-call to an iterator?hasNext() iterator In hasNext() the iterator needs to determine whether a subsequent call to next() may return another chunk. In other words, the sole purpose of hasNext() is to de- termine whether the iterator may still deliver further result subsets or whether the iter- 5.3 Query Execution Models 249 Example Translation with Operators iseekOP(a1=42, A) SHJOP(.,.) MJOPC.b_id=B.id(.,.) sortOPb_id(.) ﬁlterOPa3=12(.) isamOP(id, B)scanOP(C) Figure 5.13: The operator interface and an example translation of a pipelining using operators. ator is already exhausted (completed). Unfortunately, in order to compute the result to hasNext(),often the entire next chunk has to becomputed anyways. Hence,inpractice it is a better design choice to not use a hasNext()-method but rather let next() indicate whether the element returned is part of the result or indicating that the iterator/operator is exhausted. What is a chunk and which special cases of chunks are important? chunk Again: a chunk represents a subset of the ﬁnal result. Therefore, we have many diﬀerent options to choose what a chunk represents. In a disk-based (volcano-style) pipeline each chunk may represent a tuple or a horizontal partition/set of tuples. In a main-memory system a chunk may represent an entire column or a horizontal partition of a column 250 Query Planning and Optimization (This is sometimes called vectorized pipelining. System examples include VectorWise.) What would be a simple, textbook-style, translation of a plan using operators? Asimple translation would be toleteach physicaloperation, e.g.ahash join orasort- merge join, implement the operator interface. A join will require two input operators to obtain its input. In turn a join will feed its results to one parent operator. An example translation of a pipeline implemented by operators is shown in Figure 5.13. What is ResultSet-style iteration?ResultSet JDBC uses ResultSets. This kind of interface is similar to iterators and operators, but not quite the same. The major diﬀerence is that in a ResultSet next() moves an internal pointer forward to the next result chunk. However, next() does not directly return that next chunk. In contrast, next() only returns a bool indicating whether the result set is exhausted or not. the actual data contained in a chunk has to be accessed through additional get*-methods. For instance, in JDBC chunks are considered to be rows. At- tributes in a row are numbered. Hence, a call getString(int attributeIndex) returns the attributeIndexth attribute of the row as a string. Are ResultSets restricted to rows or can we use it for columns as well?row column JDBC assumes each chunk to represent one row. However, ResultSet-style iteration could also be used to consider each chunk a column, a horizontal partition of a row or any other suitable subset of the result set. Would it make sense to iterate over pages?page Yes, absolutely. Assuming row layout, each page represents a horizontal partition (as- suming no row crosses a page boundary). In a column layout, each page represents a horizontal partition of a column, again: the latter is called vectorized processing. Quizzes 1. Which of the following is NOT a method of the Operator interface for implementing pipelines explained in the video? (a) open() (b) next() (c) remove() 5.3 Query Execution Models 251 5.3.3 Operator Example Implementations Material Video: Original Slides: Inverted Slides: Additional Material Literature: [BBD +01] Learning Goals and Content Summary Selection Operator class Selection implements Operator<Row>{ \t private Operator<Row> input;\t\t //internal handle to input Operator \t private Predicate<Row> sel; \t public Selection(Operator<Row> input, Predicate<Row> sel){\t\t //constructor \t\t this.input = input; this.sel = sel; \t } \t public void open(){input.open();}\t\t //initializes the operator \t public Row next(){\t \t //returns the next row of data \t\t For (Row tmp = input.next(); tmp != NULL; tmp = input.next()){ \t\t \t If( sel.execute( tmp ) ){ \t\t \t \t return tmp; \t\t \t } \t\t } \t\t return NULL;\t\t //signal end of input \t } \t public void close(){input.close();}\t\t //performs cleanup work (if necessary)\t } Save the State as an Attribute class Enumerate implements Operator<Integer>{ \t private int current, from, to; \t public Enumerate(int from, int to){\t\t //return [from;to], i.e. both including \t\t this.from = from; this.to = to; \t } \t public void open(){\t\t //initializes the operator \t\t current = from;\t\t //initializes the state of the operator \t } \t public Integer next(){\t \t //returns the next row of data \t\t If (current <= to){\t\t //if still in range \t\t \t Integer nextToReturn = current;\t\t //this is what we return \t\t \t current++;\t\t //need to increment internal state \t\t \t return nextToReturn;\t\t //return next element \t\t }\t\t \t\t Else return NULL;\t\t //signal end of input \t } \t public void close(){}\t\t //performs cleanup work (if necessary)\t } Figure 5.14: Pseudo code for selection and enumeration operators 252 Query Planning and Optimization How do we implement speciﬁc operators without unnecessarily breaking the pipeline? As a general rule we should implement operators such that they return subsets of their result as early as possible. Whether it is possible to return a speciﬁc result without running the operator until completion depends on the type of operation computed by the operator. For instance, if we want to compute the maximum value of an input of unsorted integers, it is impossible to return that maximum without consuming the entire input of the operator. Recall our discussion on pipeline breakers in Section 5.3.1. Is it hard to implement a projection or selection operator?projection selection operator Not really. Let’s assume row-wise iteration, i.e. we consider a chunk to represent a row. A selection operator may inspect each input tuple one by one whether it fulﬁlls the selection predicate. If the selection predicate holds, the selection operator may immediately return that tuple. If the selection predicate does not hold, the selection operator continues consuming its input until it ﬁnds a qualifying tuple. Similarly, for a projection, we rely on a mapping function projecting one input tuple to an output tuple anyways. This mapping function may be applied for each tuple independently. Hence, a projection operator simply needs to consume one tuple from its input. Then it apples the mapping function and returns the result. Only if its parent operator requests the next tuple, the projection operator will continue processing. How do we handle loops in operator implementations?loops If we want to return some result while being in the middle of a loop, we need to make sure that a subsequent call to next does not reinitialize the loop. This means, if we use aloop in a next-call, we need to be able to continue the loop at the position where we terminated it in the last next-call. This can easily be achieved by removing the variable initializations we are iterating on from the loop. Those variables become part of the state of the operator, i.e. attributes of the class we are implementing. For instance, in Figure 5.14, in class Enumerate,we introduce a private attribute current. This attribute stores the current position of the loop. Any call to next will access that attribute and change its state. What do we mean by ‘ state’ here?state State may be deﬁned in diﬀerent ways. In the context of operators we use state to denote any variables that are introduced and kept as part of an instance of an operator. This means, by state we mean additional state introduced to be able to continue processing later on for a speciﬁc operator. Examples of state that may be kept by an individual operator include: loop counters, temporary or buﬀered data, and intermediate results. We ignore in this discussion external state, i.e. state that is kept not solely for the purpose of this operator. Examples for external state include indexes that are kept by the database system and any temporary data that is buﬀered outside the scope of the operator. 5.3 Query Execution Models 253 Quizzes 1. What is the minimal set of methods each operator has to implement based on the Operator interface? (a) open(), next(), close() (b) open(), hasNext(), next(), close() (c) constructor, open(), next(), close() 2. Which of the following relational algebra operators can be implemented as stateless Operators on rows: (a) projection (b) selection (c) aggregation with grouping (d) natural joins 5.3.4 Query Compilation Material Literature: [Neu11] (Sections 1 to 4) Additional Material Code Example: W Java Code Example for Compiling Predicates Code Example Slides: Wslides Literature: WVisitor Pattern [Vig13] Learning Goals and Content Summary What is the performance problem with operators? Operators may incur a lot of overhead in terms of function calls for next-calls, i.e. in the extreme case there is one call for each row at each operator in the execution plan. These costs may not be a big problem in a disk-based system where the I/O-costs typically overshadow the function-call overheads. In a main-memory system however, the costs for these function calls may be substantial and become a bottleneck in query processing. How is the pipeline organized logically when compiling code? compiling code The query plan is divided into blocks where each block is run as one pipeline until the next natural pipeline breaker. A natural pipeline breaker is, for instance, a hash table materializing all of its inputs. Then each block is translated into program code (either using C/C++, LLVM or any suitable combination). 254 Query Planning and Optimization How to translate entire plans into LLVM/C++ code?LLVM C++ An entire plan ins translated using a mix of C++ and LLVM. The idea is to keep com- plex code, e.g. precompiled index structures or operators, written in C++, however code actually accessing data is kept in LLVM. The major design rationale is to write all ‘hot’ code, i.e. code that is executed many times in LLVM. Switching here and there to C++ does not hurt much as long as long as the heavy lifting is done in LLVM. What is the biggest advantage of using LLVM? The biggest advantage of using LLVM is orders of magnitude faster code compilation times compared to C++-compilers. This allows for just-in-time-compilation, i.e. when the query is issued to the database systems it may be compiled on the ﬂy and executed directly. This typically does not pay oﬀ with C++ as the compilation times cancel out the beneﬁts of compiled code. Quizzes 1. Is there any disadvantage of using the Operator or Iterator interface (implemented) to execute query plans? (a) No, there aren’t any. (b) Yes, they usually incur a virtual (function) call, or a call via a function pointer, which is more expensive than a regular function call. (c) Yes, they can only be applied when the Chunk parameter is implemented as pages of memory. 2. What is the main idea presented in the paper for executing queries in a DBMS? (a) The query plan is rewritten into a more eﬃcient algebraic representation. (b) The query is directly translated into (a compact and eﬃcient) machine code -thusconsideringany givenquery asasmall programthat must beexecuted by the DBMS. 3. The method described in the paper aims mainly at (on a high level)? (a) Maximizing data- and code locality. (b) Producing codes without branches. (c) Vectorizing all loops. 4. What is the goal of the method presented in the paper? (a) To make query execution operator-centric (b) To make query execution data-centric 5. At which point in query planning does the method explained in the paper diﬀer from the traditional (Operator-, Iterator-based) scheme? (a) After the initial algebraic expression is created, the method explained in the paper generates an optimized version, while the traditional scheme does not. 5.3 Query Execution Models 255 (b) After the initial algebraic expression is created and optimized, the method ex- plained in the paper compiles this optimized algebraic expression into machine code. (c) After the physical query plan is created, some joins are reordered, then the plan is translated into machine code. 6. What tool is used to generate machine code for a given query? (a) Java Virtual Machine (b) C++ compiler (c) Low Level Virtual Machine (LLVM) 7. Could the method presented in the paper be parallelized, say by SIMD instructions and/or multi-core parallelization? (a) Yes, data is consumed in chunks, which allows for parallelization. (b) No, data ﬂow is highly dependent on execution ﬂow, which make parallelization impossible. 8. From the experiments shown by the author comparing diﬀerent systems, what can we conclude? (a) The new LLVM-based code is not better than existing solutions. (b) The new LLVM-based code is not more eﬃcient, but compiling such optimized code is way faster than the existing solutions. (c) The new LLVM-based code is indeed more eﬃcient, both in execution- and compilation time, although the latter is in general the main advantage of the method presented. 5.3.5 Anti-Projection, Tuple Reconstruction, Early and Late Ma- terialization Material Video: Original Slides: Inverted Slides: Additional Material Further Reading: [MBNK04] [AMDM07] [IKM09] 256 Query Planning and Optimization Early Materialization T σA=8 SELECT B,C FROM T WHERE A=8 AND B=5 σB=5 ∏B,C T.B ∏B,C T.CT.A ⋈RID σA=8 σB=5 (Partially) Late Materialization T σA=8 SELECT B,C FROM T WHERE A=8 AND B=5 σB=5 ∏B,C T.B σA=8 σB=5 ∏B,C T.C T.A ⋈RID ∏RID ∏B,RID ⋈RID C ∏ = Figure 5.15: Early vs (partially) late materialization Learning Goals and Content Summary What is early materialization?early materialization In early materialization we read all attributes that are required to process a query as early as possible. This means for any plan requiring say attributes A, B, and C of table T, we replace T by the join over columns A, B, and C, as join key we use the RID. In other words, we read all three columns A, B, and C and keep them in a joint representation for further processing, typically a row layout, in this case a row of three attributes. See Figure 5.15 for an example. What is the relationship of early materialization to column and vertically partitionedcolumn (column grouped) layouts? 5.3 Query Execution Models 257 (Really) Late Materialization T σA=8 SELECT B,C FROM T WHERE A=8 AND B=5 σB=5 ∏B,C T.B σA=8 σB=5 ∏B,C T.C T.A ⋂RID ⋈RID ∏RID ∏RID B,C ∏ = Projection vs “Anti-Projection“ A B C When to narrow tuples? A B∏A,B When to widen tuples? C ∏ Figure 5.16: Late materialization and the relationship of projection and anti-projection In column grouping, suitable columns are grouped before the query is executed. In con- trast in early materialization, we group columns as the ﬁrst step of query processing at query time. Another way of seeing this is that in column grouping the RID join over the diﬀerent columns is already materialized in the store. You could consider the result of the RID join a materialized view already available in the store. Notice that as early materialization is not the best processing strategy in a column store, this relationship also gives you an intuition that vertically partitioning data in order to avoid the RID join is not necessarily a good strategy. What is late materialization? In late materialization all tuples are read as late as possible. For instance, in Figure 5.16 258 Query Planning and Optimization this is pushed to the extreme: the RID join over columns A and B does not even keep the values of column B. Only afterwards, in the RID join with C column B is read again to retrieve the necessary values. Notice that for this particular query B=5 for all tuples in the result set anyways. Hence, let’s hope for the best that the query optimizer will not read column B again anyways. However, this does not change the principle underlying problem: consider that we change the condition on column B to B>5. In that case, the query optimizer has to make a decision on when to fetch the attributes of B. What is partially late materialization? Whether we name this technique partially late materialization or partially early mate- rialization does not make a diﬀerence. The key property of this technique is that some attributes are read early as in early materialization, however other attributes are read later in the pipeline as in late materialization, e.g. when they are actually required in a particular operation, e.g. a join or in order to produce the ﬁnal result tuples. Thus this technique is a combination of both early and late materialization. An example for partially late materialization is shown in Figure 5.15. Here, attributes A and B are ma- terialized early and ﬁltered. Then we intersect their RID-lists to compute the conjunct (the AND in the WHERE-clause). Only after that we read attribute C (this is the late materialization part) and perform a RID join. What is an anti-projection and what is the relationship to tuple reconstruction joins?anti-projection tuple reconstruction join An anti-projection is the inverse operation to a projection. Recall that in relational algebra a projection removes some of the input attributes, i.e. it narrows the schema 1. In contrast, an anti-projection adds attributes to an input schema, i.e. it widens the schema. How could we implement early materialization in a column store? Again, early materialization simply implies that all required attributes are read, joined, and typically converted into a row-layout which corresponds to a materialized view of the input. What is the impact on query processing? The strategy used for tuple reconstruction may have considerable impact on the overall runtime of a query. In particular in cases where many attribute values need to be anti- projected, tuple reconstruction costs may overshadow all other costs of the query plan. What is the impact on query planning?query planning As any anti-projection can be seen as a join of a relation with the attribute to anti-project, we could phrase tuple reconstruction as a join order problem. Recall Section 5.1.3 where we considered diﬀerent orders of joining relations. In that section we still assumed that the inputs to the joins at the leaf-level of the join-tree are entire relations. However, if we assume that the input to a join is just one column of a relation, we could phrase anti-projections as a join order problem. All techniques that are suitable to the standard join order problem then also apply to tuple reconstruction. As the number of attributes of a schema is larger or equal than the number of its relations, enumerating all of these 1Technically, a projection may also keep the input schema as is. In that case the projection does not make much sense though. 5.3 Query Execution Models 259 options becomes quickly infeasible. See also the discussion in [SS16]. What is a join index? join index Ajoin indexmaterialized the pairs of tuples thatbelongtothe join result. Forinstance for a join of two relations R and S, the join index stores a table with two columns R.RID and S.RID where each row in that table marks a result belonging to the join of R and S. Notice that in contrast to a materialized view, which may materialize an arbitrary query (including a join), a join index materializes the join keys and/or RIDs only. Quizzes 1. Assume your are the developer of a DBMS, and assume you want to write code to perform the following query: SELECT A, B FROM C WHERE a1 < A < a2 AND b1 < B < b2. Does the exact same query processing algorithms perform equally well regardless of whether the DBMS is row-oriented or column-oriented? (a) No (b) Yes 2. Assume your are the developer of a column-oriented DBMS, and assume you want to write code to perform the following query: SELECT A, B FROM C WHERE a1 < A < a2 AND b1 < B < b2. Moreover, assume you already have the code for answering this query in a row-oriented DBMS. What paradigm helps you to reuse the existing code as much as possible? (a) Late materialization (b) Early materialization 3. Assume you perform the following query: SELECT D FROM T WHERE a1 <= A <= a2 AND b1 <= B <= b2 on a column-oriented DBMS. Which of the following paradigms project initially to row ID? (a) Early materialization (b) Late materialization 4. Could both early and late materialization be used in join processing as well, or only in selections? (a) Yes, they both can be used in join processing as well. (b) No, only late materialization can be extended to join processing. (c) No, only early materialization can be extended to join processing. 5. Which of the following is NOT an advantage of late materialization? (a) Lazy construction of tuples (only when necessary) (b) No re-accessing of the data (c) Potential minimization of intermediate results 260 Query Planning and Optimization 6. Which of the following makes tuples wider? (a) Projection (b) Anti-projection 7. When performing an equi-join using late materialization, what does get initially projected in the join index? (a) Column data (b) Row IDs Exercise Assume you have a main-memory column-store database with implicit keys, and table T containing 1, 000, 000, 000 tuples. (a) Provide two query plans: one using late-materialization and one using early- materialization for each of the following queries: Q1: SELECT a FROM T WHERE b< 4 AND d> 42; Q2: SELECT a, AVG (b) FROM T GROUP BY a HAVING AVG (b) > 100; Q3: SELECT a, b, MIN (c) FROM T WHERE b> d GROUP BY a, b HAVING MIN (d) > 100; (b) Assume that the selectivities of the ﬁlter conditions in Q1 and Q3 are as follows: sel(b< 4) = 0.02,sel(d> 42) = 0.2,sel(b> d)=0.125, and that these are independent from each other. Further, you have a column-scan operator which takes a bitlist of row-IDs as a parameter, and returns only the at- tributes stored at those positions. Our simplistic cost function considers only the 5.3 Query Execution Models 261 number of attribute values read from each column. Provide a cost-estimate for each of the query-plans you have created in (a). (c) There is more than one possible plan using late-materialisation for Q1. List all possible plans and provide the cost-estimates for them using the previously introduced cost function. 262 Query Planning and Optimization Chapter 6 Recovery 6.1 Core Concepts 6.1.1 Crash Recovery, Error Scenarios, Recovery in any Software, Impact on ACID Material Video: Original Slides: Inverted Slides: Additional Material Literature: [LÖ09], Logging and Recovery [LÖ09], Logging/Recovery Subsystem [LÖ09], Disaster Recovery [LÖ09], Crash Recovery 264 Recovery Example: Loss of Main Memory transfer 100 € from account A to account B ✘ power failure! transfer 300 € from account C to account B time committed, must be persisted did not commit, must not be persisted Figure 6.1: Possible eﬀects when loosing main memory due to a power failure Learning Goals and Content Summary What are possible error scenarios?error scenario Possible errors scenarios include: power failures, hardware failures (also due to outside occurrences like ﬁres, ﬂoods), burglary, and software errors. What could be the impact of a power failure on the database buﬀer?power failure database buffer In the worst case, if you only lose the contents of the database buﬀer, all dirty pages are lost, i.e. all changes that were made in the buﬀer but were not persisted on disk (or any other persistent medium) yet. What does the term ‘ recovery’ mean?recovery By recovery we mean that we can get back to a consistent state of the database according to the ACID properties. Recall again, that all transactions that were running but not committed at the time of the crash have to be replayed in their entirety. Is recovery just important for database systems? No, recovery is important in many diﬀerent situations in computer science. It makes software robust against failures. Recovery is strongly related to the undo-functionality you ﬁnd in many software products, to versioning, to backups and archiving, to journaled ﬁle systems (which typically can recover the metadata like the ﬁle system’s structure but not the ﬁle contents), and also embedded systems. In fact, any message like “do not switch oﬀ, system is performing XYZ” can be interpreted as a sign that recovery was not implemented. What is the impact of error scenarios on ACID?ACID Error scenarios typically impact atomicity, i.e. only some of the actions of a transcation were executed, and durability, i.e. only some of the actions of a transaction were made 6.1 Core Concepts 265 durable. These problems then also impact the consistency of the database. What is a local error? local error This is an error or ABORT in an application program, i.e. a program using the database through some interface (like JDBC). In addition, the DBMS may trigger a local error (a reset of one transaction) to resolve a deadlock situation in concurrency control. What is redo? redo Redo, in general, means that we have to reapply actions to the database state as those actions (for whatever reason) were not reﬂected in the database state. What is undo? undo Undo, in general, means that we have to remove eﬀects of actions done to the database state as those actions were reﬂected in the database state, however, have to be removed as they belong to uncommitted (loser) transactions. What does losing main memory mean? Losing main memory implies that we lose all dirty pages (clean pages, by deﬁnition, are all persisted on disk anyway). This means, all changes applied to those dirty pages are lost. What if we lose (some) external storage? This depends on how much of external storage we lose. If we “just” lose the database, but still have the log ﬁle, we can reconstruct the database using that log. Recall, that the database is just an optimized materialized view of the (unpruned) log. See also Section 1.3.9. If we also lose the log ﬁle, then we may be in trouble. For these situations we can hopefully rely on a backup copy of the database and/or the log ﬁle. The particular setup, how the database and/or the log ﬁle is archived and how often should be considered with care. What is the central idea of recovery? The central idea of recovery is always the same: get back to some consistent (or incon- sistent) old state of the database and then perform recovery to bring the database to a more recent but deﬁnitely consistent state. Quizzes 1. Which of the ACID properties can potentially be violated if there is a power failure on an operating DBMS? (a) A (b) C (c) I (d) D 2. How could a software keep backup copies? 266 Recovery (a) There is the master (editable) ﬁle, and a single copy of this (master) ﬁle is made and kept updated concurrently for each and every change as the master ﬁle changes. (b) There is the master (editable) ﬁle, and diﬀerent snapshots of this (master) ﬁle are created and kept as the master ﬁle changes. 6.1.2 Log-Based Recovery, Stable Storage, Write-Ahead Logging (WAL) Material Video: Original Slides: Inverted Slides: Additional Material Literature: see 6.1.1 and 6.2 Learning Goals and Content Summary What is ‘ stable storage’?stable storage Stable storage refers to a special (typically additional device) that is used to persist the log ﬁle. “stable” is a relative term. Typically, we will argue in the following that a transaction can be considered committed once all log records created by that transaction are persisted on stable storage. This means, our notion of whether a database is considered committed highly depends on the deﬁnition of “stable”. If we lose stable storage, we will lose transactions, previously considered committed. What is its relationship between stable storage and the database store?database store You may consider stable storage to be part of the store, but may also consider it to be an outside, special, storage functionality. We stick with the latter notion in the following. What is the relationship between stable storage and logging?logging It is a reﬁnement of logging where the log ﬁle is assumed to be kept on an extra device. What is write-ahead logging (WAL)?write-ahead logging WAL Write-ahead logging introduces two constraints on the order of persisting data in the store and the log. First, before persisting any change done to the store, all log records reﬂecting that state must be persisted, i.e. the log is written-ahead in that sense that the log (on disk) is in a newer version than the database state (on disk). Second, a transaction may only be considered committed after all log records created by that transaction are persisted on stable storage. What is and what is not allowed following WAL? It is not allowed that any page in the database state is persisted without ﬁrst persisting the corresponding log records. In other words: database pages must not be persisted 6.1 Core Concepts 267 Indexer Query Optimizer DBMS Store stable storage Eventually create a new Edition 1.1 stable storage flash/hard disk main memory page 23: “datbase“ → “database“ page 23: “datbase“ → “database“ Store flash/hard disk main memory Figure 6.2: The relationship of the ‘normal’ database store and stable storage in terms of architecture and durability of updates in newer versions than the corresponding log records. In addition, the database system is not allowed to tell an application program that a particular transaction committed successfully before all log records created by that transaction are persisted in the log — whatever the state of the database then is: it does not matter for this rule. What does WAL imply for the database buﬀer and page eviction of dirty pages? database buffer page eviction dirty page Before writing a dirty page to the database store (be it due to page eviction or a back- ground thread regularly writing back dirty pages), we have to persist all log records reﬂecting changes done to that page in the log. 268 Recovery Quizzes 1. What is the main diﬀerence between the store and the stable storage on a DBMS? (a) The stable storage works as a secondary store in case the main store runs out of space. (b) The stable storage is only considered for redundantly logging changes occurring in the DBMS, as to facilitate recovering from a possible failure. 2. What does write-ahead logging (WAL) in a disk-based database system mean? It means... (a) that every single change operation performed by a transaction is ﬁrst ﬂushed to the disk used by the log ﬁle, and then the change operation is ﬂushed to the persistent disk used by the database store. (b) that every single operation performed by a transaction is ﬁrst ﬂushed to the disk used by the log ﬁle, and then the operation is ﬂushed to the persistent disk used by the database store. (c) that every single change operation performed by a transaction is ﬁrst ﬂushed to the disk used by the log ﬁle, and only afterwards the change operation is applied on the database store (including the non-volatile DB buﬀer in main memory and hard disks). 3. Assume you have a DBMS with multiple storage layers, for the store as well as for the stable storage. Let these storage layers be main memory (RAM) and disk (HDD) in both, the store and the stable storage. Which of the following is NOT WAL? (a) First log the transactions on the two storage layers of stable storage (from RAM to HDD), and then perform the transaction on the DBMS and write out to the store (from RAM to HDD). (b) First log the transactions on the RAM of the stable storage, then perform the transaction on the DBMS and write out to the store (from RAM to HDD), and ﬁnally log the transactions on the HDD of the stable storage. (c) First log the transactions on the RAM of the stable storage, then perform the transaction on the DBMS (RAM), then log on the HD of the stable storage, and ﬁnally write out to the store (HDD) of the DBMS. Exercise Assume a disk-based DBMS not using WAL. The transaction execution example we look at is performing two interleaved transactions, transferring 50 Euro from account A to account B by transaction 1, while transaction 2 transfers 10 Euro from account A to account C. You can assume that dirty pages are only ﬂushed to disk if explicitly stated in the following action log: 6.1 Core Concepts 269 (1) T1:Read the balance of account A into variable bala1 (2) T1: bala1 := bala1 \u0000 50 (3) T2:Read the balance of account A into variable bala2 (4) T2: bala2 := bala2 \u0000 10 (5) T2:Set the balance of account A to bala2 (6) T1:Set the balance of account A to bala1 [Flush buﬀer pages to disk] (7) T1:Read the balance of account B into variable balb1 (8) T1: balb1 := balb1 + 50 (9) T1:Set the balance of account B to balb1 (10) T1 commit (11) T2:Read the balance of account C into variable balc2 (12) T2: balc2 := balc2 + 10 (13) T2:Set the balance of account C to balc2 [Flush buﬀer pages to disk] (14) T2 commit Assume one of our consistency constraints is that the sum of the amounts of money from all accounts should be equal at all times. Which of the ACID properties are violated in the above transaction execution ex- ample? Refer to the exact lines causing a violation in your justiﬁcation. If the system crashes directly after the execution of command (12), will the committed transaction be durable? 6.1.3 What to log, Physical, Logical, and Physiological Logging, Trade-Oﬀs, Main Memory versus Disk-based Systems Material Video: Original Slides: Inverted Slides: Additional Material Literature: see 6.1.1 and 6.2 270 Recovery Learning Goals and Content Summary Physical Logging states (byte images) are logged before image contains state before change was performed after image contains state after change was performed Kemera Page 42 Page 42 lens aperture depth of field camera lens aperture depth of field Page 42: image at 367,2; before:,‘Ke‘; after: ‘ca‘ Log Logical Logging high-level operations are logged not necessarily limited to a single page Kemera Page 42 Page 42 lens aperture depth of field camera lens aperture depth of field CameraLingo: update(0, ‘Kemera‘ => ‘camera‘) Log CameraLingoCameraLingo termID term 0 Kemera 1 lens 2 aperture 3 depth of field 4 aperture 5 body 6 shutter Figure 6.3: Physical vs logical logging What is physical logging?physical logging In physical logging a snapshot of a subset of the database state is logged. What is an after image and a before image?after image before image The before image logs the state as of before applying a change. The after image logs the state as of after applying a change. Obviously, this is only eﬃcient if the amount of data recorded in the before and after images is relatively small. How big may before and after images become? Well, in theory, if everything was changed by an operation, physical logging has to log 6.1 Core Concepts 271 Physiological Logging like logical logging, but: log entry may only aﬀect a single page Kemera Page 42 Page 42 lens aperture depth of field camera lens aperture depth of field Page 42: update(0, ‘Kemera‘ => ‘camera‘) Log Figure 6.4: Physiological logging the entire database. What is logical logging? logical logging Logical logging logs the transitions between database states, i.e. we log the operation that is required to get from an older database state to a newer database state and vice versa. As transitions we use diﬀerent granules. One extreme version is to simply log the SQL-command (in some byte-eﬃcient representation). Another extreme is to simply log transitions operating on atomic data values, e.g. a=a+10. How does logical logging diﬀer from physiological logging? physiological logging Logical logging is not restricted to a particular page. This means, the transition recorded in a logical log record may eﬀect multiple pages. This is not the case for physiological logging: even though these log records describe a transition, that transition may only eﬀect one particular page. How do log entries for logical, physical, and physiological logging typically look like? Physical log records contain an after and a before image and (typically) a reference to a storage location where these images should be applied (on the level of pages). Like physical log records, physiological log records keep a reference to a storage location (typically a page), yet they do not store images but transitions. Logical log records do not keep a reference to a single page and keep transitions. What are the performance trade-oﬀs for the diﬀerent logging variants in general? We can identify two extremes: logical logging at the level of transactions (or their IDs) and physical logging using before and after images. An important consideration when choosing a particular method is the processing time triggered by a particular log record during recovery. For instance, a logical log record just storing the ID of a complex query 272 Recovery log-size ≈ I/O-time to read log logical (logical operations) x physical x x physiological x logical (transactions) Main Performance Trade-Oﬀ (disk-based Systems) log entry processing time [minutes] log-size ≈ I/O-time to read log log entry processing time [ms] x physical x x physiological x logical (transactions) Main Performance Trade-Oﬀ (Main Memory Systems) best deal ms logical (logical operations) Figure 6.5: Logging trade-oﬀs in diﬀerent database systems: disks-based vs main-memory systems may aﬀect several pages, it may involve heavy join computations and so on and thus in total it may take a long time to be redone. In contrast, replaying a physical log record merely involves loading one particular page and copying the after image to that page. So there is a clear trade-oﬀ among the space required for the log records and the time it takes to process those log records. What are the performance trade-oﬀs for the diﬀerent logging variants in a disk-based system? In particular in a disk-based system the processing time for a particular log record may be huge. Hence, the I/O-time to read the log is not the bottleneck, but rather the time to apply the log records. It makes sense to store log records on a more physical level. If 6.1 Core Concepts 273 logical log records are used, it may make sense to consider these log records “transactions” and to translate them into physical log records (as done in the full-blown ARIES algorithm where logical undos are translated into physical or physiological log records). What are the performance trade-oﬀs for the diﬀerent logging variants in a main-memory system? In a main-memory system the processing time for a particular log record is on a completely diﬀerent scale than it is in a disk-based system: milliseconds rather than seconds/minutes. Therefore, the size of the log and the time it takes to read it, may severely limit the overall recovery time. Therefore, it makes sense to keep log entries as small as possible, e.g. by just storing queryIDs and their invocation parameters, similar to what is done in prepared statements and stored procedures. While those queries are running and considered uncommitted, the system must keep a main-memory resident undo-log, in case that query cannot be committed successfully. What is the relationship between logical logging and dictionary compression? dictionary compression Storing queryIDs rather than queries is an application of dictionary compression. Why do the trade-oﬀs diﬀer so much in the two types of systems? Again, due to the very diﬀerent timescales used to change data in a disk-based store versus a main-memory store. Quizzes 1. What is physical logging? (a) At a page level, explicit before and after changes are recorded. (b) Changes are kept at a high level, say at a table level, where the information of what entries are changed is kept. Changes here are not necessarily bounded to be kept at a page level. (c) We record high level information but restricted to a particular page. 2. What is logical logging? (a) At a page level, explicit before and after changes are recorded. (b) Changes are kept at a high level, say at a table level, where the information of what entries are changed is kept. Changes here are not necessarily bounded to be kept at a page level. (c) We record high level information but strongly restricted to a particular page. 3. What is physiological logging? (a) At a page level, explicit before and after changes are recorded. (b) Changes are kept at a high level, say at a table level, where the information of what entries are changed is kept. Changes here are not necessarily bounded to be kept at a page level. (c) We record high level information but strongly restricted to a particular page. 274 Recovery 4. Is there any disadvantage in logical logging? (a) No. (b) Yes, the size of the log increases signiﬁcantly if there are many changes. (c) Yes, changes touching multiple pages must be performed in an atomic manner (all or none) when recovering from a failure, they (possibly) could not be performed one by one. 5. What logging technique is in general more space-eﬃcient? (a) Physical logging. (b) Logical logging. (c) Physiological logging. 6.2 ARIES Material Video: Original Slides: Inverted Slides: Literature: [Fra97] (Section 3.2 only) Additional Material Literature: [MHL +92] Further Reading: [Moh99] [LÖ09], Multi-Level Recovery and the ARIES Algorithm Learning Goals and Content Summary What are the three phases of ARIES?ARIES 1. Analysis, 2. Redo, and 3. Undo What are the core tasks of the three phases of ARIES? The three phases can technically be summarized as follows: 1. Analysis Phase: construct metadata on dirty pages and non-committed transactions (DPT and TT) as of the time of the crash; compute where to start the Redo phase (the minDirtyPageLSN) 2. Redo Phase: repeat all actions including those from loser transactions and changes undone previously (the CLRs); restore database to where it was at the time of the crash (repeating history) 3. Undo Phase: undo actions of all loser transactions; log those undos to special log records (called CLRs) 6.2 ARIES 275 ✘ 3 Phases of ARIES log file Analysis Redo Undo first log record last log record earliest possibly dirty page earliest change applied by a loser transaction notice: for the moment no checkpointing! Example Log File 1: [-, 1, “update“, 42, a+=1, a-=1] 2: [-, 2, “update“, 42, b+=3, b-=3] 3: [2, 2, “update“, 46, c+=2, c-=2] 4: [1, 1, “update“, 42, b+=1, b-=1] 5: [3, 2, “commit“] LSN: [prevLSN, TaID, type]\t all LSN: [prevLSN, TaID, “update“, pageID, redo info, undo info]\r “update“ TTTT TaID lastLSN 1 4 2 3 DPTDPT pageID recoveryLSN 42 1 46 3 log page 42 a=78 b=59 LSN=4 page 46 c=24 LSN=3 DB buffer Figure 6.6: The three phases of ARIES and an example logging scenario showing the DB buﬀer, the log, the transaction table (TT) and the dirty page table (DPT) We will explain the diﬀerent concepts in the following. What is an LSN? LSN In order to keep track of log records during recovery, we need a way of uniquely identifying log records. This is done by introducing log sequence numbers(LSNs). Logsequence log sequence number numbers do not need to be stored explicitly in the log. The easiest implementation of LSNs is to assume that in the log ﬁle for each log record, its oﬀset corresponds to its LSN. What is the purpose of the transaction table (TT)? transaction table TTThe transaction table (TT) maintains the list of ongoing (uncommitted) transactions. This is useful during recovery to identify the loser transactions, i.e. the transactions that 276 Recovery Example Log File 1: [-, 1, “update“, 42, a+=1, a-=1] 2: [-, 2, “update“, 42, b+=3, b-=3] 3: [2, 2, “update“, 46, c+=2, c-=2] 4: [1, 1, “update“, 42, b+=1, b-=1] 5: [3, 2, “commit“] LSN: [prevLSN, TaID, type]\t all LSN: [prevLSN, TaID, “update“, pageID, redo info, undo info]\r “update“ LSN: [prevLSN, TaID, “compensation“, redoTheUndo info, undoNextLSN]\r “compensation“ TTTT TaID lastLSN 1 7 2 3 DPTDPT pageID recoveryLSN 42 1 46 3 6: [4, 1, “compensation“, 42, b-=1, 1] 7: [6, 1, “compensation“, 42, a-=1, - ] ✘ log ✘ page 42 a=77 b=58 LSN=7 page 46 c=24 LSN=3 DB buffer ✘ Starting Points log file Redo Undo first log record minDirtyPageLSN :=\t SELECT min(recoveryLSN) FROM DPT; note: relative starting points of the phases may differ last log record Analysis notice: for the moment no checkpointing! earliest possibly dirty page earliest change possibly applied by a loser transaction firstLSN minDirtyPageLSN Figure 6.7: The logging scenario under multiple crashes and a schematic view on the diﬀerent possible starting points for the three diﬀerent phases in ARIES did not commit and whose changes must be removed from the database store. In the TT, for each transaction we keep the last log record triggered by that transaction. This is called the lastLSN. This implies that if a transaction performs another change triggeringlastLSN a new log record, its entry in the TT must be updated. If a transaction commits, it may be removed from the TT. What is the purpose of the dirty page table (DPT)?dirty page table DPT The dirty page table (DPT) maintains the list of pages in the database buﬀer which have not yet been written back to the database store. This is useful during recovery to identify pages that were not persisted in the database store w.r.t. their latest version, i.e. some of the changes applied to those pages in normal operation were not persisted in the database 6.2 ARIES 277 store. On those “dirty pages” we may have to redo information during recovery later on. For each page in DPT we keep the recoveryLSN. That is the LSN that did the ﬁrst change to that page making it diﬀerent from the version on disk, i.e. this was the ﬁrst change applied to that page converting it from a clean page to a dirty page. This implies that if atransaction performsanother change triggeringa newlog record on apage thatalready exists in DPT, its recoveryLSN must not be updated. The only way to remove entries from DPT is to write dirty pages back to the store, e.g. using a background thread. In that case the page should be removed from DPT as it is then considered a clean page. Which assumptions do we make in ARIES-recovery? We assume that pages may contain changes from loser and winner transaction at the same time, i.e. two concurrent transactions may operate on the same page. What information is contained in an update log record? update log record The update log records in ARIES have a particular structure. The redo information is physical or physiological, however the undo-information may be logical. For simplicity, we constrain undo information to be physical or physiological as well. This implies that there is a ﬁeld pageID recording the page that is aﬀected by this update log record. If pageID achangeaﬀectsmultiplepagestheremustbean updaterecord foreach of thosepages. In addition, an update log record contains a ﬁeld TaID, that is the ID of the transaction TaID that performed this change. Moreover, there is a ﬁeld prevLSN,that is the LSN of prevLSN the previous action performed by transaction TaID. If this action is the ﬁrst action of transaction TaID, prevLSN is set to empty (or NULL). For details on full logical undo see the article on multi-level recovery [REF above]. What is the general idea of a compensation log record (CLR)? compensation log record CLRA compensation log record is written in the undo-phase of ARIES. A CLR reﬂects a change done to the database store while removing a database store modiﬁcation performed by a loser transaction earlier. In other words, the CLR, just like a normal update log record, records a database store modiﬁcation. In contrast to a normal update log record, that modiﬁcation was not triggered by a transaction, but by recovery itself. Other than that, a CLR is treated just like any other update log record: in case of another crash it will be considered in the redo-phase just like any other update log record; that CLR must be ignored in any further undo-phases. What is the redoTheUndo ﬁeld contained in a CLR? redoTheUndo The redoTheUndo ﬁelds records the undo information applied by recovery during the undo-phase, i.e. semantically this ﬁeld reﬂects undo information, yet it will be replayed in the redo phase. and not in the undo phase. Where does undoNextLSN point to? undoNextLSN This attribute points to the LSN of the next log record to be undone by this transaction. How do we shorten ( prune)a logﬁle? prune We can shorten a log ﬁle by introducing checkpoints. How exactly depends on relative order of the start points of the redo and undo phases. Be careful that this pruning may 278 Recovery change the value of LSNs if there are implemented as oﬀsets in the log ﬁle. How do we shorten the analysis phase? We shorten the analysis phase by regularly writing fuzzy checkpoint. Then we do not have to read the entire log during analysis, but rather start from a well-deﬁned position in the log, i.e. the checkpoint, and only read from that checkpoint until the end of the log. What is a fuzzy checkpoint?fuzzy checkpoint A fuzzy checkpoint stores the contents of TT and DPT in the log ﬁle. How exactly is a fuzzy checkpoint written to the log ﬁle? We ﬁrst write a begin-checkpoint record. It marks the version of this fuzzy checkpoint. Then we write an end-checkpoint. The end-checkpoint contains a copy of TT and DPT as of the time of the begin-checkpoint. Between the begin-checkpoint and the end-checkpoint records ongoing transactions may write other log records. How do we shorten the redo phase? We can shorten the redo phase by running a background thread regularly writing out dirty pages to disk. This thread is run in normal operation of the database, i.e. not only during or after recovery. That thread will write out dirty pages regularly. As the redo phase determines its starting point by computing the minimum of all recoveryLSNs in the DPT, this has a direct eﬀect: that minimum is pushed forward to a later point in time. In other words, the background thread avoids that the version of pages on disk and in the DB-buﬀer do not diﬀer too much (in the sense that only very recent log records may need to be replayed on those pages). How do we shorten the undo phase? One source for a long undo phase is long running loser transactions, i.e. a transaction started a long time ago but did not commit before the crash happened. The undo-phase has to go back until the earliest change performed by any loser transaction, i.e. the earliest LSN written by any loser transaction. Therefore, anything that allows us to push the starting point of the earliest loser transaction forward in time, i.e. to a more recent LSN, helps shortening the undo-phase. One reason why a transaction may be long running could be that a transaction got stalled by a user-interaction. For instance, consider that a transaction waits for a user to enter some input on a screen. Only after that the transaction will continue processing. These kinds of transactions may be considered uncommitted by the database system over a long period of time. One way to ﬁx this is to write transactions such that they are only executed after all user input has been collected (if that is possible depends on the application). Another possibility is to enforce atimeout fortransactions(again,howexactlydependson theapplication). How do we determine ﬁrstLSN and where do we cut-oﬀ the log?ﬁrstLSN It is determined implicitly while processing the log backwards during the undo-phase. A better, and more reliable way would be to extend TT to record an extra ﬁeld ﬁrstLSN for each transaction. Then, at all times, we are able to compute the minimum ﬁrstLSN of TT. 6.2 ARIES 279 Then, the minimum over the ﬁrstLSN column, mindirtyPageLSN,and begin-checkpoint mindirtyPageLSN of the last successful log record determine the cut-oﬀ point where to safely prune the log ﬁle. What does ‘ repeating history’ mean in this context? repeating history This means that during the redo phase all changes are replayed on the databases store —including the ones done bylosertransactions. Where do the diﬀerent phases start and where do they end? Figure 6.7 gives a principle overview on the starting points. Notice that the relative order of the starting points may be diﬀerent. 1. Analysis Phase: starts at the beginning of the log ﬁle or (if it exists) the youngest fuzzy checkpoint successfully written (in which case it starts at the begin record of that checkpoint); ends at the last log record 2. Redo Phase: starts at the earliest possibly dirty page as determined by minDirty- PageLSN; ends at the last log record 3. Undo Phase: starts at the youngest log record of any loser transaction; ends at oldest log record possibly applied by a loser transaction (ﬁrstLSN) Notice that the costs for the analysis phase may be shortened considerably by regularly writing fuzzy checkpoints. In addition, the costs of the redo phase may be shortened considerably by regularly running a background thread writing out dirty pages. Quizzes 1. Which of the following is true? (a) ARIES uses logical redo and physical undo (b) ARIES uses logical redo and physiological undo (c) ARIES uses physiological redo and logical undo (d) ARIES uses physical redo and physical undo 2. Consider the following scenario: transaction A performs certain updates on page P. Transaction B kicks in before A ﬁnishes and also performs updates on page P, but in such a way that P is broken into more pages. Then B commits. Then A aborts. What kind of undo would work in this scenario? (a) Both physical undo and logical undo would succeed. (b) Physical undo would fail and logical undo would succeed. 3. Which of the following is correct? (a) ARIES works in two phases (after the crash). First redo and then undo. (b) ARIES works in three phases (after the crash). First Analysis, then redo, and ﬁnally undo. 280 Recovery 4. What are the two most important data structures kept by ARIES? (a) Transaction Table and Checkpoint Table. (b) Transaction Table and Dirty Page Table. (c) Dirty Page Table and Checkpoint Table. 5. Which of the following is correct? (a) The analysis phase processes the log forward to ﬁnd the newest checkpoint before the crash. (b) The analysis phase actually, besides scanning the log, reconstructs an up-to- date version of the Transaction and Dirty Page Tables that the redo and undo phases will use. (c) The redo phase undoes actions based on redo information present in compen- sation log records. (d) The undo phase undoes actions based on redo information present in compen- sation log records. 6. In the redo phase, does the system log a redo operation? (a) No. (b) Yes, with an update record. (c) Yes, with a compensation log record. 7. In the undo phase, does the system log an undo operation? (a) No. (b) Yes, with an update record. (c) Yes, with a compensation log record. 8. Does ARIES redo actions of loser transactions? (a) No. (b) Yes. 9. How can you reduce the costs of the redo phase in ARIES? (a) By running a background thread that periodically writes back dirty pages to the disk of the database store. (b) By running a background thread that periodically writes back dirty pages to stable storage. (c) By writing out more fuzzy checkpoints (without ﬂushing dirty pages to the disk of the database store). 10. How can you reduce the costs of the analysis phase in ARIES? 6.2 ARIES 281 (a) By running a background thread that periodically writes back dirty pages to the disk of the database store. (b) By running a background thread that periodically writes back dirty pages to stable storage. (c) By writing out more fuzzy checkpoints (without ﬂushing dirty pages to the disk of the database store). 11. Assume ARIES crashes while being in one of the three recovery phases. Assume no change whatsoever has been done to the database store by ARIES yet. Now, ARIES starts a second time with the recovery process. Which of the three phases may potentially require less write I/O-operations (a) Analysis (b) redo (c) undo Exercise Assume we are using a disk-based DBMS using ARIES-style recovery. Our buﬀer manager can keep 3 data pages in the buﬀer at any given time and uses LRU as a buﬀer replacement strategy. With this information you can decide which (possibly dirty) pages get ﬂushed to disk. Checkpointing is performed after every 10 th log entry, storing the current TT and DPT. Consider the following transaction execution example, where the concrete actions are not speciﬁed, only the aﬀected page and the transaction performing that action are recorded. Assume empty buﬀer, TT and DPT in the beginning. (a) Create a log from the transaction execution example. Add placeholders for undo- and redo information. (b) Show the state of the buﬀer after each log entry and, where applicable, the page being ﬂushed. (c) Show the pageLSNs of the ﬂushed pages (as they are available on disk), the TT and the DPT after the checkpoint and after the last operation. Actions: (1) T1 updates P1 (2) T1 updates P1 (3) T1 updates P3 (4) T2 updates P1 (5) T2 updates P4 282 Recovery (6) T3 updates P2 (7) T1 updates P1 (8) T2 updates P4 (9) T3 updates P4 (10) T4 updates P5 (11) T1 updates P6 (12) T1 updates P7 (13) T2 updates P3 (14) T2 updates P5 (15) T3 updates P1 Exercise Consider the following log: (1) (–, T1,\"update\", P1,redo1,undo1) (2) (1, T1,\"update\", P2,redo2,undo2) (3) (–, T2,\"update\", P3,redo3,undo3) (4) (begin_checkpoint) (5) (3, T2,\"update\", P1,redo5,undo5) (6) (end_checkpoint): Table 6.1: TT TaID LastLSN T1 2 T2 3 (7) (5, T2,\"compensation\", P1,undo5,3) (8) (2, T1,\"update\", P3,redo8,undo8) (9) (8, T1,\"update\", P2,redo9,undo9) Table 6.2: DPT PageID RecLSN P1 1 6.2 ARIES 283 (10) (9, T1,\"commit\") (11) (–, T3,\"update\", P4,redo11,undo11) (12) (11, T3,\"update\", P2,redo12,undo12) The system has crashed after the last operation in the log. The pageLSNs of the pages on disk after the crash are: PageID pageLSN P1 – P2 9 P3 3 P4 – Perform recovery using the ARIES algorithm, describing what you do according to the following: 1. Where does each phase of the ARIES algorithm begin and end? Be precise about the LSNs. 2. Show the TT and the DPT after the analysis phase. 3. Which operations (i.e. log records) are redone in the Redo phase? Provide a justi- ﬁcation when skipping a log record. 4. Show the log after the Undo phase. Exercise Consider the log from the previous exercise up to and including LSN 13 which was written during a system recovery (LSN 13 is the last log record written in this scenario). Assume the system tried to recover and in that process ﬂushed all dirty pages after LSN 8. Then it crashed while recovering after writing LSN 13. Show all the possible pageLSNs available on each page immediately after the crash. Perform a second recovery pass using the ARIES algorithm (now assume that no dirty pages were written back after LSN 8). Describe what you do in the same way as in the previous exercise. 284 Recovery Bibliography [ADHS01] Anastassia Ailamaki, David J. DeWitt, Mark D. Hill, and Marios Skounakis. Weaving Relations for Cache Performance. In VLDB,pages 169–180, 2001. [AMDM07] D.J. Abadi, D.S. Myers, D.J. DeWitt, and S.R. Madden. Materialization Strategies in a Column-Oriented DBMS. In ICDE,pages 466–475, 2007. [APR +98] Lars Arge, Octavian Procopiuc, Sridhar Ramaswamy, Torsten Suel, and Jef- frey Scott Vitter. Scalable Sweeping-Based Spatial Join. In VLDB,pages 570–581, 1998. [ASDR14] Victor Alvarez, Felix Martin Schuhknecht, Jens Dittrich, and Stefan Richter. Main Memory Adaptive Indexing for Multi-core Systems. In DaMoN,June 23, 2014. [Bac73] Charles W. Bachman. The Programmer As Navigator. Commun. ACM, 16(11):653–658, November 1973. ACM version . [BBD +01] Jochen Van den Bercken, Björn Blohsfeld, Jens-Peter Dittrich, Jürgen Krämer, Tobias Schäfer, Martin Schneider, and Bernhard Seeger. XXL - A Library Approach to Supporting Eﬃcient Implementations of Advanced Database Queries. In VLDB,pages 39–48, 2001. [Cha98] Surajit Chaudhuri. An Overview of Query Optimization in Relational Sys- tems. In PODS,pages 34–43, 1998. [CLG +94] Peter M. Chen, Edward K. Lee, Garth A. Gibson, Randy H. Katz, and David A. Patterson. RAID: High-performance, Reliable Secondary Storage. ACM Comput. Surv.,26(2):145–185,June 1994. [Cod82] E. F. Codd. Relational Database: A Practical Foundation for Productivity. Commun. ACM,25(2):109–117,February1982. ACM version . [CSRL09] Thomas H. Cormen, Cliﬀord Stein, Ronald L. Rivest, and Charles E. Leiser- son. Introduction to Algorithms. MIT Press, 3rd edition, 2009. 286 BIBLIOGRAPHY [dBBD +01] Jochen Van den Bercken, Björn Blohsfeld, Jens-Peter Dittrich, Jürgen Krämer, Tobias Schäfer, Martin Schneider, and Bernhard Seeger. XXL - A Library Approach to Supporting Eﬃcient Implementations of Advanced Database Queries. In VLDB,2001. [dBS01] Jochen Van den Bercken and Bernhard Seeger. An Evaluation of Generic Bulk Loading Techniques. In VLDB,pages 461–470, 2001. [Fra97] Michael J. Franklin. Concurrency Control and Recovery. In The Computer Science and Engineering Handbook,pages 1058–1077. 1997. [GHJV95] Erich Gamma, Richard Helm, Ralph Johnson, and John Vlissides. Design Patterns: Elements of Reusable Object-oriented Software.Addison-Wesley Longman Publishing Co., Inc., Boston, MA, USA, 1995. [GIM99] Aristides Gionis, Piotr Indyk, and Rajeev Motwani. Similarity Search in High Dimensions via Hashing. In VLDB,pages 518–529, 1999. [Gra06] Goetz Graefe. Implementing Sorting in Database Systems. ACM Comput. Surv.,38(3),September2006. [HRSD07] Allison L. Holloway, Vijayshankar Raman, Garret Swart, and David J. De- Witt. How to barter bits for chronons: compression and bandwidth trade oﬀs for database scans. In SIGMOD,pages 389–400, 2007. [IKM09] Stratos Idreos, Martin L. Kersten, and Stefan Manegold. Self-organizing Tuple Reconstruction in Column-stores. In SIGMOD,pages 297–308, 2009. [Jai91] Raj Jain. The Art of Computer Systems Performance Analysis: techniques for experimental design, measurement, simulation, and modeling.Wiley, 1991. [JFJT11] Chao Jin, Dan Feng, Hong Jiang, and Lei Tian. A Comprehensive Study on RAID-6 Codes: Horizontal vs. Vertical. In Networking, Architecture and Storage (NAS), 2011 6th IEEE International Conference on,pages 102–111, July 2011. [Knu73] Donald E. Knuth. The Art of Computer Programming, Volume III: Sorting and Searching.Addison-Wesley, 1973. [KSL13] Tim Kiefer, Benjamin Schlegel, and Wolfgang Lehner. Experimental Eval- uation of NUMA Eﬀects on Database Management Systems. In Daten- banksysteme für Business, Technologie und Web (BTW), 15. Fachtagung des GI-Fachbereichs \"Datenbanken und Informationssysteme\" (DBIS), 11.- 15.3.2013 in Magdeburg, Germany. Proceedings,pages 185–204, 2013. [LÖ09] Ling Liu and M. Tamer Özsu, editors. Encyclopedia of Database Systems. Springer US, 2009. BIBLIOGRAPHY 287 [MBNK04] Stefan Manegold, Peter Boncz, Niels Nes, and Martin Kersten. Cache- conscious Radix-decluster Projections. In VLDB,pages 684–695, 2004. [MHL +92] C. Mohan, Don Haderle, Bruce Lindsay, Hamid Pirahesh, and Peter Schwarz. ARIES: A Transaction Recovery Method Supporting Fine-granularity Lock- ing and Partial Rollbacks Using Write-ahead Logging. ACM Trans. Database Syst.,17(1):94–162,March 1992. [Moh99] C. Mohan. Repeating History Beyond ARIES. In VLDB,pages 1–17, 1999. [Neu11] Thomas Neumann. Eﬃciently Compiling Eﬃcient Query Plans for Modern Hardware. Proc. VLDB Endow.,4(9):539–550,June 2011. [OL90] Kiyoshi Ono and Guy M. Lohman. Measuring the Complexity of Join Enu- meration in Query Optimization. In VLDB,pages 314–325, 1990. [PH12] David A. Patterson and John L. Hennessy. Computer Organization and De- sign - The Hardware / Software Interface (Revised 4th Edition). The Mor- gan Kaufmann Series in Computer Architecture and Design. Academic Press, 2012. [RAD15] Stefan Richter, Victor Alvarez, and Jens Dittrich. A Seven-Dimensional Analysis of Hashing Methods and its Implications on Query Processing. In accepted at PVLDB 9, September 2015,2015. [RDS02] Ravishankar Ramamurthy, David J. DeWitt, and Qi Su. A Case for Fractured Mirrors. In Proceedings of the 28th International Conference on Very Large Data Bases, VLDB ’02, pages 430–441. VLDB Endowment, 2002. [RG03] Raghu Ramakrishnan and Johannes Gehrke. Database Management Systems. McGraw-Hill, Inc., New York, NY, USA, 3rd edition, 2003. [SG07] Bianca Schroeder and Garth A. Gibson. Disk Failures in the Real World: What Does an MTTF of 1,000,000 Hours Mean to You? In Proceedings of the 5th USENIX Conference on File and Storage Technologies,FAST ’07, 2007. [SS16] Jens Dittrich Stefan Schuh, Xiao Chen. An Experimental Comparison of Thirteen Relational Equi-Joins in Main Memory. In SIGMOD, to appear, 2016. [Vig13] Stratis D. Viglas. Just-in-time Compilation for SQL Query Processing. Proc. VLDB Endow.,6(11),August2013. [WKHM00] Till Westmann, Donald Kossmann, Sven Helmer, and Guido Moerkotte. The Implementation and Performance of Compressed Databases. SIGMOD Rec., 29(3):55–67, 2000. [WLO +85] Harry K. T. Wong, Hsiu-Fen Liu, Frank Olken, Doron Rotem, and Linda Wong. Bit Transposed Files. In VLDB,pages 448–457, 1985. 288 BIBLIOGRAPHY [WOS06] Kesheng Wu, Ekow J. Otoo, and Arie Shoshani. Optimizing Bitmap Indices with Eﬃcient Compression. TODS,31(1):1–38,2006. Credits Figure 1: ©iStock.com: TadejZupancic Figure 2: ©iStock.com: hidesy, moenez; Rastan; hatman12; mtphoto CC: Appaloosa http://commons.wikimedia.org/wiki/File:DRAM_DDR2_512.jpg http://creativecommons.org/licenses/by-sa/3.0/deed.en Lasse Fuss http://commons.wikimedia.org/wiki/File:Lufthansa_A380_D-AIMA-1.jpg http://creativecommons.org/licenses/by-sa/3.0/deed.en Jahoe http://commons.wikimedia.org/wiki/File:Schiphol_Amsterdam_airport_control_tower.png?uselang=de http://creativecommons.org/licenses/by-sa/3.0/deed.de also used in other ﬁgures Figure 1.2: ©iStock.com: voyager624 also used in other ﬁgures CC: BY-SA Thomas Tunsch / Hula0081110.jpg (Wikimedia Commons) http://de.wikipedia.org/w/index.php?title=Datei:Hula0081110.jpg&ﬁletimestamp=20070305150205 http://creativecommons.org/licenses/by-sa/3.0/deed.de as well as public domain Twix analogy inspired from: http://duartes.org/gustavo/blog/post/what-your-computer-does-while-you-wait? [retrieved Nov 8, 2013] yet: I extended the analogy a bit Cache latency numbers are based on this article: Performance Analysis Guide for Intel’s CoreTM i7 Processor and Intel’s XeonTM 5500 processors? By Dr David Levinthal PhD. Version 1.0 http://software.intel.com/sites/products/collateral/hpc/vtune/performance_analysis_guide.pdf?[retrieved Nov 8, 2013]? Figure 1.6: ©iStock.com: mtphoto also used in other ﬁgures Figure 1.14: ©iStock.com: nico_blue; ludinko 290 BIBLIOGRAPHY CC: Asim18 http://commons.wikimedia.org/wiki/File:SanDisk_SD_Card_8GB.jpg?uselang=de http://creativecommons.org/licenses/by-sa/3.0/deed.de and public domain Figure 2.1: ©iStock.com: mtphoto CC: Appaloosa http://commons.wikimedia.org/wiki/File:DRAM_DDR2_512.jpg http://creativecommons.org/licenses/by-sa/3.0/deed.en Intel Free Press http://www.ﬂickr.com/photos/54450095@N05/6345916908 http://creativecommons.org/licenses/by/2.0/deed.de Austinmurphy at en.wikipedia http://en.wikipedia.org/wiki/File:LTO2-cart-purple.jpg http://creativecommons.org/licenses/by-sa/3.0/deed.en? Figure 2.11: CC: Wolfgang Beyer http://en.wikipedia.org/wiki/File:Mandel_zoom_08_satellite_antenna.jpg http://creativecommons.org/licenses/by-sa/3.0/ and public domain Figure 2.12: ©iStock.com: mtphoto Figure 3.1: CC: Martin Abegglen https://www.ﬂickr.com/photos/twicepix/5115400010/ https://creativecommons.org/licenses/by-sa/2.0/legalcode CC: Summi at the German language Wikipedia: http://commons.wikimedia.org/wiki/File:Wegweiser_Foggenhorn.jpg http://creativecommons.org/licenses/by-sa/3.0/legalcode Figure 3.12: ©iStock.com: Freerick_k Figure 3.13: CC: Ricardo Liberato http://de.wikipedia.org/wiki/Datei:All_Gizah_Pyramids.jpg http://creativecommons.org/licenses/by-sa/2.0/legalcode other: http://openclipart.org/image/800px/svg_to_png/26274/Anonymous_Right_Footprint.png http://openclipart.org/detail/26217/left-footprint-by-anonymous http://openclipart.org/detail/22012/weather-symbols:-sun-by-nicubunu Figure 3.14: BIBLIOGRAPHY 291 iconshock http://commons.wikimedia.org/wiki/File:Desk_lamp_icon.png http://creativecommons.org/licenses/by-sa/3.0/legalcode User Smial on de.wikipedia http://commons.wikimedia.org/wiki/File:Luefter_y.s.tech_pd1270153b-2f.jpg http://creativecommons.org/licenses/by-sa/2.0/de/legalcode Figure 3.15: public domain: http://commons.wikimedia.org/wiki/File:The_Blue_Marble.jpg 292 BIBLIOGRAPHY Index 7-Bit, 167 7Bit Encoding, 126 access path, 221 access time, 21, 30 accessibility, 119 ACID, 260 address virtualization, 82, 93 after image, 266 aggregation, 195 aggregation function algebraic, 195 distributive, 195 algebraic representation, 225 anti-projection, 254 ARIES, 270 associative addressing, 19 asymptotic complexity, 154 B-trees, 133 bandwidth, 22 batch pattern, 41 before image, 266 Bell number, 109 bitlist, 164 bitmap, 164 decomposed, 165 range-encoded, 169 uncompressed, 164 word-aligned hybrid, 167 block, 52, 113 blocking, 243 bloom ﬁlter, 171 branching factor, 135 BST, 133 bucket, 161 build phase, 193 bulkload, 142 bushy join, 226 bushy tree, 225 C++, 250 cache, 24 cache line, 81 cache miss, 85 cache-eﬃciency, 206 canonical form, 216 catalog, 99 chunk, 245 circular sector, 31 CLR, 273 cluster, 159 code generation, 215 CoGroup, 184 cogroup, 186 CoGrouping, 184 cogrouping, 195 collision, 161, 173 column, 20, 246, 252 column grouping, 108 column layout, 101, 113 column store, 102 compensation log record, 273 compiling code, 249 compress, 167 compression, 119 compression ratio, 119 computation, 14 computing core, 21 Copy On Write Pattern, 68 core, 27 correlated columns, 103 cost estimation, 225 cost model, 154 294 INDEX costs, 22, 59 COW, 68, 69 CPU, 27 CREATE DOMAIN, 122 cross product, 187 cylinder, 31 DAG, 216 data model, 19 data access, 14 data layout, 89, 219 data layouts, 81 data redundancy, 108 data replacement strategy, 25 database buﬀer, 56, 59, 260, 263 store, 262 decluster, 159 decompress, 168 delete, 140 device, 89 devirtualize, 89 dictionary, 120 dictionary compression, 120, 269 diﬀerential ﬁles, 72, 76 direct write, 61 dirty page, 263 dirty page table, 272 disk arm, 31 disk page, 81 disk sector, 81 diskhead, 31 domain encoding, 122, 127 DPHJ, 190 DPIJ, 192 DPT, 272 DRAM, 29 duplicates, 150, 182 dynamic programming, 230, 233, 236 early grouping and aggregation, 210 early materialization, 252 elevator optimization, 39 end-users, 20 error scenario, 260 evict, 59 experiment, 157 explicit key, 104 external merge sort, 197 F, 135, 200 false positive, 172 fan-in, 200 fan-out, 135, 199 ﬁll word, 168 ﬁrstLSN, 274 ﬁxed-size components, 95 ﬂash, 51 forward tuple-ID, 93 fractal design, 115 fractured mirrors, 105 free space, 98 function library, 240 fuzzy checkpoint, 274 get(Px), 59 grouping, 195 hash-based, 195 sort-based, 196 h, 135 hard disk, 31, 37, 52, 55 hard disk cache, 39 hard disk failures, 43 hash function, 159, 173 hashing, 159 chained, 160 hashmap, 195 hasNext(), 244 HD sector, 31 height, 135 history database, 17 horizontal partitioning, 113 implicit key, 103 inclusion, 25 index, 37 INDEX 295 clustered, 145 coarse-granular, 146 composite, 150 covering, 149 dense, 146 ﬁlter, 129, 147, 172 probabilistic, 171 sequential access method, 136 sparse, 146 unclustered, 146 index node, 135 index only plan, 149 indexer, 14 indexing, 129 indirect write, 61 INL, 179 INLJ, 193 inner node, 135 insert, 138 interesting order, 225 interesting physical property, 226 intermediate relation, 226 interval partitioning, 137 ISAM, 136 iteration, 233 iterator, 244 join, 186 double-pipelined hash, 190 double-pipelined index, 192 enumeration, 233 graph, 230 index, 255 index nested-loop, 179 nested-loop, 178 order, 220 predicate, 179 simple hash, 181 sort-merge, 182 join algorithm, 177 join index bitmap, 164 k, 135 k*, 136 L1, 27 lastLSN, 272 late grouping and aggregation, 210 layers, 13 lazy evaluation, 244 leaf, 135 left-deep tree, 225 lexicographical sorting, 125 linear addressing, 95 linear trees, 225 linearization, 100 linearize, 89 literal word, 168 LLVM, 250 local error, 261 locality-sensitive hashing, 159 log sequence number, 271 logging, 76, 262 logical block addressing, 34 logical cylinder/head/sector-addressing, 33 logical logging, 267 logical operator, 224 logical plan, 225 loops, 248 LSH, 159 LSN, 271 mapping steps, 89 materialize, 89 memory, 21 merge, 141 Merge on Write Pattern, 69 merge phase, 200 mergePlan(), 234 metadata, 98 mindirtyPageLSN, 275 MOW, 69 multicore architecture, 28 multicore storage hierarchy, 27 NL, 178 node, 135 296 INDEX non-blocking, 243 non-redundant, 100 Non-Uniform Memory Access, 28 NUMA, 28 O-Notation, 153 oﬀset, 84 online grouping and aggregation, 210 operating systems block, 33 operator, 248 interface, 244 optimal data layout, 114 optimal subplan, 232 overﬁtting, 157 page, 59, 246 page eviction, 263 page table, 83 pageID, 273 partitionings, 108 pass, 200 pattern, 26, 73 PAX, 112 PCI ﬂash drive, 55 performance measurement, 152 physical cylinder/head/sector-addressing, 33 physical data independence, 16, 93 physical logging, 266 physical operator, 224 physiological logging, 267 pipeline, 244 pipeline breaker, 242 platter, 31 point query, 137, 170 post-ﬁlter, 129 power failure, 260 preﬁx addressing, 83 prevLSN, 273 probe phase, 193 programming language compilation, 215 projection, 248 prune, 273 pruning, 233 pulling up data, 56 push down, 218 pushing down data, 56 query optimizer, 14, 213, 225 query parser, 213 query plan, 215, 216 query plan interpretation, 215 query planning, 254 quicksort, 206 R-Tree, 135 RAID, 43, 52 nested, 47 random access, 36 random access time, 53, 55 range query, 137 read-only DB, 74 recovery, 260 redo, 261 redoTheUndo, 273 redundancy, 50 redundant, 105 rehash, 162 relation, 89 relational algebra, 20 model, 20 relational model, 17 repeating history, 275 replacement selection, 203, 204 ResultSet, 246 RLE, 124, 167 robustness, 213 root, 142 row, 20, 246 row layout, 100 row store, 102 rowID, 93 rule, 217 rule-based optimization, 217 run, 200 run generation, 200 run length encoding, 124 INDEX 297 SAS, 55 schema, 20 search space, 225 segment, 98 selection, 248 selectivity, 130 estimate, 223 high, 130 low, 131 self-correcting block, 32 self-similar design, 115 sequential access, 32, 34, 36 sequential bandwidth, 52 serialize, 89 set of bushy trees, 225 set of left-deep trees, 225 shadow storage, 65 SHJ, 181 simulation, 155 slot, 93 slot array, 93, 95 slotted page, 92, 95 SMJ, 182 sorting, 125 sparing, 34 spatial locality, 56, 81 split, 138 SSD, 52, 55 stable storage, 262 state, 248 statistics, 214 storage capacity, 21 hierarchy, 21, 26, 39 layer, 23, 52 level, 23 space, 118, 127 store, 14 superblock, 52 system aspects, 14 System-R, 225 TaID, 273 tape, 30 tape jukebox, 30 temporal locality, 56 TLB, 85 track, 31 track skewing, 36 transaction table, 271 translation lookaside buﬀer, 85 TT, 271 tuple reconstruction join, 104, 254 tuple-wise insert, 142 twin block, 63 uncorrelated, 104 undo, 261 undoNextLSN, 273 update log record, 273 value bitmap, 164 variable-sized components, 95 vertical partitioning, 108 vertical topic, 11, 14 virtual memory, 82, 85 virtual memory page, 81 volatile memory, 52 WAH, 167 WAL, 262 wasting bits, 80 write ampliﬁcation, 52 write-ahead logging, 262 zone bit recording, 32 298 INDEX CV Jens Dittrich Jens Dittrich is a Full Professor of Computer Science in the area of Databases, Data Management, and Big Data at Saarland University, Germany. Previous aﬃliations include U Marburg, SAP AG, and ETH Zurich. He is also associated to CISPA (Center for IT- Security, Privacy and Accountability). He received an Outrageous Ideas and Vision Paper Award at CIDR 2011; a BMBF VIP Grant in 2011; a best paper award at VLDB 2014 (the second ever given to an Experiments&Analysis paper); two CS teaching awards in 2011 and 2013; several presentation awards including a qualiﬁcation for the interdisciplinary German science slam ﬁnals in 2012; and three presentation awards at CIDR (2011, 2013, and 2015). He has been a PC member of prestigious international database conferences such as PVLDB/VLDB, SIGMOD, and ICDE. In addition, he has been an area chair at PVLDB, a group leader at SIGMOD, and an associate editor at VLDBJ. His research focuses on fast access to big data including in particular: data analytics on large datasets, main-memory databases, database indexing, and reproducibility (see https://github.com/uds-datalab/PDBF). Since 2013 he has been teaching some of his classes on data management as ﬂipped classrooms (aka inverted classrooms). See: http://datenbankenlernen.de , http://youtube.com/jensdit . for a list of freely available videos on database technology in German and English (about 80 videos in German and 80 in English so far).","libVersion":"0.3.2","langs":""}