{"path":"DHBW Heidenheim/2024 SoSe/Theoretische Informatik/UnterrichtsMaterial/5. Woche Dynam. Progr. Rekursion Quicksort Theoretische Informatik II V4.pdf","text":"Studiengang Informatik Dynamic programming 1S. Berninger DHBW Heidenheim Rekursion ist oft die Ursache für manche der langsamsten Kategorien von Big-O, wie z.B. O(2N). -> Geschwindigkeitsfallen in rekursivem Code identifizieren -> Techniken finden, um sie zu fixen Studiengang Informatik Geschwindigkeitsfalle: Unnötige rekursive Aufrufe 2S. Berninger DHBW Heidenheim Diese rekursive Funktion findet die größte Zahl in einem Array (Teilproblem, Verarbeitung beginnt von rechts): // Das aktuelle Element mit dem größten des Rests des Arrays vergleichen. // Ist dieses Element größer, geben wir es als größte Zahl zurück. Studiengang Informatik 3S. Berninger DHBW Heidenheim Geschwindigkeitsfalle: Unnötige rekursive Aufrufe Wir erreichen den Vergleich mit einem bedingten Befehl. Die Prüfung der Bedingung enthält diesen Test: Die zweite Hälfte des bedingten Befehls ist: Dieser Code funktioniert, enthält aber eine verborgene Ineffizienz: Er enthält die Phrase max (array+1, size - 1) u.U. zweimal: bei jedem Test, und in jedem else-Zweig. Jedesmal, wenn wir max (array+1, size - 1) aufrufen, triggern wir eine ganze Lawine rekursiver Aufrufe... // Das aktu elle E lemen t mit d em größten d es Rests d es Arrays vergleich en . // Ist d ieses E lemen t größer, geb en wir es als größte Zah l zu rü ck. Studiengang Informatik 4S. Berninger DHBW Heidenheim Beispielarray: [1, 2, 3, 4] (Zahl ist immer kleiner als die größte Zahl des Rests) Start: Vergleich der 1 mit der größten Zahl des restlichen Array [2, 3, 4]. Vergleich der 2 mit dem Rest [3, 4]. Vergleich der 3 mit der [4]. Das triggert einen weiteren rekursiven Aufruf der [4] selbst, die Abbruchkriterium ist. Geschwindigkeitsfalle: Unnötige rekursive Aufrufe Der Weg hinunter in den Call stack: Test else Studiengang Informatik 5S. Berninger DHBW Heidenheim Wenn max() im Abbruchkriterium für [4] aufgerufen wird, ist es ziemlich einfach—ein einzelner Funktionsaufruf.. Wenn wir max() für das Array [3, 4] aufrufen, vergleichen wir in der 1. Hälfte die 3 mit dem Rest von [4]: Da die 3 nicht größer als 4 ist, trigger wir die 2. Hälfte des Befehls, die max([4]) zurückgibt — aber max([4]) triggert den rekursiven Aufruf nochmal: Geschwindigkeitsfalle: Unnötige rekursive Aufrufe -> Wir rufen max([4]) zweimal auf. Wir haben das Ergebnis von max([4]) schon einmal berechnet, warum sollten wir es für das gleiche Ergebnis nochmals aufrufen?? Test else Studiengang Informatik 6S. Berninger DHBW Heidenheim Wenn wir den Callstack weiter nach oben gehen, verschlimmert sich das Problem noch: Geschwindigkeitsfalle: Unnötige rekursive Aufrufe Komplexität: O(2n-1) für n=4: 15 Aufrufe, für n=10: 1023 Aufrufe! Studiengang Informatik Der kleine Fix für Big-O 7S. Berninger DHBW Heidenheim Wir sollten max() in jedem Durchlauf nur 1x aufrufen, und das Ergebnis in einer Variable speichern: Wir rufen max1() in Summe nur noch 4x auf! Wichtiger Unterschied: In jedem Durchlauf wird die Funktion nur noch einmal aufgerufen! Studiengang Informatik Der kleine Fix für Big-O 8S. Berninger DHBW Heidenheim Die verbesserte Variante max1() ruft sich nur noch so oft auf, wie das Array Elemente hat. Sie hat dadurch eine Effizienz von O(N). -> Sehr kleine Codeänderung führt zur Geschwindigkeitserhöhung von O(2N) zu O(N). -> Zusätzliche, unnötige rekursive Calls müssen unbedingt vermieden warden! Studiengang Informatik Diese Funktion ruft sich selbst jedoch immer zweimal auf!!!! Rekursionskategorie: Überlappende Teilprobleme 9S. Berninger DHBW Heidenheim Eine Fibonacci–Folge ist eine unendliche mathematische Zahlensequenz: jede Zahl ist die Summe der vorangegangenen beiden Zahlen der Sequenz: fib(n): fib(n-1) + fib(n-2). 0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55… Die Funktion fib() gibt die n.te Zahl eine Fibonacci-Folge zurück. Fibonacci (10): 55 (die 10. Zahl in der Folge, die 0 gilt als 0-te Zahl der Folge). n: 0, 1, 2, 3, 4, 5, 6, ... Studiengang Informatik Überlappende Teilprobleme 10S. Berninger DHBW Heidenheim Wir geben die “6” in die Funktion hinein: O ( 2n) Studiengang Informatik Überlappende Teilprobleme 11S. Berninger DHBW Heidenheim Wir müssen sowohl fib(n - 2) und fib(n - 1) aufrufen! Die obigen Teilprobleme überlappen sich, da fib(n-2) und fib(n-1) alle Funktionen inklusive und unterhalb von fib(n-2) doppelt aufrufen. Lösung: Dynamische Programmierung Dynamische Programmierung nennen wir die Optimierung rekursiver Probleme, die überlappende Teilprobleme haben. Studiengang Informatik Dynamische Programmierung durch Memoisation 12S. Berninger DHBW Heidenheim a) Dynamische Programmierung durch Memoisation (top-down) Memoisation (Abspeicherung) ist eine Technik, um Computerprogramme zu beschleunigen, indem Rückgabewerte von Funktionen zwischengespeichert anstatt mehrfach neu berechnet werden Fibonacci-Beispiel: erster Aufruf von fib(3): führt Berechnung durch und gibt die Zahl “2” zurück. Sie könnten vor der Weiterarbeit dieses Ergebnis in eine Hashtabelle speichern. Hashtabelle: {3: 2} – das Ergebnis von fib(3) ist die Zahl 2. Ergebnisse aller neuen, erstmaligen Berechnungen werden abgespeichert. Hashtabelle nach der Durchführung von fib(4), fib(5) und fib(6): { 3: 2, 4: 3, 5: 5, 6: 8 } Studiengang Informatik Dynamische Programmierung durch Memoisation 13S. Berninger DHBW Heidenheim fib(4) ruft nicht mehr fib(3) und fib(2) auf, sondern schlägt zunächst den Key 4 in der Hashtabelle nach • enthalten?: Wert wird sofort zurückgegeben (bereits berechnet) • noch nicht in der Hashtabelle? Berechnung von fib(4) starten! Überlappende Teilprobleme führen die gleichen rekursiven Berechnungen wieder und wieder aus. Durch Memoisation machen wir keine Berechnung mehr, die vorher schon durchgeführt wurde. Wie bekommt eine rekursive Funktion Zugriff auf diese Hashtabelle? -> Deren Adresse wird als zusätzlicher Parameter in die Funktion übergeben. Studiengang Informatik Dynamische Programmierung durch Memoisation 14S. Berninger DHBW Heidenheim Beim Funktionsaufruf übergeben wir jetzt die Zahl und eine “leere” Hashtabelle: Studiengang Informatik Die Übersicht der rekursiven Aufrufe sieht für unsere Version mit Memoisation jetzt so aus: Dynamische Programmierung durch Memoisation 15S. Berninger DHBW Heidenheim Jeder eingerahmte Aufruf findet das Ergebnis in der Hashtabelle! Studiengang Informatik Dynamische Programmierung ohne Memoisation 16S. Berninger DHBW Heidenheim Zum Vergleich: Studiengang Informatik Dynamische Programmierung durch Memoisation 17S. Berninger DHBW Heidenheim Was ist jetzt das Big O unserer Funktion? N Recursive calls 1 1 2 3 3 5 4 7 5 9 6 11 Wir machen für N nur (2N) - 1 Aufrufe: ein O(N)-Algorithmus! Enorme Beschleunigung gegenüber O(2N)! Studiengang Informatik Dynamische Programmierung durch Bottom-up 18S. Berninger DHBW Heidenheim Rekursiver Ansatz mit Memoisation: • Berechnung von fib(n) als Summe der beiden vorherigen Fibonacci-Zahlen. • startete mit der höchsten Zahl, und berechnete diese auf Basis der niedrigeren Zahlen (top-down). b) Dynamische Programmierung durch Bottom-up (Iteration) Statt dem rekursiven Ansatz: • bottom-up-Ansatz verwendbar -> Rekursion völlig vermeidbar • überlappende Teilprobleme werden damit überwunden Studiengang Informatik Dynamische Programmierung durch Bottom-up 19S. Berninger DHBW Heidenheim Bottom-up-Ansatz mit einer normalen Schleife: Studiengang Informatik Memoisation (Top-down) vs. Bottom-up 20S. Berninger DHBW Heidenheim Ist ein Verfahren besser als das andere? Hängt vom Problem ab und davon, warum Sie Rekursion benutzen möchten: Bietet die Rekursion eine elegante und verständliche Lösung für das Problem? -> Memoisation nutzen, um die überlappenden Teilprobleme zu eliminieren -> Allerdings: Memoisation erfordert die Nutzung einer Speicherplatz konsumierenden Hashtabelle. Bottom-up-Ansatz existiert, der funktioniert und für Andere verständlich ist? -> diesen verwenden Studiengang Informatik Zusammenfassung 21S. Berninger DHBW Heidenheim Jetzt können Sie effizienten rekursiven Code schreiben. Studiengang Informatik Übung 22S. Berninger DHBW Heidenheim Die folgende Funktion berechnet rekursiv the n-te Zahl einer mathematischen Sequenz, die “Golomb-Folge”1 genannt wird. Sie ist furchtbar ineffizient! Benutzen Sie Memoisation, um sie zu optimieren: 1 Die Golomb-Folge ist eine sich selbst erzeugende Folge ganzer Zahlen, bei der die an n-ter Stelle aufsteigend stehende natürliche Zahl an angibt, wie oft n in der aufsteigend erzeugten Folge vorkommt. Index n: 1 2 3 4 5 6 7 8 9 … Zahl: 1 2 2 3 3 4 4 4 5 5 5 6 6 6 6 7 7 7 7 8 8 8 8 9 9 9 9 9 Studiengang Informatik Lösung 23S. Berninger DHBW Heidenheim Die folgende Funktion berechnet rekursiv the n-te Zahl einer mathematischen Sequenz, die “Golomb-Folge”1 genannt wird. Sie ist furchtbar ineffizient! Benutzen Sie Memoisation, um sie zu optimieren: Studiengang Informatik Übung: 15 min. 24S. Berninger DHBW Heidenheim Die folgende Funktion erhält ein Array von Zahlen und gibt deren Summe zurück, solange keine der Zahlen die Summe auf über 100 erhöht. Sobald eine der Zahlen die Summe auf über 100 erhöhen würde, wird sie ignoriert. Die Funktion macht jedoch unnötige rekursive Aufrufe. Fixen sie den Code, um unnötige Rekursionen zu vermeiden: Studiengang Informatik Lösung 25S. Berninger DHBW Heidenheim Wir haben hier 2 rekursive Aufrufe der Funktion selbst: Wir können diese auf einen Aufruf reduzieren: Studiengang Informatik Im realen Leben werden diese (für die Lehre gut geeigneten) Algorithmen nicht wirklich für die Arraysortierung verwendet. Die meisten Programmiersprachen haben on-board Sortierfunktionen für Arrays (Eigenimplementierung unnötig) Der in vielen dieser Sprachen unter der Haube laufende Algorithmus: Quicksort Rekursive Algorithmen für mehr Geschwindigkeit 26S. Berninger DHBW Heidenheim Wir wollen im Folgenden verstehen, warum Rekursion auch der Schlüssel zu Algorithmen sein kann, die viel, viel schneller laufen: Bereits behandelte Sortieralgorithmen: Bubble Sort, Selection Sort, und Insertion Sort. Idee: Nimm die (erste )Karte aus dem Stapel. Durchlaufe die restlichen Karten und teile sie auf in alle mit einem Wert kleiner oder gleich dem der ersten Karte (Stapel 1) und mit Wert größer als dem der ersten Karte (Stapel 2). Gib die beiden so entstandenen Teilstapel, wenn sie überhaupt Karten enthalten, an je einen Helfer mit der Bitte, auch nach dem hier beschriebenen Verfahren vorzugehen. Warte, bis Dir beide sortierte Teile zurückgegeben wurden, dann lege zuunterst den sortierten Stapel 1, darauf die anfangs gezogene Karte, darauf den sortierten Stapel 2 und gib das Ganze als sortiert zurück. QuickSort: Sortieren durch Zusammensetzen http://panthema.net/2013/sound-of-sorting 27S. Berninger DHBW Heidenheim QuickSort: Sortieren durch Zusammensetzen 28S. Berninger DHBW Heidenheim „J“ „O“ „P“ - „T“ „U“ – „Z“ „A“ - „J“ „K“ – „O“ „T“ „A“ - „O“ „P“ - „Z“ „P „Q“, „R“ „Q“ „R“ „P“ Studiengang Informatik Quicksort 29S. Berninger DHBW Heidenheim Quicksort: • ist ein extrem schneller Sortieralgorithmus, besonders effizient für Average-Szenarien • performed in worst-case Szenarien (invers sortierten Arrays) gleich zu Insertion und Selection sort • ist sehr viel schneller für Average-Szenarien – die am häufigsten auftreten! • stützt sich auf ein Partitionierung genanntes Konzept ab: Einführung siehe nächste Folien Studiengang Informatik Beispiel: Quicksort: Partitionierung im Array 30S. Berninger DHBW Heidenheim Partitionierung eines Arrays: • einen zufälligen Wert des Arrays nehmen – wir nennen ihn Pivot(-element) • sicherstellen, dass jede Zahl des Arrays, die kleiner als das Pivotelement ist, links davon einsortiert wird, und jede Zahl, die größer ist als das Pivotelement, rechts davon einsortiert wird Der Konsistenz halber wählen wir hier stets das ganz rechte Element als Pivot (könnten technisch jedes nehmen) Wir wählen dann Indices —initial einen auf das Element des Arrays vor dem 0., und den Index 0 des Arrays. left right index index Studiengang Informatik Quicksort: Partitionierung im Array 31S. Berninger DHBW Heidenheim Partitionierung: 1. Der rechte Index wird solange immer wieder auf die nächste Zelle erhöht, bis er einen Wert erreicht, der kleiner als das Pivotelement ist (oder das Ende des Arrays), und stoppt dann. 2. Erreicht der rechte Index einen Wert < Pivot, erhöht er den linken Pointer und tauscht den Wert mit dessen Inhalt. 3. Final vertauschen wir das Pivotelement mit dem Inhalt des um 1 erhöhten linken Index. left index right index Studiengang Informatik Quicksort: Partitionierung 33S. Berninger DHBW Heidenheim Nach der Partitionierung gilt: • alle Werte links vom linken Index sind kleiner/ gleich dem Pivot • alle Werte rechts vom Pivot sind größer als das Pivot • das Pivot selbst ist jetzt am korrekten Platz im Array • die anderen Werte sind untereinander noch nicht sortiert… Studiengang Informatik Quicksort: Partitioning S. Berninger DHBW Heidenheim Anwendung auf das Beispiel: Schritt #1 (Wert 0): Ist der Inhalt des rechten Index kleiner als das Pivot (Wert 3)? Ja: linker Index wird erhöht, swap. Schritt #1 (Wert 5): Ist der Inhalt des rechten Index kleiner als das Pivot (Wert 3)? Nein. Schritt #2 (Wert 5): Der rechte Index wird erhöht: Ist das Pivot erreicht? Nein: Schritt 1. 36 left index right index left index right index left index right index Schritt #2 (Wert 2): Der rechte Index wird erhöht: Ist das Pivot erreicht? Nein: Schritt 1. Studiengang Informatik Quicksort: Partitioning S. Berninger DHBW Heidenheim Anwendung auf das Beispiel: Schritt #1 (Wert 2): Ist der Inhalt des rechten Index kleiner als das Pivot (Wert 3)? Ja: linker Index wird erhöht, swap. Schritt #1 (Wert 1): Ist der Inhalt des rechten Index kleiner als das Pivot (Wert 3)? Ja: linker Index wird erhöht, swap. Schritt #2 (Wert 1): Der rechte Index wird erhöht: Ist das Pivot erreicht? Nein: Schritt 1. 37 left index right index left index right index left index right index Schritt #2 (Wert 6): Der rechte Index wird erhöht: Ist das Pivot erreicht? Nein: Schritt 1. left index right index Studiengang Informatik Quicksort: Partitioning S. Berninger DHBW Heidenheim Anwendung auf das Beispiel: Schritt #1 (Wert 6): Ist der Inhalt des rechten Index kleiner als das Pivot (Wert 3)? Nein. Schritt #2 (Pivot): Der rechte Index wird erhöht: Ist das Pivot erreicht? Ja. 38 left index right index left index right index Schritt #3: Finaler Schritt der Partitionierung: Pivot tauscht mit Inhalt des um 1 erhöhten linken Index. Stop. Partitionierung ist erfolgreich abgeschlossen (Array ist nicht komplett sortiert). Das Pivot (die 3) ist jetzt an ihrem korrekten Platz im Array. left index right index Studiengang Informatik Quicksort: Partitioning 40 public class Algorithms { public int[]? NumArray { get; set; } // Array private int Partition (int left, int right) // between left and right border { int pivot = NumArray[right]; // select pivotPointer from right border int i = left - 1; for (int j = left; j < right; j++) { if (NumArray[j] <= pivot) // number < pivot, swapped with latest smaller than pivot { i++; int temp = NumArray[i]; NumArray[i] = NumArray[j]; NumArray[j] = temp; } } int temp1 = NumArray[i + 1]; // Pivot swapped with the number next to the latest smaller one NumArray[i + 1] = NumArray[right]; NumArray[right] = temp1; return i + 1; // new pivot position } S. Berninger DHBW Heidenheim Studiengang Informatik Initiale Partitionierung des Arrays [0, 5, 2, 1, 6, 3] für Quicksort: Quicksort: Partitioning Der Quicksort-Algorithmus basiert stark auf Partitionierung: S. Berninger DHBW Heidenheim Dann: Alle Zahlen links vom Pivot als eigenes Array behandeln und partitionieren. 3. Haben wir nur noch Teilarrays mit 0 oder 1 Element, haben wir das Abbruchkriterium erreicht und wir stoppen. 2. Wir behandeln das linke und rechte Teilarray als eigenständige Arrays, und wiederholen rekursiv die Schritte #1 und #2. Wir partitionen dabei jedes Teilarray und erhalten kleinere Teil-Teil-Arrays links und rechts von jedem Teilarray-Pivot. Wir partitionieren dann diese Teil-Teil-Arrays, usw. usf. … 1. Wir partitionieren das Array. Das Pivot ist jetzt am richtigen Platz. 41Studiengang Informatik Quicksort: Partitioning Wir haben nun ein Teilarray [0, 1] links vom Pivot (der 2) und kein Teilarray rechts. Nächstes Pivot ist damit die 1: S. Berninger DHBW Heidenheim Nun fokussieren wir uns auf [6, 5]: 42Studiengang Informatik Quicksort: Implementierung S. Berninger DHBW Heidenheim 44 Kein doppelter Aufruf! public class Algorithms { public int[]? NumArray { get; set; } // Array private int Partition (int left, int right) // between left and right border { // .... } public void QuickSort(int left, int right) { if (left < right) { int pivot = Partition(left, right); QuickSort( left, pivot - 1); QuickSort( pivot + 1, right); } } } Studiengang Informatik Die Effizienz von Quicksort Effizienz einer einzelnen Partitionierung aller Elemente: Partitionierung hat 2 Arten von Schritten: Vergleiche: Wir vergleichen jeden Wert mit dem Pivot Vertauschungen: Wenn nötig, vertauschen wir die Inhalte des linken und rechten Pointers (abhängig vom Sortierungsgrad der Daten maximal N / 2 Vertauschungen). Bei zufällig sortierten Daten vertauschen wir ca. die Hälfte der Werte. Im Schnitt machen wir also N/4 Vertauschungen. Das ergibt ca. 1.25N Schritte für N Datenelemente. In Big O-Notation: O(N) Das ist die Effizienz einer einzelnen Partitionierung. S. Berninger DHBW Heidenheim 46Studiengang Informatik Die Effizienz von Quicksort Wie häufig partitionieren wir die Elementmenge? S. Berninger DHBW Heidenheim 8 Elemente partitionieren wir log(n) mal! Quicksort besteht aus einer Serie von Partitionierungen, und jede Partitionierung braucht N Schritte für N Elemente eines Teilarrays. Gesamtzahl der Quicksort-Schritte: Produkt aller nötigen Partitionierungen (log(n)) mit den zu partitionierenden Elementen (n) Studiengang Informatik Die Effizienz von Quicksort S. Berninger DHBW Heidenheim Die Anzahl der Quicksort-Schritte ist ca. N * log N: Quicksort ist ein O(N log N) – Algorithmus im Average case. Eine für uns neue Big O - Kategorie! 49Studiengang Informatik Die Effizienz von Quicksort Einordnung von O(N log N) in die anderen Kategorien von Big O: S. Berninger DHBW Heidenheim 50Studiengang Informatik Die Effizienz von Quicksort Für ein Array der Größe 8 benötigen wir 3 “Halbierungen”, und wir schauen deshalb 3 x 8 Elemente an. Quicksort benötigt N * log N Schritte: Wir brauchen log N Halbierungen, und für jede Halbierung führen wir eine Partitionierung aller Teilarrays durch, deren Elementanzahl insgesamt N beträgt. S. Berninger DHBW Heidenheim 51Studiengang Informatik Übung Die worst-case Effizienz von Quicksort ist: A: O(1) B: O(logN) C: O(N) D: O(N*logN) E: O(N2) S. Berninger DHBW Heidenheim 55Studiengang Informatik Übung Die average-case Effizienz von Quicksort ist: A: O(1) B: O(logN) C: O(N) D: O(N*logN) E: O(N2) S. Berninger DHBW Heidenheim 57Studiengang Informatik Übung Die best-case Effizienz von Quicksort ist: A: O(1) B: O(logN) C: O(N) D: O(N*logN) E: O(N2) S. Berninger DHBW Heidenheim 59Studiengang Informatik Abschliessender Vergleich Worst case: N2 N2 + N N2/2 N2 Best case: N N N2/2 N*logN Average case: (N2-N)/4 N2/2 N2/2 N*logN Bubble sort Insertion sort Selection sort Quick sort S. Berninger DHBW Heidenheim 61Studiengang Informatik Woher weiss ein Algorithmus, welche Knoten in welcher Reihenfolge zur gleichen verketteten Liste gehören? • jeder Knoten trägt eine Extrainformation: die Speicheradresse des nächsten Knoten in der Liste • Typ: Pointer auf die Adresse des nächsten Elements • Daten: vier Datenelemente: \"a\", \"b\", \"c\", und \"d\" Knotenbasierte Datenstrukturen 62 Knoten sind Teile verbundener Daten, die im Speicher gestreut liegen können • Verkettete Listen: einfachste knotenbasierte Datenstruktur • Statt einem fortlaufenden Speicherblock können die Knoten verketteter Listen verstreut sein. S. Berninger DHBW Heidenheim Studiengang Informatik Wiederholung: Verkettete Listen 63 Die Beispielliste benötigt 8 Speicherzellen, da jeder Knoten aus 2 Speichereinträgen besteht: • die 1. Zelle jeden Knotens enthält die Daten, • die 2. Zelle enthält den Pointer auf den nächsten Knoten • Der Pointer des letzten Knoten enthält NULL (=0) • In C verwenden wir die Datenstruktur “struct” für den Knotentyp S. Berninger DHBW Heidenheim Anker • Knoten können (müssen) dynamisch angefordert und hinzugefügt bzw. entfernt und freigegeben werden • Unabhängig von der Datenstruktur selbst braucht man stets den Anker (oder “root”) als Zeiger auf das erste Element/ den ersten Knoten Studiengang Informatik Verkettete Listen: Implementierung 64 • Manche Sprachen (z.B. C#, Java) unterstützen Listen als eigene Datenstruktur, aber viele Sprachen auch nicht S. Berninger DHBW Heidenheim Anker • Struktur für den Knoten (Bsp: Daten als int-Wert): • Liste mit 2 Elementen: Studiengang Informatik Zugriff, Suche, Einfügen und Löschen 65 Zugriff: Auf den Wert des 3. Listenelements kann nicht direkt zugegriffen werden! Zuerst muss der erste Knoten adressiert werden. Dann folgt man dem Pointer des 1. Knotens zum 2., und dann dem Pointer des 2. zum 3. Knoten. S. Berninger DHBW Heidenheim Anker -> Verkettete Listen haben einen worst-case-Zugriff von O(N) (im Vergleich Arrays: O(1) ). Studiengang Informatik Implementierung: Zugriff 66 Zugriffsfunktion: S. Berninger DHBW Heidenheim Anker Studiengang Informatik Implementierung: Suche nach Datenwert 67 • Die Suche in Arrays kostet O(N), weil der Algorithmus jeden Wert einzeln ansehen musss • Nicht überraschend: verkettete Listen haben ebenfalls eine Suchgeschwindigkeit von O(N)! • Gleicher Algorithmus wie beim Zugriff – aber nicht der Index wird verglichen, sondern der Datenwert… : Aufruf: struct node *pElement= searchElement (pAnker, 42); S. Berninger DHBW Heidenheim Studiengang Informatik Implementierung: Einfügen 68 U.U. entscheidende Vorteile verketteter Listen beim Einfügen gegenüber Arrays: • Das worst-case-Szenario für das Einfügen in Arrays ist das Einfügen am Index 0, weil alle Elemente des Arrays verschoben werden müssen (O(N)) • Das Einfügen am Beginn einer Liste kostet nur O(1). • Das Einfügen in eine verkettete Liste kostet überall nur einen Schritt – wenn man bereits den Pointer auf das vorige Element hat, nach dem eingefügt werden soll S. Berninger DHBW Heidenheim Anker Studiengang Informatik Implementierung: Einfügen 69S. Berninger DHBW Heidenheim Anker Anker Schritt 1: Zugriff auf das Element mit dem Wert „blue“: worst case O(n) Schritt2: Einfügen Szenario Array Verkettete Liste Einfügen am Anfang Worst case Best case Einfügen in der Mitte Average case Average case Einfügen am Ende Best case Worst case Studiengang Informatik Implementierung: Einfügen nach einem Element 70S. Berninger DHBW Heidenheim Studiengang Informatik Löschen (nach einem Element) 71S. Berninger DHBW Heidenheim Gleich zum Einfügen: Szenario Array Verkettete Liste Einfügen am Anfang Worst case Best case Einfügen in der Mitte Average case Average case Einfügen am Ende Best case Worst case Studiengang Informatik Effizienz verketteter Listen 72S. Berninger DHBW Heidenheim Operation Array Verkettete Liste Zugriff O(1) O(N) Suche O(N) O(N) Einfügen O(N) (O(1) am Ende) O(1) Löschen O(N) (O(1) am Ende) O(1) Die reinen Schritte für’s Einfügen und Löschen in verketteten Listen sind nur O(1). Meist haben wir das vorige Element durch vorangegangene Suchvorgänge schon in der Hand! Studiengang Informatik Doppelt verkettete Listen 73S. Berninger DHBW Heidenheim Jeder Knoten hat zwei Verbindungen—eine zeigt auf den nächsten, die andere auf den vorigen Knoten. Zusätzlich bewahrt man stets den Anker auf den ersten Knoten und den Endepointer auf den letzten Knoten auf! Ende:Anker: Studiengang Informatik Doppelt verkettete Listen: Implementierung 74S. Berninger DHBW Heidenheim Ende:Anker: Doppelt verkettete Liste mit 2 Pointern: Studiengang Informatik Doppelt verkettete Listen: Einfügen 75S. Berninger DHBW Heidenheim Einfügen am Ende einer doppelt verketteten Liste Anker→ Ende Studiengang Informatik Queues als doppelt verkettete Listen 76S. Berninger DHBW Heidenheim Doppelt verkettete Listen haben: • einen Direktzugriff zum Anfang der Liste • einen Direktzugriff zum Ende der Liste -> sie können Daten mit O(1) an jedem der beiden Enden einfügen und auch löschen. Aufgrund dieser Eigenschaft sind doppelt verkettete Listen die perfekte Basis-Datenstruktur für Queues. (Arrays sind O(1) für Einfügen am Ende, jedoch O(N) für Löschen am Beginn) (Einfach verkettete Listen: O(N) für Einfügen/ Löschen am Ende, jedoch O(1) für Éinfügen/Löschen am Beginn) Studiengang Informatik Zusammenfassung 77S. Berninger DHBW Heidenheim Feine Unterschiede zwischen Arrays und Listen schaffen neue Wege, unseren Code (z.B. für Queues) schneller zu machen. Verkettete Listen sind die Einfachste der knotenbasierten Strukturen. Studiengang Informatik Übung: 10 min. für Lösungsideen 78S. Berninger DHBW Heidenheim Ein brilliantes kleines Puzzle verketteter Listen: Sie haben Zugriff auf einen bestimmten Knoten irgendwo in der Mitte einer einfach verketteten Liste, nicht aber auf den Anker selbst. Sie haben also nur einen Knoten, aber nicht die Instanz der Liste. In dieser Situation haben Sie durch Verfolgen der Links Zugriff auf alle Knoten von dem bestimmten bis zum letzten, aber Sie haben keine Chance, Zugriff auf die Knoten davor zu bekommen. Wie können Sie trotzdem diesen bestimmten Knoten effizient aus der Liste löschen (heißt: die Liste effizient um diesen Knoten verkürzen)? Studiengang Informatik Lösung 79S. Berninger DHBW Heidenheim temp","libVersion":"0.3.2","langs":""}